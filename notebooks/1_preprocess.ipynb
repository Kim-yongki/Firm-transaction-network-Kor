{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1317ad20",
   "metadata": {},
   "source": [
    "## 0. Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeceb4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os, gc\n",
    "from tqdm import tqdm\n",
    "import re, glob\n",
    "import numpy as np\n",
    "\n",
    "os.chdir(os.getcwd())\n",
    "\n",
    "def print_save(file, t, a='a'):\n",
    "    file = '1_log/' + file\n",
    "    with open(file, a) as f:\n",
    "        f.write(f\"\\n{t}\")\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4165212a",
   "metadata": {},
   "source": [
    "## 2.108 preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e735d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firm transaction data (108)\n",
    "\n",
    "INPUT_PATH  = \"data/raw/VATTXDT0108/VATTXDT0108.txt\"  # if .txt, use sep='|'\n",
    "SEP         = '|' if INPUT_PATH.lower().endswith('.txt') else ','\n",
    "CHUNK_SIZE  = 10000000\n",
    "LOG_PATH    = \"1_log/1_108_preprocess.txt\"\n",
    "\n",
    "# Years to process: list for specific years, None for all\n",
    "YEARS       = [2016, 2017, 2018, 2019, 2020, 2021, 2022]\n",
    "TOTAL_ROWS  = 2073201182\n",
    "\n",
    "ALL_COLS = ['1_BZNO','2_BASE_YY','3_PRD_CCD','4_SALE_BUY_CCD','5_VAT_ITM_KCD','6_VAT_SEQ',\n",
    "            '7_CONO_PID','8_KEDCD','9_CO_BZNO_CCD','10_TX_PRD_FROM','11_TX_PRD_TO','12_TX_PRD_SUTB_YN',\n",
    "            '13_TXPL_TXRI_RNK','14_TXRI','15_TXPL_BZNO','16_TXPL_PID','17_TXPL_KEDCD','18_TXPL_NM',\n",
    "            '19_TXPL_TCN','20_TXPL_TSPPR','21_TXPL_TTAX_AM','22_BZNO_SIMSID','23_TXPL_BZNO_SIMSID','24_BZNO_del','None']\n",
    "USECOLS = ['2_BASE_YY','4_SALE_BUY_CCD','22_BZNO_SIMSID','23_TXPL_BZNO_SIMSID']\n",
    "DTYPES  = {'2_BASE_YY':'int32','4_SALE_BUY_CCD':'int8','22_BZNO_SIMSID':'object','23_TXPL_BZNO_SIMSID':'object'} \n",
    "\n",
    "OUT_DIR = \"data/processed/deal_network\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.dirname(LOG_PATH), exist_ok=True)\n",
    "\n",
    "# Buffers for tracking yearly results\n",
    "curr_year = None\n",
    "year_rows_total_buf = 0            # rows after year filter, before dropna\n",
    "year_rows_kept_buf  = 0            # rows kept after dropna\n",
    "year_pairs_buf      = set()        # unique (src, dst) pairs for the current year\n",
    "per_year_rows_total = {}           # total rows per year\n",
    "per_year_rows_kept  = {}           # kept rows per year\n",
    "years_touched       = set()        # years processed\n",
    "\n",
    "rows_seen = 0\n",
    "n_chunks  = 0\n",
    "\n",
    "# Reader setup\n",
    "reader = pd.read_csv(\n",
    "    INPUT_PATH, sep=SEP, header=0, names=ALL_COLS, usecols=USECOLS,\n",
    "    dtype=DTYPES, chunksize=CHUNK_SIZE, low_memory=False\n",
    ")\n",
    "total_iter = (TOTAL_ROWS // CHUNK_SIZE + 1) if TOTAL_ROWS else None\n",
    "\n",
    "# Helper to write yearly outputs\n",
    "def flush_year(y, merge_pairs):\n",
    "    out_path = os.path.join(OUT_DIR, f\"filtered_108_{y}.csv\")\n",
    "    df_out = pd.DataFrame(merge_pairs, columns=['22_BZNO_SIMSID','23_TXPL_BZNO_SIMSID'])\n",
    "    if os.path.exists(out_path):\n",
    "        prev = pd.read_csv(out_path, dtype = {'2_BASE_YY':'int32', '22_BZNO_SIMSID': str, '23_TXPL_BZNO_SIMSID': str})\n",
    "        prev_pairs = set(prev[['22_BZNO_SIMSID', '23_TXPL_BZNO_SIMSID']].itertuples(index=False, name=None))\n",
    "        union_pairs = prev_pairs | merge_pairs\n",
    "        df_out = pd.DataFrame(union_pairs, columns=['22_BZNO_SIMSID','23_TXPL_BZNO_SIMSID'])\n",
    "\n",
    "    df_out.insert(0,'2_BASE_YY', y)\n",
    "    df_out.to_csv(out_path, index=False)\n",
    "\n",
    "# Main chunk loop\n",
    "for chunk in tqdm(reader, total=total_iter, desc=\"Processing chunks\"):\n",
    "    n_chunks  += 1\n",
    "    rows_seen += len(chunk)\n",
    "\n",
    "    # Year filter\n",
    "    if YEARS is not None:\n",
    "        chunk = chunk[chunk['2_BASE_YY'].isin(YEARS)]\n",
    "    if chunk.empty:\n",
    "        gc.collect()\n",
    "        continue\n",
    "\n",
    "    # Iterate through years within chunk (boundary-safe)\n",
    "    for y in chunk['2_BASE_YY'].astype('int32').drop_duplicates().to_numpy():\n",
    "        y = int(y)\n",
    "\n",
    "        # Flush if year changes\n",
    "        if curr_year is None:\n",
    "            curr_year = y\n",
    "        elif y != curr_year:\n",
    "            if year_rows_total_buf > 0:\n",
    "                flush_year(curr_year, year_pairs_buf)\n",
    "                year_rows_total_buf = 0\n",
    "                year_rows_kept_buf  = 0\n",
    "                year_pairs_buf.clear()\n",
    "                gc.collect()\n",
    "            curr_year = y\n",
    "\n",
    "        # Subset for current year\n",
    "        sub0 = chunk.loc[chunk['2_BASE_YY'] == y,\n",
    "                         ['22_BZNO_SIMSID','23_TXPL_BZNO_SIMSID','4_SALE_BUY_CCD']]\n",
    "        year_rows_total_buf += len(sub0)\n",
    "        years_touched.add(y)\n",
    "        per_year_rows_total[y] = per_year_rows_total.get(y, 0) + len(sub0)\n",
    "    \n",
    "        sub = sub0.dropna()\n",
    "        kept = len(sub)\n",
    "        year_rows_kept_buf += kept\n",
    "        per_year_rows_kept[y] = per_year_rows_kept.get(y, 0) + kept\n",
    "\n",
    "        # Normalize: swap SIMSID for purchases (2)\n",
    "        mask_in = (sub['4_SALE_BUY_CCD'] == 2)\n",
    "        if mask_in.any():\n",
    "            tmp = sub.loc[mask_in, '22_BZNO_SIMSID'].to_numpy(copy=True)\n",
    "            sub.loc[mask_in, '22_BZNO_SIMSID'] = sub.loc[mask_in, '23_TXPL_BZNO_SIMSID'].to_numpy(copy=True)\n",
    "            sub.loc[mask_in, '23_TXPL_BZNO_SIMSID'] = tmp\n",
    "\n",
    "        # Add unique pairs\n",
    "        year_pairs_buf.update(sub[['22_BZNO_SIMSID','23_TXPL_BZNO_SIMSID']].itertuples(index=False, name=None))\n",
    "\n",
    "        del sub0, sub\n",
    "\n",
    "    del chunk\n",
    "    gc.collect()\n",
    "\n",
    "# Flush last year\n",
    "if curr_year is not None and year_rows_total_buf > 0:\n",
    "    flush_year(curr_year, year_pairs_buf)\n",
    "    year_pairs_buf.clear()\n",
    "    gc.collect()\n",
    "\n",
    "# Summary log\n",
    "final_years = sorted(years_touched)\n",
    "per_year_unique_pairs = {}\n",
    "for y in final_years:\n",
    "    p = os.path.join(OUT_DIR, f\"filtered_108_{y}.csv\")\n",
    "    if os.path.exists(p):\n",
    "        try:\n",
    "            n = max(0, sum(1 for _ in open(p, 'r', encoding = 'utf-8')) - 1)\n",
    "        except Exception:\n",
    "            df_tmp = pd.read_csv(p, dtype = {'2_BASE_YY':'int32', '22_BZNO_SIMSID': str, '23_TXPL_BZNO_SIMSID': str})\n",
    "            n = len(df_tmp)\n",
    "            del df_tmp\n",
    "        per_year_unique_pairs[y] = n\n",
    "    else:\n",
    "        per_year_unique_pairs[y] = 0\n",
    "\n",
    "with open(LOG_PATH, 'w', encoding = 'utf-8') as f:\n",
    "    f.write(\"Preprocess summary (dataset 108)\\nPer year:\\n\")\n",
    "    all_rows = 0\n",
    "    all_filtered_rows = 0\n",
    "    all_pairs = 0\n",
    "    for y in final_years:\n",
    "        total_ = per_year_rows_total.get(y,0)\n",
    "        all_rows += total_\n",
    "        kept_ = per_year_rows_kept.get(y,0)\n",
    "        all_filtered_rows += kept\n",
    "        uniq_ = per_year_unique_pairs.get(y,0)\n",
    "        all_pairs += uniq_\n",
    "        f.write(f\"  - Year {y}: rows = {total_:,}, filtered_rows = {kept_:,}, unique_pairs = {uniq_:,}\\n\")\n",
    "    \n",
    "    f.write(f\"\\nAll rows = {rows_seen:,}, 2016-2022 rows = {all_rows:,}, all_filtered_rows = {all_filtered_rows}, all_pairs = {uniq_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512126d2",
   "metadata": {},
   "source": [
    "## 3. Sims preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08efc0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "D001 = pd.read_sas(\"data/raw/sims_d001.sas7bdat\", format='sas7bdat', encoding='ansi')\n",
    "D001.columns=['1_사업자등록번호','2_법인등록번호','3_우편번호','4_설립일자','5_휴업폐업일자','6_표준산업분류항목코드','7_표준산업분류 대분류','8_주요생산품내용','9_법인여부','10_기업공개코드','11_기업상태코드','12_기업형태코드','13_기업규모코드','14_우수기업여부','15_사업자성격구분코드']\n",
    "\n",
    "#1m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50895391",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = D001\n",
    "print_save('1_SIMS_preprocess.txt', 'SIMS_preprocess', a= 'w')\n",
    "\n",
    "\n",
    "# read spatial info\n",
    "gdfs = []\n",
    "sd = os.listdir('data/raw/원본파일(시도별)')\n",
    "for map in sd:\n",
    "    gdf = gpd.read_file(f'data/raw/원본파일(시도별)/{map}/TL_KODIS_BAS.shp', encoding='cp949')\n",
    "    gdfs.append(gdf)\n",
    "BAS_map = gpd.GeoDataFrame(pd.concat(gdfs, ignore_index=True))\n",
    "BAS_map[['BAS_ID', 'SIG_CD', 'SIG_KOR_NM']] = BAS_map[['BAS_ID', 'SIG_CD', 'SIG_KOR_NM']].astype(str)\n",
    "BAS_map = BAS_map[['BAS_ID', 'SIG_CD', 'SIG_KOR_NM']]\n",
    "\n",
    "# merge spatial info\n",
    "print_save('1_SIMS_preprocess.txt', f'\\nBAS_ID missing rate: {round(df[\"3_우편번호\"].isna().mean(), 3)}')\n",
    "df = df.merge(BAS_map, left_on='3_우편번호', right_on='BAS_ID', how='left')\n",
    "\n",
    "# filter error data\n",
    "df['4_설립일자'] = pd.to_datetime(df['4_설립일자'], format='%Y%m%d', errors='coerce')\n",
    "df['5_휴업폐업일자'] = pd.to_datetime(df['5_휴업폐업일자'], format='%Y%m%d', errors='coerce')\n",
    "\n",
    "df = df[['1_사업자등록번호', '4_설립일자', '6_표준산업분류항목코드','7_표준산업분류 대분류','13_기업규모코드', 'SIG_CD', 'SIG_KOR_NM']]\n",
    "df.to_csv(\"data/processed/deal_network/filtered_SIMS.csv\", index=False)\n",
    "\n",
    "#30s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ac0faf",
   "metadata": {},
   "source": [
    "## 4.Stats preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9202b8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_D = pd.read_csv('data/raw/250714_중소기업 지원정책 수립 지원을 위한 자료제공_수정_DATA_ENC.txt', sep = \"\\t\",  dtype={\n",
    "                'BRNO': str,\n",
    "                'ADMDST_CLSF_CD': str,\n",
    "                \"ENT_SCL_SE_CD\": str,\n",
    "                \"BZENT_KSIC_LVL5_CD\": str\n",
    "            })\n",
    "\n",
    "#1m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a329e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_D_filter = stat_D[['CRTR_1', 'BRNO', 'ENT_SCL_SE_CD', 'BZENT_KSIC_LVL5_CD']]\n",
    "stat_D_filter.columns = ['기준년도', '1_사업자등록번호', '13_기업규모코드_stat', '6_표준산업분류항목코드_stat']\n",
    "stat_D_filter.loc[stat_D_filter['13_기업규모코드_stat']=='2', '13_기업규모코드_stat'] = '1'\n",
    "\n",
    "stat_D_filter.loc[stat_D_filter['13_기업규모코드_stat']=='4', '13_기업규모코드_stat'] = '2'\n",
    "stat_D_filter.loc[stat_D_filter['13_기업규모코드_stat']=='5', '13_기업규모코드_stat'] = '2'\n",
    "\n",
    "stat_D_filter.loc[stat_D_filter['13_기업규모코드_stat']=='6', '13_기업규모코드_stat'] = '4'\n",
    "\n",
    "stat_D_filter.loc[stat_D_filter['13_기업규모코드_stat']=='91', '13_기업규모코드_stat']= '9999'\n",
    "stat_D_filter = stat_D_filter.sort_values('기준년도', ascending=False)\n",
    "stat_D_filter = stat_D_filter.groupby('1_사업자등록번호')[['1_사업자등록번호','13_기업규모코드_stat', '6_표준산업분류항목코드_stat']].bfill()\n",
    "stat_D_filter = stat_D_filter.drop_duplicates('1_사업자등록번호')\n",
    "stat_D_filter['6_표준산업분류항목코드_stat'] = stat_D_filter['6_표준산업분류항목코드_stat'].fillna('9999').astype(int)\n",
    "\n",
    "\n",
    "conditions = [\n",
    "  (stat_D_filter['6_표준산업분류항목코드_stat'].astype(str).str[:2].astype(int) >= 1) & (stat_D_filter['6_표준산업분류항목코드_stat'].astype(str).str[:2].astype(int) <=3),\n",
    "  (stat_D_filter['6_표준산업분류항목코드_stat'].astype(str).str[:2].astype(int) >= 5) & (stat_D_filter['6_표준산업분류항목코드_stat'].astype(str).str[:2].astype(int) <=8),\n",
    "  (stat_D_filter['6_표준산업분류항목코드_stat'].astype(str).str[:2].astype(int) >= 10) & (stat_D_filter['6_표준산업분류항목코드_stat'].astype(str).str[:2].astype(int) <= 34),\n",
    "  (stat_D_filter['6_표준산업분류항목코드_stat'].astype(str).str[:2].astype(int) == 35),\n",
    "  (stat_D_filter['6_표준산업분류항목코드_stat'].astype(str).str[:2].astype(int) >= 36) & (stat_D_filter['6_표준산업분류항목코드_stat'].astype(str).str[:2].astype(int) <= 39),\n",
    "  (stat_D_filter['6_표준산업분류항목코드_stat'].astype(str).str[:2].astype(int) >= 41) & (stat_D_filter['6_표준산업분류항목코드_stat'].astype(str).str[:2].astype(int) <= 42),\n",
    "  (stat_D_filter['6_표준산업분류항목코드_stat'].astype(str).str[:2].astype(int) >= 45) & (stat_D_filter['6_표준산업분류항목코드_stat'].astype(str).str[:2].astype(int) <= 47),\n",
    "  (stat_D_filter['6_표준산업분류항목코드_stat'].astype(str).str[:2].astype(int) >= 49) & (stat_D_filter['6_표준산업분류항목코드_stat'].astype(str).str[:2].astype(int) <= 52),\n",
    "  (stat_D_filter['6_표준산업분류항목코드_stat'].astype(str).str[:2].astype(int) >= 55) & (stat_D_filter['6_표준산업분류항목코드_stat'].astype(str).str[:2].astype(int) <= 56),\n",
    "  (stat_D_filter['6_표준산업분류항목코드_stat'].astype(str).str[:2].astype(int) >= 58) & (stat_D_filter['6_표준산업분류항목코드_stat'].astype(str).str[:2].astype(int) <= 63),\n",
    "  (stat_D_filter['6_표준산업분류항목코드_stat'].astype(str).str[:2].astype(int) >= 64) & (stat_D_filter['6_표준산업분류항목코드_stat'].astype(str).str[:2].astype(int) <= 66),\n",
    "  (stat_D_filter['6_표준산업분류항목코드_stat'].astype(str).str[:2].astype(int) == 68),\n",
    "  (stat_D_filter['6_표준산업분류항목코드_stat'].astype(str).str[:2].astype(int) >= 70) & (stat_D_filter['6_표준산업분류항목코드_stat'].astype(str).str[:2].astype(int) <= 73),\n",
    "  (stat_D_filter['6_표준산업분류항목코드_stat'].astype(str).str[:2].astype(int) >= 74) & (stat_D_filter['6_표준산업분류항목코드_stat'].astype(str).str[:2].astype(int) <= 76),\n",
    "  (stat_D_filter['6_표준산업분류항목코드_stat'].astype(str).str[:2].astype(int) == 84),\n",
    "  (stat_D_filter['6_표준산업분류항목코드_stat'].astype(str).str[:2].astype(int) == 85),\n",
    "  (stat_D_filter['6_표준산업분류항목코드_stat'].astype(str).str[:2].astype(int) >= 86) & (stat_D_filter['6_표준산업분류항목코드_stat'].astype(str).str[:2].astype(int) <= 87),\n",
    "  (stat_D_filter['6_표준산업분류항목코드_stat'].astype(str).str[:2].astype(int) >= 90) & (stat_D_filter['6_표준산업분류항목코드_stat'].astype(str).str[:2].astype(int) <= 91),\n",
    "  (stat_D_filter['6_표준산업분류항목코드_stat'].astype(str).str[:2].astype(int) >= 94) & (stat_D_filter['6_표준산업분류항목코드_stat'].astype(str).str[:2].astype(int) <= 96),\n",
    "  (stat_D_filter['6_표준산업분류항목코드_stat'].astype(str).str[:2].astype(int) >= 97) & (stat_D_filter['6_표준산업분류항목코드_stat'].astype(str).str[:2].astype(int) <= 98),\n",
    "  (stat_D_filter['6_표준산업분류항목코드_stat'].astype(str).str[:2].astype(int) == 99)\n",
    "]\n",
    "\n",
    "values = ['A' ,'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U']\n",
    "\n",
    "stat_D_filter['7_표준산업분류 대분류_stat'] = np.select(conditions, values, default = None)\n",
    "\n",
    "stat_D_filter.loc[stat_D_filter['6_표준산업분류항목코드_stat']=='9999', '6_표준산업분류항목코드_stat']= None\n",
    "stat_D_filter.loc[stat_D_filter['13_기업규모코드_stat']=='9999', '13_기업규모코드_stat']= None\n",
    "stat_D_filter = stat_D_filter[['1_사업자등록번호', '13_기업규모코드_stat', '6_표준산업분류항목코드_stat', '7_표준산업분류 대분류_stat']]\n",
    "stat_D_filter['6_표준산업분류항목코드_stat'] = stat_D_filter['6_표준산업분류항목코드_stat'].astype(int).astype(str)\n",
    "stat_D_filter.to_csv(\"data/processed/deal_network/filtered_Stat.csv\", index=False)\n",
    "\n",
    "#3m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9e97ad",
   "metadata": {},
   "source": [
    "## 5. geo_network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab42f47",
   "metadata": {},
   "source": [
    "### 5.1. merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a32d16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read files\n",
    "\n",
    "deal_files = sorted(glob.glob(\"data/processed/deal_network/filtered_108_*.csv\"))\n",
    "\n",
    "stat_D_filter = pd.read_csv(\n",
    "    \"data/processed/deal_network/filtered_Stat.csv\",\n",
    "    dtype = {\n",
    "        '1_사업자등록번호': 'string',\n",
    "        \"13_기업규모코드_stat\": str,\n",
    "        \"6_표준산업분류항목코드_stat\": str\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "SIMS = pd.read_csv(\n",
    "    \"data/processed/deal_network/filtered_SIMS.csv\",\n",
    "    dtype={\n",
    "        '1_사업자등록번호': 'string',\n",
    "        '6_표준산업분류항목코드': str,\n",
    "        '13_기업규모코드': str,\n",
    "        'SIG_CD': str,\n",
    "        'SIG_KOR_NM': str\n",
    "    }\n",
    ")\n",
    "\n",
    "SIMS = SIMS[SIMS['SIG_CD'].notnull()]\n",
    "#20s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40a47d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"data/processed/deal_network/deal_by/year_by\", exist_ok=True)\n",
    "\n",
    "print_save('1_geo_network.txt', f\"\\n\\nstart\", a='w')\n",
    "\n",
    "missings_df = pd.DataFrame()\n",
    "\n",
    "for year_file in deal_files:\n",
    "    # 1) Load yearly file and merge seller/buyer information\n",
    "    df = pd.read_csv(year_file, dtype={'2_BASE_YY': str, '22_BZNO_SIMSID': 'string', '23_TXPL_BZNO_SIMSID': str})\n",
    "    year = int(df['2_BASE_YY'].mode().iloc[0]) if len(df) else -1\n",
    "    \n",
    "    IDset = set(list(df['22_BZNO_SIMSID'].unique()) + list(df['23_TXPL_BZNO_SIMSID'].unique()))\n",
    "    stat_D_filter_temp = stat_D_filter[stat_D_filter['1_사업자등록번호'].isin(IDset)]\n",
    "    SIMS_temp = SIMS[SIMS['1_사업자등록번호'].isin(IDset)]\n",
    "    del IDset\n",
    "    \n",
    "    df = pd.merge(df, stat_D_filter_temp, how='left',\n",
    "                  left_on='22_BZNO_SIMSID', right_on='1_사업자등록번호', suffixes=('_seller', '_buyer'))\n",
    "    df = pd.merge(df, stat_D_filter_temp, how='left',\n",
    "                  left_on='23_TXPL_BZNO_SIMSID', right_on='1_사업자등록번호', suffixes=('_seller', '_buyer'))\n",
    "    df = df.drop(columns=['1_사업자등록번호_seller', '1_사업자등록번호_buyer'])\n",
    "    \n",
    "    df = pd.merge(df, SIMS_temp, how='left',\n",
    "                  left_on='22_BZNO_SIMSID', right_on='1_사업자등록번호', suffixes=('_seller', '_buyer'))\n",
    "    df = pd.merge(df, SIMS_temp, how='left',\n",
    "                  left_on='23_TXPL_BZNO_SIMSID', right_on='1_사업자등록번호', suffixes=('_seller', '_buyer'))\n",
    "    df = df.drop(columns=['1_사업자등록번호_seller', '1_사업자등록번호_buyer'])\n",
    "\n",
    "    # Fill missing attributes with fallback (stat table → SIMS table)\n",
    "    df['13_기업규모코드_seller'] = df['13_기업규모코드_stat_seller'].fillna(df['13_기업규모코드_seller'])\n",
    "    df['6_표준산업분류항목코드_seller'] = df['6_표준산업분류항목코드_stat_seller'].fillna(df['6_표준산업분류항목코드_seller'])\n",
    "    df['7_표준산업분류 대분류_seller'] = df['7_표준산업분류 대분류_stat_seller'].fillna(df['7_표준산업분류 대분류_seller'])\n",
    "    df['13_기업규모코드_buyer'] = df['13_기업규모코드_stat_buyer'].fillna(df['13_기업규모코드_buyer'])\n",
    "    df['6_표준산업분류항목코드_buyer'] = df['6_표준산업분류항목코드_stat_buyer'].fillna(df['6_표준산업분류항목코드_buyer'])\n",
    "    df['7_표준산업분류 대분류_buyer'] = df['7_표준산업분류 대분류_stat_buyer'].fillna(df['7_표준산업분류 대분류_buyer'])\n",
    "    \n",
    "    # Missing-value statistics (stat table vs SIMS table)\n",
    "    summ_missings = {\n",
    "        'Columns' : ['기업규모코드_seller', '표준산업분류항목코드_seller', '표준산업분류 대분류_seller',\n",
    "                     '기업규모코드_buyer', '표준산업분류항목코드_buyer', '표준산업분류 대분류_buyer'],\n",
    "        'Stats'   : df[['13_기업규모코드_stat_seller', '6_표준산업분류항목코드_stat_seller',\n",
    "                        '7_표준산업분류 대분류_stat_seller', '13_기업규모코드_stat_buyer',\n",
    "                        '6_표준산업분류항목코드_stat_buyer', '7_표준산업분류 대분류_stat_buyer']].isnull().mean().values.tolist(),\n",
    "        'Kodata' : df[['13_기업규모코드_seller', '6_표준산업분류항목코드_seller',\n",
    "                       '7_표준산업분류 대분류_seller', '13_기업규모코드_buyer',\n",
    "                       '6_표준산업분류항목코드_buyer', '7_표준산업분류 대분류_buyer']].isnull().mean().values.tolist()\n",
    "    }\n",
    "    \n",
    "    summarry_stats = pd.DataFrame(summ_missings)\n",
    "    summarry_stats['diff'] = summarry_stats['Kodata'] - summarry_stats['Stats']\n",
    "    summarry_stats[['Stats', 'Kodata', 'diff']] = round(summarry_stats[['Stats', 'Kodata', 'diff']]*100, 1)\n",
    "    summarry_stats['year'] = year\n",
    "    missings_df = pd.concat([missings_df, summarry_stats])\n",
    "    \n",
    "    # Select and rename columns\n",
    "    df = df[['2_BASE_YY', '22_BZNO_SIMSID', '23_TXPL_BZNO_SIMSID',\n",
    "             '4_설립일자_seller','6_표준산업분류항목코드_seller','7_표준산업분류 대분류_seller','13_기업규모코드_seller',\n",
    "             'SIG_CD_seller','SIG_KOR_NM_seller',\n",
    "             '4_설립일자_buyer','6_표준산업분류항목코드_buyer','7_표준산업분류 대분류_buyer','13_기업규모코드_buyer',\n",
    "             'SIG_CD_buyer','SIG_KOR_NM_buyer']]\n",
    "    \n",
    "    # 2) Derive year and firm age\n",
    "    df['2_BASE_YY'] = pd.to_datetime(df['2_BASE_YY'], format='%Y', errors='coerce').dt.year\n",
    "    df['4_설립일자_seller'] = pd.to_datetime(df['4_설립일자_seller'], errors='coerce').dt.year\n",
    "    df['4_설립일자_buyer']  = pd.to_datetime(df['4_설립일자_buyer'], errors='coerce').dt.year\n",
    "    df['기업연령_seller'] = df['2_BASE_YY'] - df['4_설립일자_seller']\n",
    "    df['기업연령_buyer']  = df['2_BASE_YY'] - df['4_설립일자_buyer']\n",
    "\n",
    "    # 3) Final selection & renaming\n",
    "    df = df[['2_BASE_YY',\n",
    "             '기업연령_seller','6_표준산업분류항목코드_seller','7_표준산업분류 대분류_seller','13_기업규모코드_seller',\n",
    "             'SIG_CD_seller','SIG_KOR_NM_seller',\n",
    "             '기업연령_buyer','6_표준산업분류항목코드_buyer','7_표준산업분류 대분류_buyer','13_기업규모코드_buyer',\n",
    "             'SIG_CD_buyer','SIG_KOR_NM_buyer']]\n",
    "    df.columns = ['1_기준연도',\n",
    "                  '4_기업연령_seller','5_표준산업분류항목코드_seller','6_표준산업분류 대분류_seller','7_기업규모코드_seller',\n",
    "                  '8_시군구코드_seller','9_시군구명_seller',\n",
    "                  '10_기업연령_buyer','11_표준산업분류항목코드_buyer','12_표준산업분류 대분류_buyer','13_기업규모코드_buyer',\n",
    "                  '14_시군구코드_buyer','15_시군구명_buyer']\n",
    "\n",
    "    # 4) Data type adjustments (int vs str)\n",
    "    int_cols = ['1_기준연도','4_기업연령_seller','10_기업연령_buyer']\n",
    "    str_cols = ['5_표준산업분류항목코드_seller','11_표준산업분류항목코드_buyer',\n",
    "                '7_기업규모코드_seller','13_기업규모코드_buyer',\n",
    "                '8_시군구코드_seller','14_시군구코드_buyer']\n",
    "\n",
    "    df[int_cols] = df[int_cols].fillna(9999).astype(int)\n",
    "    df[str_cols] = df[str_cols].fillna('9999').astype(str)\n",
    "\n",
    "    # 5) Map firm size labels and bin firm ages\n",
    "    label_map = {'1':'대기업','2':'중소기업','3':'중견기업','4':'소상공인'}\n",
    "    df['7_기업규모코드_seller'] = df['7_기업규모코드_seller'].map(label_map).fillna('9999')\n",
    "    df['13_기업규모코드_buyer'] = df['13_기업규모코드_buyer'].map(label_map).fillna('9999')\n",
    "\n",
    "    df['4_기업연령_seller'] = pd.cut(df['4_기업연령_seller'],\n",
    "                                  bins=[-1,1,5,10,100],\n",
    "                                  labels=['1년 미만','1~5년 미만','5~10년 미만','10년 이상'])\n",
    "    df['10_기업연령_buyer']  = pd.cut(df['10_기업연령_buyer'],\n",
    "                                  bins=[-1,1,5,10,100],\n",
    "                                  labels=['1년 미만','1~5년 미만','5~10년 미만','10년 이상'])\n",
    "\n",
    "    # 6) Group definitions\n",
    "    innovation_codes = ['21','261','2621','263','264','265','266','284','285','27','282','313']\n",
    "    urban_codes = ['C','F','G','H','I','J','K','L','M','N','P','Q','R','S']\n",
    "\n",
    "    groups = []\n",
    "    groups.append(('all', None))\n",
    "    groups.append(('man', (df['6_표준산업분류 대분류_seller']=='C') & (df['12_표준산업분류 대분류_buyer']=='C')))\n",
    "    groups.append(('innovation',\n",
    "        (df['5_표준산업분류항목코드_seller'].str.startswith(tuple(innovation_codes))) &\n",
    "        (df['11_표준산업분류항목코드_buyer'].str.startswith(tuple(innovation_codes)))\n",
    "    ))\n",
    "    for age_group in df['4_기업연령_seller'].dropna().unique():\n",
    "        groups.append((f'urban_age_{age_group}',\n",
    "            (df['6_표준산업분류 대분류_seller'].isin(urban_codes)) &\n",
    "            (df['4_기업연령_seller']==age_group) &\n",
    "            (df['10_기업연령_buyer']==age_group)))\n",
    "    for size in set(df['7_기업규모코드_seller'].dropna().unique()) - {'9999'}:\n",
    "        groups.append((f'urban_size_{size}',\n",
    "            (df['6_표준산업분류 대분류_seller'].isin(urban_codes)) &\n",
    "            (df['7_기업규모코드_seller']==size) &\n",
    "            (df['13_기업규모코드_buyer']==size)))\n",
    "\n",
    "    # 7) Derive year directly from data (not filename)\n",
    "    year = int(df['1_기준연도'].mode().iloc[0]) if len(df) else -1\n",
    "    print_save('1_geo_network.txt', f\"\\n\\n====== {year} ======\")\n",
    "\n",
    "    # 8) Loop through groups, mask, and save per year\n",
    "    for group_name, group_con in groups:\n",
    "        print_save('1_geo_network.txt', f\"\\n====== {group_name} ======\")\n",
    "\n",
    "        df_group = df if group_con is None else df[group_con]\n",
    "\n",
    "        # Count total transactions by (year, seller SGG, buyer SGG)\n",
    "        df_temp = (\n",
    "            df_group\n",
    "            .groupby(['1_기준연도','8_시군구코드_seller','14_시군구코드_buyer'], dropna=False)\n",
    "            .size()\n",
    "            .reset_index(name='총거래관계')\n",
    "        )\n",
    "        df_temp['총거래관계'] = df_temp['총거래관계'].astype(int)\n",
    "\n",
    "        print_save('1_geo_network.txt', f'original links: {len(df_temp)}')\n",
    "\n",
    "        # Save per-year network files\n",
    "        out_pref = f\"data/processed/deal_network/deal_by/year_by/network_by_{group_name}_{year}\"\n",
    "        df_temp.to_csv(out_pref + \".csv\", index=False)\n",
    "        \n",
    "missings_df[['year','Columns','Stats','Kodata','diff']].to_csv(\"data/processed/deal_network/size_ind_missings.csv\", index=False)\n",
    "\n",
    "# 30m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335d47ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_dir = \"data/processed/deal_network/deal_by/year_by\"\n",
    "out_dir  = \"data/processed/deal_network/deal_by\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# Use only per-year original files from year_by (exclude masked versions)\n",
    "paths = [p for p in glob.glob(os.path.join(year_dir, \"network_by_*_*.csv\"))]\n",
    "\n",
    "# Extract group and year from filenames (group = all text before the last underscore)\n",
    "rows = []\n",
    "for p in paths:\n",
    "    fname = os.path.basename(p)\n",
    "    m = re.match(r\"network_by_(.+)_(\\d{4})\\.csv$\", fname)\n",
    "    if m:\n",
    "        rows.append((p, m.group(1), int(m.group(2))))\n",
    "idx = pd.DataFrame(rows, columns=[\"path\",\"group\",\"year\"])\n",
    "\n",
    "if idx.empty:\n",
    "    print(\"No yearly network files found:\", year_dir)\n",
    "\n",
    "for g in sorted(idx[\"group\"].unique()):\n",
    "    sub = idx[idx[\"group\"] == g]\n",
    "\n",
    "    parts = []\n",
    "    for p in sub[\"path\"]:\n",
    "        df_y = pd.read_csv(\n",
    "            p,\n",
    "            dtype={\n",
    "                \"1_기준연도\": \"int64\",\n",
    "                \"8_시군구코드_seller\": str,\n",
    "                \"14_시군구코드_buyer\": str,\n",
    "                \"총거래관계\": \"int64\",\n",
    "            },\n",
    "        )\n",
    "        # Keep only required columns\n",
    "        df_y = df_y[[\"1_기준연도\",\"8_시군구코드_seller\",\"14_시군구코드_buyer\",\"총거래관계\"]]\n",
    "        parts.append(df_y)\n",
    "\n",
    "    if not parts:\n",
    "        continue\n",
    "\n",
    "    df_all = pd.concat(parts, ignore_index=True)\n",
    "\n",
    "    # Aggregate OD flows including year\n",
    "    df_agg = (\n",
    "        df_all\n",
    "        .groupby([\"1_기준연도\",\"8_시군구코드_seller\",\"14_시군구코드_buyer\"], dropna=False)[\"총거래관계\"]\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "        .astype({\"1_기준연도\":\"int64\",\"총거래관계\":\"int64\"})\n",
    "        .sort_values([\"1_기준연도\",\"8_시군구코드_seller\",\"14_시군구코드_buyer\"])\n",
    "    )\n",
    "\n",
    "    # Save aggregated results (with year included)\n",
    "    out = os.path.join(out_dir, f\"network_by_{g}.csv\")\n",
    "    df_agg.to_csv(out, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firm-transaction-network",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
