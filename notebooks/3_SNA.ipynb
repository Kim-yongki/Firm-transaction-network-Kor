{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bf2450b",
   "metadata": {},
   "source": [
    "## 0. Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285dab77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager as fm\n",
    "from matplotlib.offsetbox import AnnotationBbox, OffsetImage\n",
    "import matplotlib.image as mimg\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "import networkx as nx\n",
    "os.chdir(os.getcwd())\n",
    "\n",
    "# --- Set current working directory ---\n",
    "# Change the working directory to the project root\n",
    "import os\n",
    "if os.getcwd().endswith('notebooks'):\n",
    "    os.chdir('..')\n",
    "\n",
    "\n",
    "# 1) Specify the font path using absolute paths (adjust filename/path as needed)\n",
    "font_regular = os.path.abspath(\"assets/malgun.ttf\")     # Regular\n",
    "font_bold    = os.path.abspath(\"assets/malgunbd.ttf\")   # Bold\n",
    "\n",
    "# 3) Register fonts with Matplotlib font manager\n",
    "fm.fontManager.addfont(font_regular)\n",
    "fm.fontManager.addfont(font_bold)\n",
    "try:\n",
    "    # Rebuild cache (not always necessary depending on version)\n",
    "    fm._rebuild()  # Private method, but used here as a workaround\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# 4) Safely get the family name from the font file and apply globally\n",
    "fam = fm.FontProperties(fname=font_regular).get_name()  # Expected: 'Malgun Gothic'\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": fam,\n",
    "    \"axes.unicode_minus\": False,  # Prevent minus sign from being rendered incorrectly\n",
    "})\n",
    "\n",
    "ARROW_FILE = \"assets/north_arrow.png\"  \n",
    "def add_north_arrow(ax, x, y, arrow_file, zoom=0.05):\n",
    "    \"\"\"Add a north arrow image to the plot.\"\"\"\n",
    "    im = mimg.imread(arrow_file)\n",
    "    ax.add_artist(AnnotationBbox(OffsetImage(im, zoom=zoom), (x, y), \n",
    "                                 xycoords='axes fraction', frameon=False))\n",
    "\n",
    "def add_scale_bar(ax, length, location=(0.1,0.02), linewidth=3, color='black'):\n",
    "    \"\"\"Add a scale bar to the plot.\"\"\"\n",
    "    xlim, ylim = ax.get_xlim(), ax.get_ylim()\n",
    "    sb_x = xlim[0] + (xlim[1]-xlim[0])*location[0]\n",
    "    sb_y = ylim[0] + (ylim[1]-ylim[0])*location[1]\n",
    "    ax.plot([sb_x, sb_x+length], [sb_y, sb_y], color=color, linewidth=linewidth)\n",
    "    ax.text(sb_x+length/2, sb_y, f'{round(length/1000):,} km', \n",
    "            va='bottom', ha='center', fontsize=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64907dd",
   "metadata": {},
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeccd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all matching CSV files\n",
    "all_files = glob.glob(\"data/processed/deal_network/deal_by/network_by_*.csv\")\n",
    "\n",
    "# Create a dictionary to store DataFrames\n",
    "nw_dict = {}\n",
    "\n",
    "for file in all_files:\n",
    "    # Extract key from file name, e.g., 'network_by_age (man).csv' → 'age (man)'\n",
    "    key = os.path.basename(file).replace(\"network_by_\", \"\").replace(\".csv\", \"\")\n",
    "    \n",
    "    # Read the CSV file\n",
    "    nw = pd.read_csv(file, dtype={'14_시군구코드_buyer':str, '8_시군구코드_seller':str, '1_기준연도':int})\n",
    "    \n",
    "    nw = nw[~nw['14_시군구코드_buyer'].str.startswith('9999')&~nw['8_시군구코드_seller'].str.startswith('9999')]\n",
    "    nw.columns = ['기준연도', '시군구코드_seller', '시군구코드_buyer', '거래관계']\n",
    "    \n",
    "    # Store DataFrame in dictionary\n",
    "    nw_dict[key] = nw\n",
    "\n",
    "desired_order = [\n",
    "    'all',\n",
    "    'man',\n",
    "    'innovation',\n",
    "    'urban_size_소상공인',\n",
    "    'urban_size_중소기업',\n",
    "    'urban_size_중견기업',\n",
    "    'urban_size_대기업',\n",
    "    'urban_age_1년 미만',\n",
    "    'urban_age_1~5년 미만',\n",
    "    'urban_age_5~10년 미만',\n",
    "    'urban_age_10년 이상'\n",
    "]\n",
    "\n",
    "# 2. 새로운 딕셔너리를 만들어 순서를 적용합니다.\n",
    "#    (혹시 모를 에러를 방지하기 위해 nw_dict에 실제 있는 키만 가져옵니다.)\n",
    "ordered_nw_dict = {key: nw_dict[key] for key in desired_order if key in nw_dict}\n",
    "\n",
    "# 3. 원래 nw_dict를 정렬된 딕셔너리로 교체합니다.\n",
    "nw_dict = ordered_nw_dict\n",
    "\n",
    "SGG_map = gpd.read_file('data/processed/map/SGG_map.gpkg').to_crs(epsg=4326)\n",
    "SGG_map = SGG_map[['SIG_KOR_NM', 'SIG_CD', 'geometry'] + [col for col in SGG_map.columns if col.endswith('pop')]]\n",
    "\n",
    "# 표준화된 컬럼 생성\n",
    "SGG_map['SIGUNGU_NM'] = SGG_map['SIG_KOR_NM']\n",
    "SGG_map['SIGUNGU_CD'] = SGG_map['SIG_CD']\n",
    "SGG_map['adm_nm']     = SGG_map['SIG_KOR_NM']\n",
    "\n",
    "SGG_map = SGG_map[[ 'SIGUNGU_CD', 'SIGUNGU_NM', 'geometry'] + [col for col in SGG_map.columns if col.endswith('pop')]]\n",
    "SGG_map.to_file('data/processed/map/SGG_map_zone.gpkg', driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc54e6ff",
   "metadata": {},
   "source": [
    "## 3. Network centrality calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e184ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "SGG_map = gpd.read_file('data/processed/map/SGG_map_zone.gpkg').to_crs(epsg=4326)\n",
    "SIDO_NM_map = {\n",
    "    '11': '서울특별시', '26': '부산광역시', '27': '대구광역시', '28': '인천광역시',\n",
    "    '29': '광주광역시', '30': '대전광역시', '31': '울산광역시', '36': '세종특별자치시',\n",
    "    '41': '경기도', '51': '강원특별자치도', '43': '충청북도', '44': '충청남도',\n",
    "    '52': '전북특별자치도', '46': '전라남도', '47': '경상북도', '48': '경상남도',\n",
    "    '50': '제주특별자치도'\n",
    "}\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ====== User-adjustable parameters ======\n",
    "out_tbl_dir = \"outputs/tables/centralities\"\n",
    "weights = [\"거래관계\"]   # Can be [\"거래횟수\", \"거래관계\", \"거래액\"], etc.\n",
    "drop_nodes = [\"9999\"]   # Codes to be masked/removed\n",
    "# ========================================\n",
    "\n",
    "# Prepare output directory\n",
    "os.makedirs(out_tbl_dir, exist_ok=True)\n",
    "\n",
    "# ── Automatically detect code column in SGG_map (only once) ──\n",
    "_code_candidates = [\"SIGUNGU_CD\", \"SIG_CD\", \"ADM_CD\", \"code\", \"CODE\"]\n",
    "_code_col = next((c for c in _code_candidates if c in SGG_map.columns), None)\n",
    "if _code_col is None:\n",
    "    raise ValueError(\"No administrative code column found in SGG_map. (e.g., SIGUNGU_CD, SIG_CD)\")\n",
    "\n",
    "_all_zone_nodes = (\n",
    "    SGG_map[_code_col].astype(str).str.zfill(5).unique().tolist()\n",
    ")\n",
    "\n",
    "# 2) Execution (procedural, no separate module/function)\n",
    "_eps = 1e-9\n",
    "_drop_set_orig = set(str(x) for x in (drop_nodes or []))\n",
    "_drop_set_5 = set([s.zfill(5) for s in _drop_set_orig])\n",
    "_drop_both = set(_drop_set_orig) | set(_drop_set_5)\n",
    "\n",
    "\n",
    "for key, df in nw_dict.items():\n",
    "    if not isinstance(df, pd.DataFrame) or df.empty:\n",
    "        print(f\"[WARN] '{key}' is empty — skipped\")\n",
    "        continue\n",
    "    if \"기준연도\" not in df.columns:\n",
    "        raise ValueError(f\"[{key}] Missing '기준연도' column.\")\n",
    "\n",
    "    years = sorted(pd.unique(df[\"기준연도\"].dropna()))\n",
    "    print(f\"▶ Network '{key}' — {len(years)} years, {len(df):,} raw edges\")\n",
    "\n",
    "    for w in weights:\n",
    "        if w not in df.columns:\n",
    "            print(f\"  - [WARN] '{key}' has no weight column '{w}' — skipped\")\n",
    "            continue\n",
    "        subdir = os.path.join(out_tbl_dir, key, w)\n",
    "        os.makedirs(subdir, exist_ok=True)\n",
    "\n",
    "        # ── Process per-year ─────────────────────────────────────────────\n",
    "        for y in years:\n",
    "            if os.path.exists(os.path.join(subdir, f\"centrality_{y}.csv\")):\n",
    "                print(f\"  - [{key}/{w}/{y}] Already processed — skipped\")\n",
    "                continue\n",
    "\n",
    "            sub = df.loc[df[\"기준연도\"] == y, [\"시군구코드_seller\", \"시군구코드_buyer\", w]].copy()\n",
    "            if sub.empty:\n",
    "                out_csv = os.path.join(subdir, f\"centrality_{y}.csv\")\n",
    "                pd.DataFrame(columns=[\n",
    "                    \"node\",\"in_degree\",\"out_degree\",\"in_strength\",\"out_strength\",\n",
    "                    \"in_degree_c\",\"out_degree_c\",\"closeness\",\"betweenness\",\"eigenvector\",\"pagerank\"\n",
    "                ]).to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "                print(f\"  - [{key}/{w}/{y}] Empty year — saved empty CSV\")\n",
    "                continue\n",
    "\n",
    "            # Aggregate (seller, buyer) + remove self-loops\n",
    "            sub[w] = pd.to_numeric(sub[w], errors=\"coerce\").fillna(0.0).astype(float)\n",
    "            sub = sub.dropna(subset=[\"시군구코드_seller\", \"시군구코드_buyer\"]) \n",
    "            agg = (\n",
    "                sub.groupby([\"시군구코드_seller\", \"시군구코드_buyer\"], as_index=False)[w]\n",
    "                   .sum()\n",
    "            )\n",
    "            agg = agg[agg[\"시군구코드_seller\"] != agg[\"시군구코드_buyer\"]].reset_index(drop=True)\n",
    "\n",
    "            # Build directed graph (DiGraph) + define length\n",
    "            G = nx.DiGraph()\n",
    "            for _, r in agg.iterrows():\n",
    "                u = r[\"시군구코드_seller\"]\n",
    "                v = r[\"시군구코드_buyer\"]\n",
    "                weight_val = float(max(0.0, r[w]))\n",
    "                if G.has_edge(u, v):\n",
    "                    G[u][v][\"weight\"] += weight_val\n",
    "                else:\n",
    "                    G.add_edge(u, v, weight=weight_val)\n",
    "                    \n",
    "            for u, v, data in G.edges(data=True):\n",
    "                ww = float(data.get(\"weight\", 0.0))\n",
    "                data[\"length\"] = 1.0 / (ww + _eps) if ww > 0 else 1.0 / _eps\n",
    "                \n",
    "            # >>> INSERT A: Add nodes from SGG_map (include isolated nodes)\n",
    "            nodes_to_add = [n for n in _all_zone_nodes if n not in _drop_both and n not in G]\n",
    "            if nodes_to_add:\n",
    "                G.add_nodes_from(nodes_to_add)\n",
    "\n",
    "            nodes = list(G.nodes())\n",
    "\n",
    "\n",
    "            # Compute centralities (same logic as module version)\n",
    "            if len(nodes) == 0:\n",
    "                central = pd.DataFrame(columns=[\n",
    "                    \"node\",\"in_strength\",\"out_strength\",\"degree_strength\",\n",
    "                    \"closeness\",\"betweenness\",\"eigenvector\",\"pagerank\"\n",
    "                ])\n",
    "            else:\n",
    "                in_strength = dict(G.in_degree(weight=\"weight\"))\n",
    "                out_strength = dict(G.out_degree(weight=\"weight\"))\n",
    "                degree_strength = {u: in_strength.get(u, 0.0) + out_strength.get(u, 0.0) for u in nodes}\n",
    "\n",
    "                closeness = nx.closeness_centrality(G, distance=\"length\")\n",
    "                betweenness = nx.betweenness_centrality(G, weight=\"length\", normalized=True)\n",
    "                eigen = nx.eigenvector_centrality(G, max_iter=5000, tol=1e-6, weight=\"weight\")\n",
    "                pagerank = nx.pagerank(G, alpha=0.85, weight=\"weight\")\n",
    "\n",
    "\n",
    "                central = pd.DataFrame({\n",
    "                    \"node\": nodes,\n",
    "                    \"in_strength\": [float(in_strength.get(u, 0.0)) for u in nodes],\n",
    "                    \"out_strength\": [float(out_strength.get(u, 0.0)) for u in nodes],\n",
    "                    \"degree_strength\": [float(degree_strength.get(u, 0.0)) for u in nodes],\n",
    "                    \"closeness\": [float(closeness.get(u, 0.0)) for u in nodes],\n",
    "                    \"betweenness\": [float(betweenness.get(u, 0.0)) for u in nodes],\n",
    "                    \"eigenvector\": [float(eigen.get(u, 0.0)) for u in nodes],\n",
    "                    \"pagerank\": [float(pagerank.get(u, 0.0)) for u in nodes],\n",
    "                })\n",
    "            # Apply drop-node filtering\n",
    "            if not central.empty:\n",
    "                central = central[~central[\"node\"].astype(str).isin(_drop_set_orig) & ~central[\"node\"].astype(str).isin(_drop_set_5)]\n",
    "                \n",
    "            central = pd.merge(\n",
    "                central,\n",
    "                SGG_map[['SIGUNGU_CD','SIGUNGU_NM']],\n",
    "                left_on='node', right_on='SIGUNGU_CD', how='left'\n",
    "            )\n",
    "            central['SIDO_CD'] = central['node'].str[:2].str.zfill(2)\n",
    "            central['SIDO_NM'] = central['SIDO_CD'].map(SIDO_NM_map)\n",
    "            central = central.drop(columns=['SIGUNGU_CD', 'SIDO_CD'])\n",
    "\n",
    "            out_csv = os.path.join(subdir, f\"centrality_{y}.csv\")\n",
    "            central.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a468b92e",
   "metadata": {},
   "source": [
    "# 3. network centrality map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a15b1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === User-defined parameters (edit only if needed) ===\n",
    "BASE_DIR = \"outputs/tables/centralities\"   # Root directory for centrality CSV files\n",
    "ORIGIN_DIR = \"outputs/figures/centrality/centralities_origin\"        # Output root for raw maps\n",
    "QUANT_DIR  = \"outputs/figures/centrality/centralities_percentile\"    # Output root for percentile maps\n",
    "CRS_PROJ = \"EPSG:5179\"                       # Recommended: meter-based projection (for scale accuracy)\n",
    "NETS_FILTER    = None                        # Example: [\"all_masked\",\"innovation_masked\"] (None = auto)\n",
    "WEIGHTS_FILTER = None                        # Example: [\"거래액\"] (None = auto)\n",
    "YEARS_FILTER   = None                        # Example: [\"2020\",\"2021\"] or [2020,2021] (None = all)\n",
    "METRICS_FILTER = None                        # None = default metrics + strength\n",
    "CTPRVN = gpd.read_file(\"data/raw/bnd_sido_00_2024_2Q/bnd_sido_00_2024_2Q.shp\").to_crs(epsg=5179)  # Provincial boundaries (optional)\n",
    "\n",
    "# === Load and preprocess SGG_map ===\n",
    "SGG_map_path = \"data/processed/map/SGG_map_zone.gpkg\" \n",
    "SGG_map = gpd.read_file(SGG_map_path)\n",
    "SGG_map[\"SIGUNGU_CD\"] = SGG_map[\"SIGUNGU_CD\"].astype(str).str.zfill(5)\n",
    "if CRS_PROJ:\n",
    "    SGG_map = SGG_map.to_crs(CRS_PROJ)\n",
    "\n",
    "# === Auto-detect network folders (apply filter if specified) ===\n",
    "if NETS_FILTER is None:\n",
    "    NETS = [d for d in os.listdir(BASE_DIR) if os.path.isdir(os.path.join(BASE_DIR, d))]\n",
    "    NETS.sort()\n",
    "else:\n",
    "    NETS = NETS_FILTER\n",
    "\n",
    "# === Main loop (procedural, no functions) ===\n",
    "for net in NETS:\n",
    "    net_dir = os.path.join(BASE_DIR, net)\n",
    "    if not os.path.isdir(net_dir):\n",
    "        continue\n",
    "\n",
    "    # Auto-detect weight folders (apply filter if specified)\n",
    "    if WEIGHTS_FILTER is None:\n",
    "        W_LIST = [d for d in os.listdir(net_dir) if os.path.isdir(os.path.join(net_dir, d))]\n",
    "        W_LIST.sort()\n",
    "    else:\n",
    "        W_LIST = WEIGHTS_FILTER\n",
    "\n",
    "    for w in W_LIST:\n",
    "        w_dir = os.path.join(net_dir, w)\n",
    "        if not os.path.isdir(w_dir):\n",
    "            continue\n",
    "\n",
    "        # Build list of metrics to plot\n",
    "        base_metrics = [\"pagerank\", \"eigenvector\", \"in_strength\", \"out_strength\", \"degree_strength\"]\n",
    "        METRIC_LIST = METRICS_FILTER or (base_metrics)\n",
    "\n",
    "        # Iterate over centrality_*.csv files\n",
    "        for fname in sorted(os.listdir(w_dir)):\n",
    "            if not fname.startswith(\"centrality_\") or not fname.endswith(\".csv\"):\n",
    "                continue\n",
    "\n",
    "            year_label = fname.replace(\"centrality_\", \"\").replace(\".csv\", \"\")\n",
    "            # Apply year filter\n",
    "            if YEARS_FILTER is not None and str(year_label) not in set(map(str, YEARS_FILTER)):\n",
    "                continue\n",
    "\n",
    "            csv_path = os.path.join(w_dir, fname)\n",
    "            central = pd.read_csv(csv_path)\n",
    "            if central.empty or \"node\" not in central.columns:\n",
    "                continue\n",
    "            central[\"node\"] = central[\"node\"].astype(str).str.zfill(5)\n",
    "\n",
    "            # Join with SGG_map\n",
    "            g = SGG_map.merge(central, left_on=\"SIGUNGU_CD\", right_on=\"node\", how=\"left\")\n",
    "\n",
    "            # Prepare output folders\n",
    "            raw_out_dir = os.path.join(ORIGIN_DIR, net)     # ← Root for raw maps\n",
    "            quant_out_dir = os.path.join(QUANT_DIR, net)    # ← Root for percentile maps\n",
    "            os.makedirs(raw_out_dir, exist_ok=True)\n",
    "            os.makedirs(quant_out_dir, exist_ok=True)\n",
    "\n",
    "            for metric in METRIC_LIST:\n",
    "                if metric not in g.columns:\n",
    "                    continue\n",
    "\n",
    "                safe_metric = metric.replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n",
    "                raw_png   = os.path.join(raw_out_dir,   f\"map_{safe_metric}_{year_label}.png\")\n",
    "                quant_png = os.path.join(quant_out_dir, f\"map_{safe_metric}_{year_label}.png\")\n",
    "\n",
    "                # ---- RAW value map ----\n",
    "                if os.path.exists(raw_png):\n",
    "                    print(f\"[OK] {raw_png} already exists\")\n",
    "                    continue\n",
    "\n",
    "                fig, ax = plt.subplots(figsize=(10, 9), dpi=100)\n",
    "                g.plot(\n",
    "                    column=metric,\n",
    "                    cmap=\"coolwarm\",\n",
    "                    linewidth=0.3,\n",
    "                    edgecolor=\"white\",\n",
    "                    legend=True,\n",
    "                    ax=ax,\n",
    "                    missing_kwds={\"color\": \"#f0f0f0\", \"label\": \"No data\"},\n",
    "                )\n",
    "                CTPRVN.plot(ax=ax, color=\"none\", edgecolor=\"black\", linewidth=1)\n",
    "                ax.set_title(f\"[{net}] {w} — {metric} ({year_label})\", fontsize=14)\n",
    "                add_north_arrow(ax, x=0.92, y=0.92, arrow_file=ARROW_FILE, zoom=0.2)\n",
    "                add_scale_bar(ax, length=100000, location=(0.1, 0.02))\n",
    "                ax.axis(\"off\")\n",
    "                plt.savefig(raw_png, bbox_inches=\"tight\")\n",
    "                #plt.show()\n",
    "                plt.close()\n",
    "\n",
    " \n",
    "                # ---- Percentile map (fixed at 0–100) ----\n",
    "                if os.path.exists(quant_png):\n",
    "                    print(f\"[OK] {quant_png} already exists\")\n",
    "                    continue\n",
    "\n",
    "                # Compute percentiles while preserving NaN\n",
    "                pct = g[metric].rank(pct=True) * 100\n",
    "                fig, ax = plt.subplots(figsize=(10, 9), dpi=100)\n",
    "                g.assign(_pct=pct).plot(\n",
    "                    column=\"_pct\",\n",
    "                    cmap=\"coolwarm\",\n",
    "                    linewidth=0.3,\n",
    "                    edgecolor=\"white\",\n",
    "                    legend=True,\n",
    "                    legend_kwds={\"label\": f\"{metric} percentile (%)\", \"orientation\": \"vertical\"},\n",
    "                    ax=ax,\n",
    "                    missing_kwds={\"color\": \"#f0f0f0\", \"label\": \"No data\"},\n",
    "                    vmin=0, vmax=100\n",
    "                )\n",
    "                CTPRVN.plot(ax=ax, color=\"none\", edgecolor=\"black\", linewidth=1)\n",
    "                ax.set_title(f\"[{net}] {w} — {metric} ({year_label})\", fontsize=14)\n",
    "                add_north_arrow(ax, x=0.92, y=0.92, arrow_file=ARROW_FILE, zoom=0.2)\n",
    "                add_scale_bar(ax, length=100000, location=(0.1, 0.02))\n",
    "                ax.axis(\"off\")\n",
    "                plt.savefig(quant_png, bbox_inches=\"tight\")\n",
    "                #plt.show()\n",
    "                plt.close()\n",
    "\n",
    "                print(f\"[INFO] {net}/{w}/{year_label} - {metric} map saved\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firm-transaction-network",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
