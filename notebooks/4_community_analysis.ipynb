{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d1b539a",
   "metadata": {},
   "source": [
    "## 0. Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285dab77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Standard library\n",
    "import os\n",
    "import gc\n",
    "import glob\n",
    "import re\n",
    "import textwrap\n",
    "import sys\n",
    "from collections import Counter\n",
    "from typing import Dict, Set, List, Tuple\n",
    "\n",
    "# 2. Third-party libraries\n",
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# 3. Visualization libraries (Matplotlib)\n",
    "import matplotlib.image as img\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors as mcolors\n",
    "from matplotlib import font_manager as fm\n",
    "from matplotlib.offsetbox import AnnotationBbox, OffsetImage\n",
    "from matplotlib.patches import Patch, PathPatch, Rectangle\n",
    "from matplotlib.path import Path\n",
    "from matplotlib.colors import to_rgba\n",
    "\n",
    "\n",
    "sys.path.append(\"/home/pauluhill/Projects/2025_Kor_transaction/src\")\n",
    "from network_analysis import NetworkAnalyzer\n",
    "\n",
    "\n",
    "# --- Set current working directory ---\n",
    "# If running from notebooks/, move up to the project root\n",
    "if os.getcwd().endswith('notebooks'):\n",
    "    os.chdir('..')\n",
    "\n",
    "# --- Matplotlib Korean font setup ---\n",
    "# 1) Specify font file paths (absolute paths)\n",
    "font_regular = os.path.abspath(\"assets/malgun.ttf\")   # Malgun Gothic (regular)\n",
    "font_bold = os.path.abspath(\"assets/malgunbd.ttf\")    # Malgun Gothic (bold)\n",
    "\n",
    "fm.fontManager.addfont(font_regular)\n",
    "fm.fontManager.addfont(font_bold)\n",
    "\n",
    "# 3) Apply global font settings\n",
    "fam = fm.FontProperties(fname=font_regular).get_name()  # e.g., 'Malgun Gothic'\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": fam,\n",
    "    \"axes.unicode_minus\": False,  # prevent minus sign rendering issues\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efd4112",
   "metadata": {},
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeccd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Data Loading\n",
    "\n",
    "# --- Get list of CSV files and sort them in desired order ---\n",
    "all_files = glob.glob(\"data/processed/deal_network/deal_by/network_by_*.csv\")\n",
    "desired_order = [\n",
    "    'all', 'man', 'innovation',\n",
    "    'urban_size_소상공인', 'urban_size_중소기업', 'urban_size_중견기업', 'urban_size_대기업',\n",
    "    'urban_age_1년 미만', 'urban_age_1~5년 미만', 'urban_age_5~10년 미만', 'urban_age_10년 이상'\n",
    "]\n",
    "# Reconstruct file list according to the desired order\n",
    "file_order_map = {os.path.basename(f).replace(\"network_by_\", \"\").replace(\".csv\", \"\"): f for f in all_files}\n",
    "sorted_files = [file_order_map[key] for key in desired_order if key in file_order_map]\n",
    "\n",
    "# --- Load each network data into a DataFrame and store in a dictionary ---\n",
    "nw_dict = {}\n",
    "print(\"\\nStarting to load network data...\")\n",
    "for file in sorted_files:\n",
    "    key = os.path.basename(file).replace(\"network_by_\", \"\").replace(\".csv\", \"\")\n",
    "    try:\n",
    "        nw = pd.read_csv(\n",
    "            file,\n",
    "            dtype={'14_시군구코드_buyer': str, '8_시군구코드_seller': str, '1_기준연도': int}\n",
    "        )\n",
    "        # Remove rows with invalid codes ('9999')\n",
    "        nw = nw[~nw['14_시군구코드_buyer'].str.startswith('9999') & ~nw['8_시군구코드_seller'].str.startswith('9999')].copy()\n",
    "        nw.columns = ['기준연도', '시군구코드_seller', '시군구코드_buyer', '거래관계']\n",
    "        nw_dict[key] = nw\n",
    "        print(f\" - {key} successfully loaded\")\n",
    "    except Exception as e:\n",
    "        print(f\"File loading error: {file}, error: {e}\")\n",
    "\n",
    "print(\"\\nNetwork data loading and sorting completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b316a37",
   "metadata": {},
   "source": [
    "## 2. Common Utility Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799b001f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARROW_FILE = 'assets/north_arrow.png'\n",
    "def add_north_arrow(ax, x, y, arrow_file, zoom=0.05):\n",
    "    \"\"\"Add a north arrow to the map.\"\"\"\n",
    "    if os.path.exists(arrow_file):\n",
    "        im = img.imread(arrow_file)\n",
    "        ax.add_artist(AnnotationBbox(OffsetImage(im, zoom=zoom), (x, y), xycoords='axes fraction', frameon=False))\n",
    "\n",
    "SCALE_LEN_M = 100_000\n",
    "def add_scale_bar(ax, length, location=(0.1, 0.02), linewidth=3, color='black'):\n",
    "    \"\"\"Add a scale bar to the map.\"\"\"\n",
    "    xlim, ylim = ax.get_xlim(), ax.get_ylim()\n",
    "    sb_x = xlim[0] + (xlim[1] - xlim[0]) * location[0]\n",
    "    sb_y = ylim[0] + (ylim[1] - ylim[0]) * location[1]\n",
    "    ax.plot([sb_x, sb_x + length], [sb_y, sb_y], color=color, linewidth=linewidth)\n",
    "    ax.text(sb_x + length / 2, sb_y, f'{round(length / 1000):,} km', va='bottom', ha='center', fontsize=10)\n",
    "\n",
    "\n",
    "MAX_ID = 230\n",
    "NO_DATA = (0.92, 0.92, 0.92, 1.0)\n",
    "\n",
    "def build_color_table(max_id=600):\n",
    "    \"\"\"Generate a color table for unique IDs.\"\"\"\n",
    "    pal = list(plt.get_cmap(\"Set1\").colors) + \\\n",
    "        list(plt.get_cmap(\"Set2\").colors) + \\\n",
    "        list(plt.get_cmap(\"Dark2\").colors)\n",
    "    return {cid: mcolors.to_rgba(pal[cid % len(pal)], 1.0) for cid in range(max_id)}\n",
    "\n",
    "COLOR_OF = build_color_table(600)\n",
    "\n",
    "def groups_from_series(s: pd.Series) -> Dict[int, Set[str]]:\n",
    "    \"\"\"Convert a Pandas Series into a dictionary {group_id: {node_set}}.\"\"\"\n",
    "    s = s.dropna().astype(int)\n",
    "    return {int(lab): set(idxs) for lab, idxs in s.groupby(s).groups.items()}\n",
    "\n",
    "def bump(mem: Dict[int, Counter], canon_id: int, nodes: Set[str], w: float = 1.0):\n",
    "    \"\"\"Update memory (community membership information) for a community ID.\"\"\"\n",
    "    cw = mem.setdefault(canon_id, Counter())\n",
    "    for n in nodes:\n",
    "        cw[n] += w\n",
    "\n",
    "def decay_memory(mem: Dict[int, Counter], rho: float):\n",
    "    \"\"\"Decay memory weights over time (rho < 1).\"\"\"\n",
    "    if rho >= 0.9999: return\n",
    "    for c in list(mem.keys()):\n",
    "        cw = mem[c]\n",
    "        for k in list(cw.keys()):\n",
    "            cw[k] *= rho\n",
    "            if cw[k] < 1e-9:\n",
    "                del cw[k]\n",
    "\n",
    "def weighted_jaccard(A_nodes: Set[str], canon_counter: Counter) -> float:\n",
    "    \"\"\"Compute weighted Jaccard similarity.\"\"\"\n",
    "    if not A_nodes: return 0.0\n",
    "    inter = sum(canon_counter.get(x, 0.0) for x in A_nodes)\n",
    "    denom = len(A_nodes) + sum(canon_counter.values()) - inter\n",
    "    return (inter / denom) if denom > 0 else 0.0\n",
    "\n",
    "def prev_year_guard(A: Set[str], prev_series: pd.Series, cand_canon: int) -> Tuple[float, float, float]:\n",
    "    \"\"\"Compare with previous year's community to compute Precision, Recall, and Expansion ratios.\"\"\"\n",
    "    if prev_series is None or prev_series.empty: return 0.0, 0.0, float(\"inf\")\n",
    "    prev_s = prev_series.dropna().astype(int)\n",
    "    mask = prev_s == cand_canon\n",
    "    if mask.sum() == 0: return 0.0, 0.0, float(\"inf\")\n",
    "    B = set(prev_s.index[mask])\n",
    "    inter = len(A & B)\n",
    "    precision = inter / len(A) if A else 0.0\n",
    "    recall = inter / len(B) if B else 0.0\n",
    "    expansion = len(A) / len(B) if B else float(\"inf\")\n",
    "    return precision, recall, expansion\n",
    "\n",
    "def safe_name(s: str) -> str:\n",
    "    \"\"\"Convert a string into a safe format for saving files.\"\"\"\n",
    "    return re.sub(r\"[^0-9A-Za-z가-힣_.\\-]+\", \"_\", str(s))\n",
    "\n",
    "def wrap_label(txt: str, width=12, max_lines=2) -> str:\n",
    "    \"\"\"Wrap long text into lines of specified width and maximum number of lines.\"\"\"\n",
    "    lines = textwrap.wrap(txt, width=width)\n",
    "    if len(lines) > max_lines:\n",
    "        lines = lines[:max_lines]\n",
    "        lines[-1] = lines[-1][:max(0, width - 1)] + \"…\"\n",
    "    return \"\\n\".join(lines) if lines else \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ba8b20",
   "metadata": {},
   "source": [
    "## 3. community detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f0e990",
   "metadata": {},
   "source": [
    "### 3.1. detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c2caf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis parameter setup\n",
    "SGG_MAP_PATH = \"data/processed/map/SGG_map_zone.gpkg\"\n",
    "COMMUNITY_METHODS = ['Infomap', \"Leiden\", \"Louvain\"]\n",
    "WEIGHTS = [\"거래관계\"]\n",
    "MIN_MODULE_NODES = 5  # Minimum community size (filter out if smaller)\n",
    "\n",
    "# Execution loop\n",
    "print(\"\\n=== Step 4: Start community detection ===\")\n",
    "SGG_base = gpd.read_file(SGG_MAP_PATH)\n",
    "\n",
    "for weight in WEIGHTS:\n",
    "    for key, df in nw_dict.items():\n",
    "        print(f\"[{key}] {weight} community analysis started\")\n",
    "        summary_rows = []\n",
    "\n",
    "        # Standardize edge data\n",
    "        tmp = (\n",
    "            df.rename(columns={\n",
    "                \"시군구코드_seller\": \"source\", \"시군구코드_buyer\": \"target\", weight: \"weights\",\n",
    "            })[[\"기준연도\", \"source\", \"target\", \"weights\"]]\n",
    "            .dropna(subset=[\"source\", \"target\", \"weights\"])\n",
    "        )\n",
    "        tmp[\"source\"] = tmp[\"source\"].astype(str)\n",
    "        tmp[\"target\"] = tmp[\"target\"].astype(str)\n",
    "        tmp = tmp[tmp[\"source\"] != '9999'].copy()\n",
    "        tmp = tmp[tmp[\"target\"] != '9999'].copy()\n",
    "        tmp[\"weights\"] = pd.to_numeric(tmp[\"weights\"], errors=\"coerce\").fillna(0)\n",
    "        tmp = tmp.groupby([\"기준연도\", \"source\", \"target\"], as_index=False)[\"weights\"].sum()\n",
    "        converted = {int(y): d[[\"source\", \"target\", \"weights\"]].copy() for y, d in tmp.groupby(\"기준연도\")}\n",
    "\n",
    "        # Build graphs and run community detection\n",
    "        analyzer = NetworkAnalyzer()\n",
    "        analyzer.polygon_files[\"unizone1\"] = SGG_MAP_PATH\n",
    "        graphs = analyzer.create_graphs_from_data(converted, include_self=True, thres=100)\n",
    "\n",
    "        out_tbl_dir = f\"data/processed/communities/{key}\"\n",
    "        out_map_dir = f\"data/processed/map/communities/{key}\"\n",
    "        os.makedirs(out_tbl_dir, exist_ok=True)\n",
    "        os.makedirs(out_map_dir, exist_ok=True)\n",
    "        SGG_map_result = SGG_base.copy()\n",
    "\n",
    "        for method in COMMUNITY_METHODS:\n",
    "            results = analyzer.analyze_communities_all_years(graphs, method=method)\n",
    "            for year in sorted(results.keys()):\n",
    "                cdf = results.get(year)\n",
    "                G = graphs[year]\n",
    "\n",
    "                # Filter out communities smaller than the minimum size\n",
    "                sizes = cdf[\"module_id\"].value_counts()\n",
    "                small_modules = sizes[sizes < MIN_MODULE_NODES].index.tolist()\n",
    "                if small_modules:\n",
    "                    cdf.loc[cdf['module_id'].isin(small_modules), 'module_id'] = np.nan\n",
    "\n",
    "                # Merge results into GeoDataFrame\n",
    "                SGG_map_result = SGG_map_result.merge(\n",
    "                    cdf[[\"name\", \"module_id\"]].rename(columns={\"name\": \"SIGUNGU_CD\", \"module_id\": f\"{method}_{year}\"}),\n",
    "                    on=\"SIGUNGU_CD\", how=\"left\"\n",
    "                )\n",
    "\n",
    "                # Compute and add summary info\n",
    "                valid_cdf = cdf.dropna(subset=['module_id'])\n",
    "                if not valid_cdf.empty:\n",
    "                    sizes = valid_cdf[\"module_id\"].value_counts().sort_values(ascending=False)\n",
    "                    hhi = float(((sizes / sizes.sum())**2).sum())\n",
    "                    summary_rows.append({\"dataset\": key, \"year\": year, \"method\": method, \"nodes\": G.number_of_nodes(), \"edges\": G.number_of_edges(), \"modules\": int(valid_cdf[\"module_id\"].nunique()), \"largest_module_size\": int(sizes.iloc[0]), \"module_concentration(HHI)\": round(hhi, 4)})\n",
    "                else:\n",
    "                    summary_rows.append({\"dataset\": key, \"year\": year, \"method\": method, \"nodes\": G.number_of_nodes(), \"edges\": G.number_of_edges(), \"modules\": 0, \"largest_module_size\": 0, \"module_concentration(HHI)\": 0.0})\n",
    "\n",
    "\n",
    "        # Save results\n",
    "        gpkg_path = f\"{out_map_dir}/{weight}_SGG_map_zone_community.gpkg\"\n",
    "        SGG_map_result.to_file(gpkg_path, driver=\"GPKG\")\n",
    "        if summary_rows:\n",
    "            pd.DataFrame(summary_rows).sort_values([\"dataset\", \"method\", \"year\"]).to_csv(f\"{out_tbl_dir}/{weight}_summary.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"[OK] Saved → {gpkg_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9334b8",
   "metadata": {},
   "source": [
    "### 3.2. Canonicalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faaac5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis parameter setup\n",
    "COMMUNITY_METHODS = ['Infomap', \"Leiden\", \"Louvain\"]\n",
    "WEIGHTS = [\"거래관계\"]\n",
    "BASE_DIR = \"data/processed/map/communities\"\n",
    "OUT_DIR  = \"outputs/gpkg/communities\"\n",
    "if not os.path.exists(OUT_DIR):\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Memory/matching thresholds\n",
    "MEM_BASE_THRESH = 0.3    # Minimum similarity for memory-based matching\n",
    "MEM_REVIVE_THRESH = 0.3  # Minimum similarity to revive a past ID\n",
    "RHO = 1.0                # Memory decay rate (1.0 = cumulative, 0.9 = 10% decay per year)\n",
    "\n",
    "# Previous-year priority matching conditions (to avoid abnormal merges)\n",
    "PREV_PREC_MIN = 0.55     # Precision: % of this year’s members that were in last year’s community\n",
    "PREV_RECALL_MIN = 0.35   # Recall: % of last year’s members retained this year\n",
    "EXPANSION_CAP_PREV = 2.0 # Expansion cap (this year size / last year size)\n",
    "\n",
    "# Conditions for reviving past IDs\n",
    "EXPANSION_CAP_REVIVE = 2.0  # Expansion cap (this year size / last seen size)\n",
    "\n",
    "# Execution loop\n",
    "print(\"\\n=== Step 5: Start community ID canonicalization ===\")\n",
    "for key in os.listdir(BASE_DIR):\n",
    "    sub = os.path.join(BASE_DIR, key)\n",
    "    if not os.path.isdir(sub): \n",
    "        continue\n",
    "\n",
    "    for weight in WEIGHTS:\n",
    "        gpkg = os.path.join(sub, f\"{weight}_SGG_map_zone_community.gpkg\")\n",
    "        if not os.path.exists(gpkg): \n",
    "            continue\n",
    "        out_dir = os.path.join(OUT_DIR, key)\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        out_gpkg = os.path.join(out_dir, f\"{weight}_SGG_map_zone_community.gpkg\")\n",
    "        \n",
    "        \n",
    "        print(f\"[{key}] Processing {weight}...\")\n",
    "        g = gpd.read_file(gpkg).set_index(\"SIGUNGU_CD\", drop=False)\n",
    "\n",
    "        for method in COMMUNITY_METHODS:\n",
    "            cols = [c for c in g.columns if c.startswith(f\"{method}_\") and \"_canon_\" not in c]\n",
    "            years = sorted({int(c.split(\"_\")[-1]) for c in cols})\n",
    "            if not years: \n",
    "                continue\n",
    "\n",
    "            raw_series = {y: g[f\"{method}_{y}\"] for y in years}\n",
    "            canon_parts, memory, last_seen_size = {}, {}, {}\n",
    "            next_id = 0\n",
    "\n",
    "            # Initialize with the first year\n",
    "            y0 = years[0]\n",
    "            g0 = groups_from_series(raw_series[y0])\n",
    "            init_map = {r: i for i, r in enumerate(sorted(g0))}\n",
    "            canon_parts[y0] = raw_series[y0].map(init_map)\n",
    "            for r, cid in init_map.items():\n",
    "                bump(memory, cid, g0[r])\n",
    "                last_seen_size[cid] = len(g0[r])\n",
    "            next_id = len(init_map)\n",
    "            prev_canon_series = canon_parts[y0]\n",
    "\n",
    "            # Sequential matching for subsequent years\n",
    "            for y in years[1:]:\n",
    "                decay_memory(memory, RHO)\n",
    "                curr_groups = groups_from_series(raw_series[y])\n",
    "                raws, canons = sorted(curr_groups.keys()), sorted(memory.keys())\n",
    "                mapping, used_canons = {}, set()\n",
    "\n",
    "                # Priority matching with previous year (greedy)\n",
    "                prev_groups = groups_from_series(prev_canon_series)\n",
    "                prev_cands = []\n",
    "                for r in raws:\n",
    "                    for c, B in prev_groups.items():\n",
    "                        precision, recall, expansion = prev_year_guard(curr_groups[r], prev_canon_series, c)\n",
    "                        if (precision >= PREV_PREC_MIN) and (recall >= PREV_RECALL_MIN) and (expansion <= EXPANSION_CAP_PREV):\n",
    "                            f1 = (2 * precision * recall) / (precision + recall + 1e-12)\n",
    "                            prev_cands.append((f1, len(curr_groups[r] & B), r, c))\n",
    "                \n",
    "                prev_cands.sort(key=lambda t: (t[0], t[1]), reverse=True)\n",
    "                used_r = set()\n",
    "                for f1, inter, r, c in prev_cands:\n",
    "                    if (r in used_r) or (c in used_canons): \n",
    "                        continue\n",
    "                    mapping[r] = c\n",
    "                    used_r.add(r)\n",
    "                    used_canons.add(c)\n",
    "\n",
    "                # Unmatched communities: memory-based + Hungarian algorithm\n",
    "                remaining_raws = [r for r in raws if r not in mapping]\n",
    "                remaining_canons = [c for c in canons if c not in used_canons]\n",
    "                if remaining_raws and remaining_canons:\n",
    "                    S = np.array([[weighted_jaccard(curr_groups[r], memory[c]) for c in remaining_canons] for r in remaining_raws])\n",
    "                    C_aug = np.hstack([1.0 - S, np.ones((len(remaining_raws), max(1, len(remaining_raws))))])\n",
    "                    ri, cj = linear_sum_assignment(C_aug)\n",
    "                    for i, j in zip(ri, cj):\n",
    "                        r = remaining_raws[i]\n",
    "                        if j < len(remaining_canons):\n",
    "                            c = remaining_canons[j]\n",
    "                            mem_score = S[i, j]\n",
    "                            exp_rev = len(curr_groups[r]) / last_seen_size.get(c, 1)\n",
    "                            if (mem_score >= MEM_BASE_THRESH) and (mem_score >= MEM_REVIVE_THRESH and exp_rev <= EXPANSION_CAP_REVIVE):\n",
    "                                mapping[r] = c\n",
    "                                used_canons.add(c)\n",
    "                            else:\n",
    "                                mapping[r] = next_id; next_id += 1\n",
    "                        else:\n",
    "                            mapping[r] = next_id; next_id += 1\n",
    "                \n",
    "                # Assign new IDs to any communities still unmatched\n",
    "                for r in raws:\n",
    "                    if r not in mapping:\n",
    "                        mapping[r] = next_id; next_id += 1\n",
    "\n",
    "                # Update memory and store results\n",
    "                for r, cid in mapping.items():\n",
    "                    nodes = curr_groups[r]\n",
    "                    bump(memory, cid, nodes)\n",
    "                    last_seen_size[cid] = len(nodes)\n",
    "                canon_parts[y] = raw_series[y].map(mapping)\n",
    "                prev_canon_series = canon_parts[y]\n",
    "\n",
    "            # Add results as new columns in the GPKG file\n",
    "            for y in years:\n",
    "                g[f\"{method}_canon_{y}\"] = canon_parts.get(y)\n",
    "        \n",
    "        g = g.reset_index(drop=True)\n",
    "        g = g[[c for c in g.columns if c in [\"SIGUNGU_CD\", \"SIGUNGU_NM\"] + [c for c in g.columns if \"_canon_\" in c]] + ['geometry']]\n",
    "\n",
    "        g.to_file(out_gpkg, driver=\"GPKG\", index=False)\n",
    "        print(f\"  [OK] Canonical labels saved → {out_gpkg}\")\n",
    "\n",
    "print(\"[DONE] Community ID canonicalization completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc98aaa7",
   "metadata": {},
   "source": [
    "## 4. Community visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82feac94",
   "metadata": {},
   "source": [
    "### 4.1. Community map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114a02d3",
   "metadata": {},
   "source": [
    "- original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7face403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Settings =====\n",
    "BASE_DIR = \"outputs/gpkg/communities\"\n",
    "OUT_DIR  = \"outputs/figures/communities/map\"\n",
    "if not os.path.exists(OUT_DIR):\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "#COMMUNITY_METHODS = [\"Infomap\", \"Infomap_igraph\", \"Louvain\", \"Leiden\", \"Leiden_igraph\"]\n",
    "COMMUNITY_METHODS = ['Infomap', \"Leiden\", \"Louvain\"]\n",
    "WEIGHTS = [\"거래관계\"]\n",
    "\n",
    "CTPRVN = gpd.read_file(\"data/raw/bnd_sido_00_2024_2Q/bnd_sido_00_2024_2Q.shp\").to_crs(epsg=5179)  # Provincial boundaries (optional)\n",
    "\n",
    "# ===== Execution =====\n",
    "for key in nw_dict.keys():  # Run in the order of nw_dict\n",
    "    sub = os.path.join(BASE_DIR, key)\n",
    "    \n",
    "    for weight in WEIGHTS:\n",
    "        gpkg = os.path.join(sub, f\"{weight}_SGG_map_zone_community.gpkg\")\n",
    "\n",
    "\n",
    "        g = gpd.read_file(gpkg)\n",
    "        if \"SIGUNGU_CD\" not in g.columns:\n",
    "            raise ValueError(\"Column 'SIGUNGU_CD' is required in GPKG.\")\n",
    "        g = g.set_index(\"SIGUNGU_CD\", drop=False)  # Prevent type mismatch\n",
    "\n",
    "        for method in COMMUNITY_METHODS:\n",
    "            # Find {method}_canon_{year} columns\n",
    "            years: List[int] = []\n",
    "            pref = f\"{method}_canon_\"\n",
    "            for c in g.columns:\n",
    "                if c.startswith(pref):\n",
    "                    try:\n",
    "                        years.append(int(c.split(\"_\")[-1]))\n",
    "                    except:\n",
    "                        pass\n",
    "            years = sorted(set(years))\n",
    "            if not years:\n",
    "                continue\n",
    "\n",
    "            for y in years:\n",
    "                outdir = os.path.join(OUT_DIR, key)\n",
    "                os.makedirs(outdir, exist_ok=True)\n",
    "                outpath = os.path.join(outdir, f\"{method}_{y}.png\")\n",
    "\n",
    "                if os.path.exists(outpath):\n",
    "                    print(f\"[SKIP] Already exists → {outpath}\")\n",
    "                    continue\n",
    "\n",
    "                col = f\"{method}_canon_{y}\"\n",
    "                if col not in g.columns:\n",
    "                    continue\n",
    "\n",
    "                # 1) Cast to integer + create color vector\n",
    "                s = g[col].astype(\"Int64\")\n",
    "                colors = s.map(lambda v: COLOR_OF[int(v)] if pd.notna(v) else NO_DATA)\n",
    "\n",
    "                # 2) Plot\n",
    "                fig, ax = plt.subplots(figsize=(10, 9), dpi=100)\n",
    "                g.to_crs(epsg=5179).plot(ax=ax, color=colors, edgecolor=\"black\", linewidth=0.3)\n",
    "                CTPRVN.plot(ax=ax, color=\"none\", edgecolor=\"black\", linewidth=1)\n",
    "                # Use external utilities if available (ignored if not)\n",
    "                add_north_arrow(ax, 0.82, 0.90, ARROW_FILE, zoom=0.18)\n",
    "                add_scale_bar(ax, SCALE_LEN_M, (0.05, 0.03))\n",
    "                ax.set_title(f\"[{key}] {weight} — {method} (Y{y})\")\n",
    "                ax.axis(\"off\")\n",
    "\n",
    "                # 3) Fixed mapping legend (only IDs that appear in this year)\n",
    "                used_ids = sorted(int(v) for v in s.dropna().unique())\n",
    "                handles = [Patch(facecolor=COLOR_OF[i], edgecolor=\"k\", label=f\"M{i}\") for i in used_ids]\n",
    "                if handles:\n",
    "                    ax.legend(handles=handles, title=\"Module (canonical)\",\n",
    "                              loc=\"lower right\", frameon=True,\n",
    "                              fontsize=8, title_fontsize=9, ncol=1)\n",
    "\n",
    "                # 4) Save\n",
    "                plt.tight_layout()\n",
    "                fig.savefig(outpath, dpi=100)\n",
    "                #plt.show()\n",
    "                plt.close(fig)\n",
    "                gc.collect()\n",
    "                print(f\"[OK] {outpath}\")\n",
    "\n",
    "print(\"[DONE] Fixed-color plotting complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a192cef",
   "metadata": {},
   "source": [
    " - Participation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c98b932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file: step3_plot_maps_with_within_module_degree.py\n",
    "# - MODIFIED: Calculate Within-Module Degree (z-score) for each region (SIGGU) and represent it with transparency (alpha).\n",
    "# - MODIFIED: The higher the within-community centrality (z-score), the more opaque; the lower, the more transparent.\n",
    "# - MODIFIED: Add legend for z-score-based role classification (Hub, Non-hub, Peripheral).\n",
    "# - Existing feature: Keep the same color mapping (\"ID->Color\") across all years for the same canonical ID.\n",
    "\n",
    "# ===== Settings =====\n",
    "BASE_DIR = \"outputs/gpkg/communities\"\n",
    "# [MODIFIED] Output folder for result images\n",
    "OUT_DIR  = \"outputs/figures/communities/map_zscore\"\n",
    "if not os.path.exists(OUT_DIR):\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "COMMUNITY_METHODS = ['Infomap', \"Leiden\", \"Louvain\"]\n",
    "WEIGHTS = [\"거래관계\"]\n",
    "\n",
    "\n",
    "CTPRVN = gpd.read_file(\"data/raw/bnd_sido_00_2024_2Q/bnd_sido_00_2024_2Q.shp\").to_crs(epsg=5179)\n",
    "\n",
    "# ===== Execution =====\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "for key in nw_dict.keys(): # Run in the order of nw_dict\n",
    "    sub = os.path.join(BASE_DIR, key)\n",
    "    if not os.path.isdir(sub):\n",
    "        continue\n",
    "\n",
    "    for weight in WEIGHTS:\n",
    "        gpkg = os.path.join(sub, f\"{weight}_SGG_map_zone_community.gpkg\")\n",
    "        if not os.path.exists(gpkg):\n",
    "            continue\n",
    "\n",
    "        g = gpd.read_file(gpkg)\n",
    "        if \"SIGUNGU_CD\" not in g.columns:\n",
    "            raise ValueError(\"GPKG must contain the 'SIGUNGU_CD' column.\")\n",
    "        g = g.set_index(\"SIGUNGU_CD\", drop=False)\n",
    "\n",
    "        for method in COMMUNITY_METHODS:\n",
    "            years: List[int] = []\n",
    "            pref = f\"{method}_canon_\"\n",
    "            for c in g.columns:\n",
    "                if c.startswith(pref):\n",
    "                    try:\n",
    "                        years.append(int(c.split(\"_\")[-1]))\n",
    "                    except:\n",
    "                        pass\n",
    "            years = sorted(set(years))\n",
    "            if not years:\n",
    "                continue\n",
    "\n",
    "            for y in years:\n",
    "                outdir = os.path.join(OUT_DIR, key)\n",
    "                os.makedirs(outdir, exist_ok=True)\n",
    "                outpath = os.path.join(outdir, f\"{method}_{y}.png\")\n",
    "                \n",
    "                if os.path.exists(outpath):\n",
    "                    print(f\"[SKIP] Already exists → {outpath}\")\n",
    "                    continue\n",
    "\n",
    "                col = f\"{method}_canon_{y}\"\n",
    "                if col not in g.columns:\n",
    "                    continue\n",
    "\n",
    "                # Compute Within-Module Degree (z-score)\n",
    "                z_score_values = {}\n",
    "                \n",
    "                df_year = nw_dict[key]\n",
    "                df_year = df_year[df_year['기준연도'] == y].copy()\n",
    "                \n",
    "                if not df_year.empty:\n",
    "                    df_year.rename(columns={'거래관계': 'weights'}, inplace=True)\n",
    "                    G = nx.from_pandas_edgelist(df_year, '시군구코드_seller', '시군구코드_buyer', ['weights'])\n",
    "                    \n",
    "                    communities_series = g[col].dropna()\n",
    "                    node_to_comm = {str(idx): int(val) for idx, val in communities_series.items()}\n",
    "\n",
    "                    # Group nodes by community\n",
    "                    comm_to_nodes = {}\n",
    "                    for node, comm in node_to_comm.items():\n",
    "                        if comm not in comm_to_nodes:\n",
    "                            comm_to_nodes[comm] = []\n",
    "                        comm_to_nodes[comm].append(node)\n",
    "\n",
    "                    # Compute sum of internal edge weights per node\n",
    "                    within_module_degrees = {n: 0.0 for n in G.nodes()}\n",
    "                    for n in G.nodes():\n",
    "                        if n not in node_to_comm: continue\n",
    "                        comm_n = node_to_comm[n]\n",
    "                        for neighbor in G.neighbors(n):\n",
    "                            if node_to_comm.get(neighbor) == comm_n:\n",
    "                                w = G[n][neighbor].get('weights', 1)\n",
    "                                within_module_degrees[n] += w\n",
    "                    \n",
    "                    # Compute community statistics (mean, std)\n",
    "                    comm_stats = {}\n",
    "                    for comm, nodes in comm_to_nodes.items():\n",
    "                        degrees = [within_module_degrees[n] for n in nodes if n in G.nodes()]\n",
    "                        if degrees:\n",
    "                            mean_k = np.mean(degrees)\n",
    "                            std_k = np.std(degrees)\n",
    "                            comm_stats[comm] = (mean_k, std_k)\n",
    "                    \n",
    "                    # Compute z-score per node\n",
    "                    for n in G.nodes():\n",
    "                        if n not in node_to_comm: continue\n",
    "                        comm_n = node_to_comm[n]\n",
    "                        if comm_n in comm_stats:\n",
    "                            mean_k, std_k = comm_stats[comm_n]\n",
    "                            k_is = within_module_degrees[n]\n",
    "                            z_score_values[n] = (k_is - mean_k) / std_k if std_k > 0 else 0.0\n",
    "\n",
    "                # Generate color + transparency vector\n",
    "                s = g[col].astype(\"Int64\")\n",
    "                \n",
    "                rgba_colors = []\n",
    "                for idx, row in g.iterrows():\n",
    "                    cid = s.get(idx)\n",
    "                    base_color_rgb = COLOR_OF.get(int(cid), NO_DATA[:3]) if pd.notna(cid) else NO_DATA[:3]\n",
    "                    \n",
    "                    z_score = z_score_values.get(str(idx), -1.0) # Default = Peripheral\n",
    "                    \n",
    "                    if z_score >= 2.5:\n",
    "                        alpha = 1.0   # Hub (opaque)\n",
    "                    elif z_score >= 1.0:\n",
    "                        alpha = 0.6   # Non-hub\n",
    "                    else:\n",
    "                        alpha = 0.25  # Peripheral (transparent)\n",
    "                    \n",
    "                    rgba_colors.append(mcolors.to_rgba(base_color_rgb, alpha=alpha))\n",
    "\n",
    "                # Plot\n",
    "                fig, ax = plt.subplots(figsize=(10, 9), dpi=100)\n",
    "                g.to_crs(epsg=5179).plot(ax=ax, color=rgba_colors, edgecolor=\"black\", linewidth=0.3)\n",
    "                CTPRVN.plot(ax=ax, color=\"none\", edgecolor=\"black\", linewidth=1)\n",
    "                \n",
    "                add_north_arrow(ax, 0.82, 0.90, ARROW_FILE, zoom=0.18)\n",
    "                add_scale_bar(ax, SCALE_LEN_M, (0.05, 0.03))\n",
    "                \n",
    "                ax.set_title(f\"[{key}] {weight} — {method} (Y{y})\")\n",
    "                ax.axis(\"off\")\n",
    "\n",
    "                # Legend modification\n",
    "                used_ids = sorted(int(v) for v in s.dropna().unique())\n",
    "                handles = [Patch(facecolor=COLOR_OF[i], edgecolor=\"k\", label=f\"M{i}\") for i in used_ids]\n",
    "                \n",
    "                # Add z-score legend\n",
    "                handles.extend([\n",
    "                    Patch(facecolor='none', edgecolor='none', label=''), # Spacer\n",
    "                    Patch(facecolor='gray', label='Within-Module Degree (Alpha)'),\n",
    "                    Patch(facecolor='gray', edgecolor='k', alpha=1.0, label='Hub (z ≥ 2.5)'),\n",
    "                    Patch(facecolor='gray', edgecolor='k', alpha=0.6, label='Non-hub (1.0 ≤ z < 2.5)'),\n",
    "                    Patch(facecolor='gray', edgecolor='k', alpha=0.25, label='Peripheral (z < 1.0)'),\n",
    "                ])\n",
    "\n",
    "                if handles:\n",
    "                    ax.legend(handles=handles, title=\"Module & Role\",\n",
    "                              loc=\"lower right\", frameon=True,\n",
    "                              fontsize=8, title_fontsize=9, ncol=1)\n",
    "\n",
    "                # Save\n",
    "                plt.tight_layout()\n",
    "                fig.savefig(outpath, dpi=100)\n",
    "                #plt.show()\n",
    "                plt.close(fig)\n",
    "                gc.collect()\n",
    "                print(f\"[OK] {outpath}\")\n",
    "\n",
    "print(\"[DONE] Fixed-color plotting with z-score complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecda6d3f",
   "metadata": {},
   "source": [
    "### 4.2. Sankey diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a8dc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file: step4_plot_sankey_consistent_colors.py\n",
    "# - Sankey/Alluvial diagram (canonical community preserved across years)\n",
    "# - Node colors/palette: same COLOR_OF mapping as step3 (fixed mapping)\n",
    "# - Data source: data/map/communities/{key}/{weight}_SGG_map_zone_community.gpkg\n",
    "# - Link values: based on the number of regions (SIGGU). \n",
    "#   If weighted flows are preferred, adjust the flow calculation section.\n",
    "\n",
    "import os, re, gc, textwrap\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle, Patch, PathPatch\n",
    "from matplotlib.path import Path\n",
    "\n",
    "# ===== Settings =====\n",
    "BASE_DIR   = \"outputs/gpkg/communities\"\n",
    "OUT_DIR    = \"outputs/figures/communities/sankey\"\n",
    "if not os.path.exists(OUT_DIR):\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "COMMUNITY_METHODS = ['Infomap', \"Leiden\", \"Louvain\"]\n",
    "WEIGHTS    = [\"거래관계\"]  # Determines which GPKG to read (node composition is the same)\n",
    "FIG_W, FIG_H = 18, 7.6\n",
    "\n",
    "# Layout parameters\n",
    "LEFT, RIGHT = 0.05, 0.97\n",
    "TOP, BOTTOM = 0.90, 0.12\n",
    "YEAR_LABEL_Y = 0.07\n",
    "COL_GAP = 1.0       # Horizontal gap between years\n",
    "NODE_W  = 0.55       # Node width\n",
    "V_GAP   = 0.01      # Vertical gap between nodes\n",
    "TEXT_K = 2          # Number of top cities to display per community\n",
    "TEXT_WRAP = 12     # Characters per wrapped line\n",
    "MAX_LINES = 2       # Maximum number of lines in node labels\n",
    "FLOW_THRESH = 0.0   # Minimum link size (relative to total number of regions)\n",
    "\n",
    "# ===== Utilities =====\n",
    "def safe_name(s: str) -> str:\n",
    "    \"\"\"Convert a string to a safe filename (remove special characters).\"\"\"\n",
    "    return re.sub(r\"[^0-9A-Za-z가-힣_.\\-]+\", \"_\", str(s))\n",
    "\n",
    "def wrap_label(txt: str, width=12, max_lines=2) -> str:\n",
    "    \"\"\"Wrap long text into multiple lines with optional truncation.\"\"\"\n",
    "    lines = textwrap.wrap(txt, width=width)\n",
    "    if len(lines) > max_lines:\n",
    "        lines = lines[:max_lines]\n",
    "        if len(lines[-1]) >= width:\n",
    "            lines[-1] = lines[-1][:max(0, width-1)] + \"…\"\n",
    "        else:\n",
    "            lines[-1] += \"…\"\n",
    "    return \"\\n\".join(lines) if lines else \"\"\n",
    "\n",
    "# ===== Execution =====\n",
    "for key in os.listdir(BASE_DIR):\n",
    "    sub = os.path.join(BASE_DIR, key)\n",
    "    if not os.path.isdir(sub):\n",
    "        continue\n",
    "\n",
    "    # ---- Load DII data (SIGGU-level) ----\n",
    "    dii_path = f\"outputs/tables/cities_DII_RSI/dii_{key}.csv\"\n",
    "    if os.path.exists(dii_path):\n",
    "        dii_df = pd.read_csv(dii_path, encoding=\"cp949\")\n",
    "        dii_dict = dii_df.set_index(\"SIG_CD\")[\"DII\"].to_dict()\n",
    "    else:\n",
    "        print(f\"[WARN] DII file not found for {key}, fallback to zero dict\")\n",
    "        dii_dict = {}\n",
    "\n",
    "    for weight in WEIGHTS:\n",
    "        gpkg = os.path.join(sub, f\"{weight}_SGG_map_zone_community.gpkg\")\n",
    "        if not os.path.exists(gpkg):\n",
    "            print(f\"[SKIP] {gpkg} not found\")\n",
    "            continue\n",
    "\n",
    "        gdf = gpd.read_file(gpkg)\n",
    "        if \"SIGUNGU_CD\" not in gdf.columns:\n",
    "            print(f\"[SKIP] {key}/{weight}: SIGUNGU_CD not found\")\n",
    "            continue\n",
    "        name_col = \"SIGUNGU_NM\" if \"SIGUNGU_NM\" in gdf.columns else None\n",
    "\n",
    "        total_regions = max(1, gdf[\"SIGUNGU_CD\"].dropna().astype(str).nunique())\n",
    "\n",
    "        for method in COMMUNITY_METHODS:\n",
    "            # canonical community assignments for each year\n",
    "            canon_cols = sorted([c for c in gdf.columns if c.startswith(f\"{method}_canon_\")],\n",
    "                                key=lambda c: int(c.split(\"_\")[-1]) if c.split(\"_\")[-1].isdigit() else 0)\n",
    "            if len(canon_cols) < 2:\n",
    "                print(f\"[SKIP] {key}/{weight} {method}: need >= 2 canonical years\")\n",
    "                continue\n",
    "            years = [int(c.split(\"_\")[-1]) for c in canon_cols]\n",
    "            nY = len(years)\n",
    "\n",
    "            node_sizes = {}    # (year, module) -> number of regions\n",
    "            node_codes = {}    # (year, module) -> [SIGGU codes]\n",
    "            node_names = {}    # (year, module) -> [SIGGU names]\n",
    "            year_modules = {y: [] for y in years}\n",
    "\n",
    "            # --- collect community composition per year ---\n",
    "            for y in years:\n",
    "                col = f\"{method}_canon_{y}\"\n",
    "                subdf = gdf[[\"SIGUNGU_CD\", col] + ([name_col] if name_col else [])].dropna(subset=[col]).copy()\n",
    "                subdf[col] = subdf[col].astype(int)\n",
    "                for m, grp in subdf.groupby(col):\n",
    "                    size = grp.shape[0]\n",
    "                    node_sizes[(y, int(m))] = size\n",
    "                    node_codes[(y, int(m))] = grp[\"SIGUNGU_CD\"].astype(str).tolist()\n",
    "                    node_names[(y, int(m))] = grp[name_col].astype(str).tolist() if name_col else grp[\"SIGUNGU_CD\"].astype(str).tolist()\n",
    "                    year_modules[y].append(int(m))\n",
    "\n",
    "            for y in years:\n",
    "                year_modules[y] = sorted(set(year_modules[y]))\n",
    "\n",
    "            # --- compute flows between consecutive years ---\n",
    "            flows = {}  # (y0, m0, y1, m1) -> number of shared regions\n",
    "            for _, row in gdf.iterrows():\n",
    "                for i in range(nY-1):\n",
    "                    y0, y1 = years[i], years[i+1]\n",
    "                    c0, c1 = f\"{method}_canon_{y0}\", f\"{method}_canon_{y1}\"\n",
    "                    if pd.isna(row.get(c0)) or pd.isna(row.get(c1)):\n",
    "                        continue\n",
    "                    m0, m1 = int(row[c0]), int(row[c1])\n",
    "                    k = (y0, m0, y1, m1)\n",
    "                    flows[k] = flows.get(k, 0) + 1\n",
    "\n",
    "            # apply flow threshold\n",
    "            flows = {k: v for k, v in flows.items() if v / total_regions >= FLOW_THRESH}\n",
    "            if not flows:\n",
    "                print(f\"[SKIP] {key}/{weight} {method}: no flows after threshold\")\n",
    "                continue\n",
    "\n",
    "            # --- figure setup ---\n",
    "            x_positions = {y: i * COL_GAP for i, y in enumerate(years)}\n",
    "            x_min, x_max = -COL_GAP*0.5, (nY-1)*COL_GAP + COL_GAP*0.5\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(FIG_W, FIG_H), dpi=100)\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(0, 1)\n",
    "            ax.axis(\"off\")\n",
    "            fig.subplots_adjust(left=LEFT, right=RIGHT, top=TOP, bottom=BOTTOM)\n",
    "\n",
    "            # --- compute node bounds (vertical stacking) ---\n",
    "            node_bounds = {}  # (year, module) -> (y0, y1)\n",
    "            for y in years:\n",
    "                mods = year_modules[y]\n",
    "                sizes = [node_sizes.get((y, m), 0) for m in mods]\n",
    "                total_size_y = sum(sizes)\n",
    "                H = 1.0 - (len(mods) - 1) * V_GAP\n",
    "                scale = H / total_size_y if total_size_y > 0 else 0.0\n",
    "                y_cursor = 0.0\n",
    "                for m, sz in zip(mods, sizes):\n",
    "                    h = sz * scale\n",
    "                    y0 = y_cursor\n",
    "                    y1 = y0 + h\n",
    "                    node_bounds[(y, m)] = (y0, y1)\n",
    "                    y_cursor = y1 + V_GAP\n",
    "\n",
    "            # --- draw nodes and labels ---\n",
    "            for (y, m), (y0, y1) in node_bounds.items():\n",
    "                x = x_positions[y]\n",
    "                face = COLOR_OF.get(int(m), NO_DATA)\n",
    "                rect = Rectangle((x - NODE_W/2, y0), NODE_W, y1 - y0,\n",
    "                                 facecolor=face, edgecolor=\"black\", lw=0.6)\n",
    "                ax.add_patch(rect)\n",
    "\n",
    "                # Node label: top 2 city names by DII + remaining count\n",
    "                codes = node_codes.get((y, m), [])\n",
    "                names = node_names.get((y, m), [])\n",
    "                paired = [(c, n) for c, n in zip(codes, names)]\n",
    "                paired_sorted = sorted(paired, key=lambda x: dii_dict.get(str(x[0]), 0), reverse=True)\n",
    "                top = [n for _, n in paired_sorted[:TEXT_K]]\n",
    "                total_n = len(paired_sorted)\n",
    "                label = \"·\".join(top) + \"\\n\" + (f\"+{total_n-2}\" if total_n > len(top) else \"\")\n",
    "                label_txt = wrap_label(label, width=TEXT_WRAP, max_lines=MAX_LINES)\n",
    "\n",
    "                ax.text(x, (y0 + y1) / 2, label_txt, ha=\"center\", va=\"center\", fontsize=9)\n",
    "\n",
    "            # --- draw flows ---\n",
    "            used_src = {k: 0.0 for k in node_bounds.keys()}\n",
    "            used_tgt = {k: 0.0 for k in node_bounds.keys()}\n",
    "\n",
    "            for i in range(nY - 1):\n",
    "                y0, y1 = years[i], years[i+1]\n",
    "                keys = [k for k in flows.keys() if k[0] == y0 and k[2] == y1]\n",
    "                keys.sort(key=lambda k: -flows[k])\n",
    "\n",
    "                s_total = sum(node_sizes.get((y0, mm), 0) for mm in year_modules[y0])\n",
    "                t_total = sum(node_sizes.get((y1, mm), 0) for mm in year_modules[y1])\n",
    "                s_scale = (1.0 - (len(year_modules[y0]) - 1) * V_GAP) / s_total if s_total > 0 else 0.0\n",
    "                t_scale = (1.0 - (len(year_modules[y1]) - 1) * V_GAP) / t_total if t_total > 0 else 0.0\n",
    "\n",
    "                for (yy0, m0, yy1, m1) in keys:\n",
    "                    v = flows[(yy0, m0, yy1, m1)]\n",
    "                    s_y0, s_y1 = node_bounds[(yy0, m0)]\n",
    "                    t_y0, t_y1 = node_bounds[(yy1, m1)]\n",
    "\n",
    "                    s_off = used_src[(yy0, m0)]\n",
    "                    t_off = used_tgt[(yy1, m1)]\n",
    "\n",
    "                    s_a = s_y0 + s_off * s_scale\n",
    "                    s_b = s_y0 + (s_off + v) * s_scale\n",
    "                    t_a = t_y0 + t_off * t_scale\n",
    "                    t_b = t_y0 + (t_off + v) * t_scale\n",
    "\n",
    "                    used_src[(yy0, m0)] += v\n",
    "                    used_tgt[(yy1, m1)] += v\n",
    "\n",
    "                    x0 = x_positions[yy0] + NODE_W/2\n",
    "                    x1 = x_positions[yy1] - NODE_W/2\n",
    "                    dx = (x1 - x0)\n",
    "                    ctrl = 0.5\n",
    "\n",
    "                    path_data = [\n",
    "                        (Path.MOVETO, (x0, s_a)),\n",
    "                        (Path.CURVE4, (x0 + ctrl*dx, s_a)),\n",
    "                        (Path.CURVE4, (x1 - ctrl*dx, t_a)),\n",
    "                        (Path.CURVE4, (x1, t_a)),\n",
    "                        (Path.LINETO, (x1, t_b)),\n",
    "                        (Path.CURVE4, (x1 - ctrl*dx, t_b)),\n",
    "                        (Path.CURVE4, (x0 + ctrl*dx, s_b)),\n",
    "                        (Path.CURVE4, (x0, s_b)),\n",
    "                        (Path.CLOSEPOLY, (x0, s_a)),\n",
    "                    ]\n",
    "                    codes, verts = zip(*path_data)\n",
    "                    c0 = COLOR_OF.get(int(m0), (0.6, 0.6, 0.6))\n",
    "                    patch = PathPatch(Path(verts, codes),\n",
    "                                      facecolor=c0, edgecolor=\"none\", alpha=0.35)\n",
    "                    ax.add_patch(patch)\n",
    "\n",
    "            # --- add year labels ---\n",
    "            for y in years:\n",
    "                n_mods = len(year_modules.get(y, []))\n",
    "                label_txt = f\"{y} (n = {n_mods})\"\n",
    "                ax.text(x_positions[y], YEAR_LABEL_Y - 0.15,\n",
    "                        label_txt, ha=\"center\", va=\"center\",\n",
    "                        fontsize=11, color=\"black\")\n",
    "\n",
    "            # --- legend (module IDs) ---\n",
    "            used_ids = sorted({m for (y, m) in node_bounds.keys()})\n",
    "            if used_ids:\n",
    "                patches = [Patch(facecolor=COLOR_OF.get(int(i), NO_DATA), edgecolor=\"black\", label=f\"M{i}\") \n",
    "                           for i in used_ids]\n",
    "                leg = ax.legend(\n",
    "                    handles=patches,\n",
    "                    title=\"Module (canonical)\",\n",
    "                    loc=\"upper left\",\n",
    "                    bbox_to_anchor=(1.01, 1.0),\n",
    "                    frameon=True,\n",
    "                    fontsize=8,\n",
    "                    title_fontsize=9,\n",
    "                    ncol=1\n",
    "                )\n",
    "\n",
    "            # --- save figure ---\n",
    "            out_dir = OUT_DIR\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "            out_png = os.path.join(out_dir, f\"sankey_{key}_{weight}_{safe_name(method)}.png\")\n",
    "            plt.savefig(out_png, dpi=100, bbox_inches=\"tight\")\n",
    "            #plt.show()\n",
    "            plt.close(fig)\n",
    "            gc.collect()\n",
    "            print(f\"[OK] {out_png}\")\n",
    "\n",
    "print(\"[DONE] Sankey plotting complete\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firm-transaction-network",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
