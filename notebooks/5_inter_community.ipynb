{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c6d175a",
   "metadata": {},
   "source": [
    "## 0. Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536f63cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import gc\n",
    "import glob\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "# Third-party libraries\n",
    "import geopandas as gpd\n",
    "import mapclassify\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "from shapely.ops import nearest_points\n",
    "\n",
    "# Visualization libraries (Matplotlib)\n",
    "import matplotlib.image as mimg\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors as mcolors\n",
    "from matplotlib import font_manager as fm\n",
    "from matplotlib import patheffects as path_effects\n",
    "from matplotlib.collections import LineCollection\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.offsetbox import AnnotationBbox, OffsetImage\n",
    "from matplotlib.patches import FancyArrowPatch, Patch\n",
    "\n",
    "if os.getcwd().endswith('notebooks'):\n",
    "    os.chdir('..')\n",
    "\n",
    "\n",
    "# Set font path with absolute path (adjust filename/path accordingly)\n",
    "font_regular = os.path.abspath(\"assets/malgun.ttf\")     # Regular\n",
    "font_bold    = os.path.abspath(\"assets/malgunbd.ttf\")   # Bold\n",
    "\n",
    "# Register fonts in Matplotlib font manager\n",
    "fm.fontManager.addfont(font_regular)\n",
    "fm.fontManager.addfont(font_bold)\n",
    "\n",
    "# Safely get the family name from the font file and apply globally\n",
    "fam = fm.FontProperties(fname=font_regular).get_name()  # Usually 'Malgun Gothic'\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": fam,\n",
    "    \"axes.unicode_minus\": False,  # Prevent minus sign from breaking\n",
    "})\n",
    "\n",
    "ARROW_FILE = \"north_arrow.png\"\n",
    "\n",
    "def add_north_arrow(ax, x, y, arrow_file, zoom=0.05):\n",
    "    \"\"\"Add a north arrow image at the given axes position.\"\"\"\n",
    "    im = mimg.imread(arrow_file)\n",
    "    ax.add_artist(\n",
    "        AnnotationBbox(OffsetImage(im, zoom=zoom), (x, y), \n",
    "                       xycoords='axes fraction', frameon=False)\n",
    "    )\n",
    "\n",
    "def add_scale_bar(ax, length, location=(0.1, 0.02), linewidth=3, color='black'):\n",
    "    \"\"\"Add a scale bar with text in kilometers.\"\"\"\n",
    "    xlim, ylim = ax.get_xlim(), ax.get_ylim()\n",
    "    sb_x = xlim[0] + (xlim[1] - xlim[0]) * location[0]\n",
    "    sb_y = ylim[0] + (ylim[1] - ylim[0]) * location[1]\n",
    "    ax.plot([sb_x, sb_x + length], [sb_y, sb_y], color=color, linewidth=linewidth)\n",
    "    ax.text(sb_x + length/2, sb_y, f'{round(length/1000):,} km',\n",
    "            va='bottom', ha='center', fontsize=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc01f00",
   "metadata": {},
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3d6ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SGG_map = gpd.read_file('data/processed/map/SGG_map.gpkg').to_crs(epsg=5179)\n",
    "SGG_map['centroid'] = SGG_map.geometry.centroid\n",
    "\n",
    "# Collect all matching CSV files\n",
    "all_files = glob.glob(\"data/processed/deal_network/deal_by/network_by_*.csv\")\n",
    "\n",
    "# Dictionary to store DataFrames\n",
    "nw_dict = {}\n",
    "\n",
    "for file in all_files:\n",
    "    # Extract a key from the file name\n",
    "    # e.g., 'network_by_age (man).csv' → 'age (man)'\n",
    "    key = os.path.basename(file).replace(\"network_by_\", \"\").replace(\".csv\", \"\")\n",
    "    \n",
    "    # Read the CSV file\n",
    "    nw = pd.read_csv(\n",
    "        file,\n",
    "        dtype={'14_시군구코드_buyer': str, '8_시군구코드_seller': str, '1_기준연도': int}\n",
    "    )\n",
    "    nw.columns = ['year', 'source', 'target', '거래관계']\n",
    "    \n",
    "    # Merge with centroid coordinates (seller and buyer separately)\n",
    "    nw = pd.merge(\n",
    "        nw, SGG_map[['SIG_CD', 'centroid']],\n",
    "        left_on='source', right_on='SIG_CD', how='left'\n",
    "    )\n",
    "    nw = pd.merge(\n",
    "        nw, SGG_map[['SIG_CD', 'centroid']],\n",
    "        left_on='target', right_on='SIG_CD', how='left',\n",
    "        suffixes=('_seller', '_buyer')\n",
    "    )\n",
    "    nw = nw.drop(columns=['SIG_CD_seller', 'SIG_CD_buyer'])\n",
    "    \n",
    "    # Store DataFrame in dictionary\n",
    "    nw_dict[key] = nw\n",
    "    \n",
    "\n",
    "# Define a preferred order of network groups\n",
    "desired_order = [\n",
    "    'all',\n",
    "    'man',\n",
    "    'innovation',\n",
    "    'urban_size_소상공인',\n",
    "    'urban_size_중소기업',\n",
    "    'urban_size_중견기업',\n",
    "    'urban_size_대기업',\n",
    "    'urban_age_1년 미만',\n",
    "    'urban_age_1~5년 미만',\n",
    "    'urban_age_5~10년 미만',\n",
    "    'urban_age_10년 이상'\n",
    "]\n",
    "\n",
    "# Create a new dictionary following the desired order\n",
    "# Only include keys that actually exist in nw_dict\n",
    "ordered_nw_dict = {key: nw_dict[key] for key in desired_order if key in nw_dict}\n",
    "\n",
    "# Replace nw_dict with the ordered dictionary\n",
    "nw_dict = ordered_nw_dict\n",
    "\n",
    "\n",
    "# ===== Utility function =====\n",
    "def build_color_table(max_id=600):\n",
    "    \"\"\"\n",
    "    Build a color table for unique IDs (e.g., for labeling node categories).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    max_id : int, optional\n",
    "        Maximum number of IDs to assign colors to (default is 600).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A mapping {id: RGBA color} for each unique ID.\n",
    "    \"\"\"\n",
    "    pal = list(plt.get_cmap(\"Set1\").colors) \\\n",
    "        + list(plt.get_cmap(\"Dark2\").colors) \\\n",
    "        + list(plt.get_cmap(\"Accent\").colors)\n",
    "    return {cid: mcolors.to_rgba(pal[cid % len(pal)], 1.0) for cid in range(max_id)}\n",
    "\n",
    "\n",
    "COLOR_OF = build_color_table(600)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4997cc",
   "metadata": {},
   "source": [
    "## 2. Data table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6016d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# ========= Paths / Parameters =========\n",
    "BASE_DEAL_DIR = \"data/processed/deal_network/deal_by\"\n",
    "COMM_BASE_DIR = \"outputs/gpkg/communities\"\n",
    "if not os.path.exists(COMM_BASE_DIR):\n",
    "    os.makedirs(COMM_BASE_DIR, exist_ok=True)\n",
    "TABLE_DIR     = \"outputs/tables/communities_DII_RSI\"   # <-- Output folder for results (RSI/DII)\n",
    "if not os.path.exists(TABLE_DIR):\n",
    "    os.makedirs(TABLE_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "COMMUNITY_METHODS = ['Infomap', \"Leiden\", \"Louvain\"]\n",
    "WEIGHTS           = [\"거래관계\"]\n",
    "\n",
    "# ===== Utility functions =====\n",
    "def compute_dii_rsi_community(ed: pd.DataFrame, lab_map: dict, weight_col: str, year: int):\n",
    "    \"\"\"Compute RSI and DII (at the community level)\"\"\"\n",
    "    ed_use = ed.dropna(subset=[\"source\",\"target\",weight_col]).copy()\n",
    "    ed_use[weight_col] = pd.to_numeric(ed_use[weight_col], errors=\"coerce\").fillna(0)\n",
    "\n",
    "    # --- map to community ---\n",
    "    ed_use[\"m_s\"] = ed_use[\"source\"].map(lab_map)\n",
    "    ed_use[\"m_t\"] = ed_use[\"target\"].map(lab_map)\n",
    "    ed_use = ed_use.dropna(subset=[\"m_s\",\"m_t\"])\n",
    "    ed_use = ed_use[ed_use[\"m_s\"] != ed_use[\"m_t\"]]\n",
    "\n",
    "    if ed_use.empty:\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    # --- aggregate flows at community level ---\n",
    "    flows = ed_use.groupby([\"m_s\",\"m_t\"])[weight_col].sum().reset_index()\n",
    "\n",
    "    # RSI (edge-level, community)\n",
    "    total_flow = flows[weight_col].sum()\n",
    "    flows[\"RSI\"] = (flows[weight_col] / total_flow * 100) if total_flow > 0 else 0.0\n",
    "    rsi_df = flows.copy()\n",
    "    rsi_df[\"year\"] = year\n",
    "\n",
    "    # DII (node-level, community)\n",
    "    inflow = flows.groupby(\"m_t\")[weight_col].sum()\n",
    "    outflow = flows.groupby(\"m_s\")[weight_col].sum()\n",
    "    io_sum = inflow.add(outflow, fill_value=0)\n",
    "    mean_io = io_sum.mean() if not io_sum.empty else 1.0\n",
    "    dii = io_sum / mean_io if mean_io > 0 else io_sum\n",
    "    dii_df = dii.rename(\"DII\").reset_index().rename(columns={dii.index.name:\"community\"})\n",
    "    dii_df[\"year\"] = year\n",
    "\n",
    "    return rsi_df, dii_df\n",
    "\n",
    "\n",
    "# ===== Main loop =====\n",
    "for key, ed in nw_dict.items():\n",
    "    all_rsi, all_dii = [], []\n",
    "\n",
    "    for weight in WEIGHTS:\n",
    "        gpkg = os.path.join(COMM_BASE_DIR, key, f\"{weight}_SGG_map_zone_community.gpkg\")\n",
    "        if not os.path.exists(gpkg): \n",
    "            continue\n",
    "\n",
    "        g = gpd.read_file(gpkg)\n",
    "        if \"SIGUNGU_CD\" not in g.columns:\n",
    "            print(f\"[SKIP] {key}/{weight}: SIGUNGU_CD not found\")\n",
    "            continue\n",
    "        g[\"SIGUNGU_CD\"] = g[\"SIGUNGU_CD\"].astype(str)\n",
    "\n",
    "        # Check available years\n",
    "        years = sorted({int(c.split(\"_\")[-1]) for c in g.columns \n",
    "                        if c.startswith(tuple(f\"{m}_canon_\" for m in COMMUNITY_METHODS))})\n",
    "        if not years: \n",
    "            print(f\"[SKIP] {key}/{weight}: no canonical years\")\n",
    "            continue\n",
    "\n",
    "        # Check edge validity\n",
    "        if not all(c in ed.columns for c in ['year','source','target',weight]):\n",
    "            print(f\"[SKIP] {key}: edges missing required columns\")\n",
    "            continue\n",
    "\n",
    "        ed_use = ed.dropna(subset=[\"source\",\"target\",\"year\"]).copy()\n",
    "        ed_use[\"source\"] = ed_use[\"source\"].astype(str)\n",
    "        ed_use[\"target\"] = ed_use[\"target\"].astype(str)\n",
    "        ed_use[weight] = pd.to_numeric(ed_use[weight], errors=\"coerce\").fillna(0)\n",
    "\n",
    "        for method in COMMUNITY_METHODS:\n",
    "            for y in years:\n",
    "                col_canon = f\"{method}_canon_{y}\"\n",
    "                if col_canon not in g.columns: \n",
    "                    continue\n",
    "\n",
    "                # Map SIGUNGU to community\n",
    "                s_year = g[[\"SIGUNGU_CD\", col_canon]].dropna(subset=[col_canon])\n",
    "                s_year[col_canon] = s_year[col_canon].astype(int)\n",
    "                lab_map = s_year.set_index(\"SIGUNGU_CD\")[col_canon].to_dict()\n",
    "\n",
    "                ede = ed_use[ed_use[\"year\"] == y].copy()\n",
    "                if ede.empty: \n",
    "                    continue\n",
    "\n",
    "                # Compute RSI/DII at community level\n",
    "                rsi_df, dii_df = compute_dii_rsi_community(ede, lab_map, weight, year=y)\n",
    "                if rsi_df.empty and dii_df.empty:\n",
    "                    continue\n",
    "                \n",
    "                # Natural breaks\n",
    "                if not dii_df.empty:\n",
    "                    try:\n",
    "                        dii_classifier = mapclassify.NaturalBreaks(dii_df[\"DII\"], k=3)\n",
    "                        dii_df[\"DII_class\"] = 3 - dii_classifier.yb  # Classes start from 1\n",
    "                    except Exception as e:\n",
    "                        print(f\"[WARN] DII classification failed for {key}/{method}/{y}: {e}\")\n",
    "                        dii_df[\"DII_class\"] = np.nan\n",
    "                \n",
    "                rsi_df[\"key\"] = key\n",
    "                rsi_df[\"method\"] = method\n",
    "                dii_df[\"key\"] = key\n",
    "                dii_df[\"method\"] = method\n",
    "\n",
    "                all_rsi.append(rsi_df)\n",
    "                all_dii.append(dii_df)\n",
    "\n",
    "    # ---- Save results (per key) ----\n",
    "    if all_rsi:\n",
    "        RSI_OUT = os.path.join(TABLE_DIR, f\"rsi_{key}.csv\")\n",
    "        pd.concat(all_rsi, ignore_index=True).to_csv(RSI_OUT, index=False, encoding=\"cp949\")\n",
    "        print(f\"[OK] RSI saved to {RSI_OUT}\")\n",
    "    else:\n",
    "        print(f\"[WARN] No RSI results for {key}\")\n",
    "\n",
    "    if all_dii:\n",
    "        DII_OUT = os.path.join(TABLE_DIR, f\"dii_{key}.csv\")\n",
    "        pd.concat(all_dii, ignore_index=True).to_csv(DII_OUT, index=False, encoding=\"cp949\")\n",
    "        print(f\"[OK] DII saved to {DII_OUT}\")\n",
    "    else:\n",
    "        print(f\"[WARN] No DII results for {key}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880bf546",
   "metadata": {},
   "source": [
    "# 3. Inter commuity visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2aa4038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Paths / Parameters =========\n",
    "BASE_DEAL_DIR = \"data/processed/deal_network/deal_by\"  # network_by_*.csv\n",
    "COMM_BASE_DIR = \"outputs/gpkg/communities\"             # …/{key}/{weight}_SGG_map_zone_community.gpkg\n",
    "OUT_DIR       = \"outputs/figures/communities/inter_flow\"\n",
    "\n",
    "# What to visualize\n",
    "COMMUNITY_METHODS = ['Infomap', \"Leiden\", \"Louvain\"]\n",
    "WEIGHTS           = [\"거래관계\"]  # Column name in the original CSV used as edge weight\n",
    "\n",
    "# Link / Node style\n",
    "BASE_LW      = 0.5\n",
    "LW_RANGE     = 4.0\n",
    "RSI_W_BLEND  = 0.30   # Blend 30% of RSI into edge width\n",
    "N_SEG        = 24     # Divide curve into N segments for gradient coloring\n",
    "CURVATURE    = 0.20   # Bézier curve curvature\n",
    "\n",
    "# Node size parameters\n",
    "BASE_NODE_SIZE = 200  # Node size (scatter “s” parameter) when DII = 1.0\n",
    "DII_CLIP_HI    = 4    # Upper clip value for DII\n",
    "\n",
    "# RSI → alpha transformation\n",
    "MIN_ALPHA        = 0.8\n",
    "MAX_ALPHA        = 1\n",
    "RSI_CLIP_LO      = 0.0\n",
    "RSI_CLIP_HI_QUANT = 0.99             # Use the 99th percentile as the upper clip\n",
    "RSI_PCT_LABELS    = [0.70,0.80,0.90,0.99]  # Percentile labels for the legend\n",
    "\n",
    "# ===== Utility functions =====\n",
    "def _curve_segments(p1: Point, p2: Point, curvature=CURVATURE, nseg=N_SEG):\n",
    "    # Generate curved line segments between two points using a quadratic Bézier curve\n",
    "    mx, my = (p1.x+p2.x)/2, (p1.y+p2.y)/2\n",
    "    dx, dy = (p2.x-p1.x), (p2.y-p1.y)\n",
    "    dist = np.hypot(dx, dy)\n",
    "    if dist == 0: return None\n",
    "    px, py = -dy/dist, dx/dist\n",
    "    cx, cy = mx + px*dist*curvature, my + py*dist*curvature\n",
    "    t = np.linspace(0, 1, nseg+1)\n",
    "    bx = (1-t)**2*p1.x + 2*(1-t)*t*cx + t**2*p2.x\n",
    "    by = (1-t)**2*p1.y + 2*(1-t)*t*cy + t**2*p2.y\n",
    "    segs = np.stack([np.column_stack([bx[:-1], by[:-1]]),\n",
    "                     np.column_stack([bx[1:],  by[1:]])], axis=1)\n",
    "    return segs\n",
    "\n",
    "def _norm01(a):\n",
    "    # Normalize array values to [0, 1]\n",
    "    a = np.asarray(a, float)\n",
    "    if a.size == 0: return a\n",
    "    vmin, vmax = np.nanmin(a), np.nanmax(a)\n",
    "    if not np.isfinite(vmin) or not np.isfinite(vmax) or vmax <= vmin:\n",
    "        return np.full_like(a, 0.5)\n",
    "    return (a - vmin) / (vmax - vmin)\n",
    "\n",
    "def _alpha_from_rsi(rsi, clip_lo=RSI_CLIP_LO, clip_hi=None):\n",
    "    # Convert RSI values into alpha transparency with clipping\n",
    "    rsi = np.asarray(rsi, float)\n",
    "    if clip_hi is None or not np.isfinite(clip_hi):\n",
    "        clip_hi = np.quantile(rsi, RSI_CLIP_HI_QUANT) if np.isfinite(np.nanmax(rsi)) else 0.01\n",
    "        clip_hi = max(clip_hi, clip_lo * 10)\n",
    "    r = np.clip(rsi, clip_lo, clip_hi)\n",
    "    z = (r - clip_lo) / (clip_hi - clip_lo) if clip_hi > clip_lo else np.zeros_like(r)\n",
    "    z = np.clip(z, 0, 1)\n",
    "    return MIN_ALPHA + (MAX_ALPHA - MIN_ALPHA) * z\n",
    "\n",
    "\n",
    "# ===== Main loop =====\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "for key, ed in nw_dict.items():\n",
    "    for weight in WEIGHTS:\n",
    "        gpkg = os.path.join(COMM_BASE_DIR, key, f\"{weight}_SGG_map_zone_community.gpkg\")\n",
    "        if not os.path.exists(gpkg): continue\n",
    "\n",
    "        g = gpd.read_file(gpkg)\n",
    "        if \"SIGUNGU_CD\" not in g.columns:\n",
    "            print(f\"[SKIP] {key}/{weight}: SIGUNGU_CD not found\")\n",
    "            continue\n",
    "        g[\"SIGUNGU_CD\"] = g[\"SIGUNGU_CD\"].astype(str)\n",
    "\n",
    "        # Extract available years from canonical columns\n",
    "        years = sorted({int(c.split(\"_\")[-1]) for c in g.columns if c.startswith(tuple(f\"{m}_canon_\" for m in COMMUNITY_METHODS))})\n",
    "        if not years:\n",
    "            print(f\"[SKIP] {key}/{weight}: no canonical years found\")\n",
    "            continue\n",
    "        \n",
    "        # Validate edge table\n",
    "        if all(c in ed.columns for c in ['year', 'source', 'target', weight]):\n",
    "            ed_use = ed.dropna(subset=[\"source\", \"target\", \"year\"]).copy()\n",
    "            ed_use[\"source\"] = ed_use[\"source\"].astype(str)\n",
    "            ed_use[\"target\"] = ed_use[\"target\"].astype(str)\n",
    "            ed_use[weight] = pd.to_numeric(ed_use[weight], errors=\"coerce\").fillna(0)\n",
    "        else:\n",
    "            print(f\"[SKIP] {key}: edges missing required columns\")\n",
    "            continue\n",
    "\n",
    "        for method in COMMUNITY_METHODS:\n",
    "            canon_cols = [c for c in g.columns if c.startswith(f\"{method}_canon_\")]\n",
    "            if not canon_cols: continue\n",
    "\n",
    "            # ---- Build master network (integrated across all years) ----\n",
    "            all_nodes_master = set()\n",
    "            all_flows_master = {}\n",
    "\n",
    "            for y_build in years:\n",
    "                col_canon_build = f\"{method}_canon_{y_build}\"\n",
    "                if col_canon_build not in g.columns: continue\n",
    "\n",
    "                s_year_build = g[[\"SIGUNGU_CD\", col_canon_build]].dropna(subset=[col_canon_build])\n",
    "                s_year_build[col_canon_build] = s_year_build[col_canon_build].astype(int)\n",
    "                all_nodes_master.update(s_year_build[col_canon_build].unique())\n",
    "\n",
    "                lab_map_build = s_year_build.set_index(\"SIGUNGU_CD\")[col_canon_build].to_dict()\n",
    "                ede_build = ed_use[ed_use[\"year\"] == y_build].copy()\n",
    "                if not ede_build.empty:\n",
    "                    ede_build[\"m_s\"] = ede_build[\"source\"].map(lab_map_build)\n",
    "                    ede_build[\"m_t\"] = ede_build[\"target\"].map(lab_map_build)\n",
    "                    ede_build = ede_build.dropna(subset=[\"m_s\", \"m_t\"])\n",
    "                    ede_build = ede_build[ede_build[\"m_s\"] != ede_build[\"m_t\"]]\n",
    "                    flows_build = ede_build.groupby([\"m_s\", \"m_t\"])[weight].sum()\n",
    "                    for (ms, mt), val in flows_build.items():\n",
    "                        all_flows_master[(int(ms), int(mt))] = all_flows_master.get((int(ms), int(mt)), 0) + val\n",
    "            \n",
    "            G_master = nx.DiGraph()\n",
    "            G_master.add_nodes_from(all_nodes_master)\n",
    "            for (ms, mt), val in all_flows_master.items():\n",
    "                G_master.add_edge(ms, mt, weight=val)\n",
    "\n",
    "            # Global layout\n",
    "            master_pos = {}\n",
    "            if G_master.number_of_nodes() > 0:\n",
    "                print(f\"Calculating master layout for [{key}/{weight}/{method}]...\")\n",
    "                master_pos = nx.kamada_kawai_layout(G_master, scale=2.0)\n",
    "            \n",
    "            # ---- Fix axis ranges ----\n",
    "            master_xlim = (-1.2, 1.2)\n",
    "            master_ylim = (-1.2, 1.2)\n",
    "            if master_pos:\n",
    "                x_coords = [p[0] for p in master_pos.values()]\n",
    "                y_coords = [p[1] for p in master_pos.values()]\n",
    "                x_min, x_max = min(x_coords), max(x_coords)\n",
    "                y_min, y_max = min(y_coords), max(y_coords)\n",
    "                x_margin = (x_max - x_min) * 0.15 if (x_max > x_min) else 0.2\n",
    "                y_margin = (y_max - y_min) * 0.15 if (y_max > y_min) else 0.2\n",
    "                master_xlim = (x_min - x_margin, x_max + x_margin)\n",
    "                master_ylim = (y_min - y_margin, y_max + y_margin)\n",
    "            \n",
    "            # ---- Yearly visualization ----\n",
    "            for y in years:\n",
    "                outdir = os.path.join(OUT_DIR, key, weight)\n",
    "                os.makedirs(outdir, exist_ok=True)\n",
    "                safe = lambda s: re.sub(r\"[^0-9A-Za-z가-힣_.\\\\-]+\", \"_\", s)\n",
    "                outpath = os.path.join(outdir, f\"flows_{safe(method)}_{y}.png\")\n",
    "\n",
    "                if os.path.exists(outpath):\n",
    "                    print(f\"  [SKIP] {key}/{weight}: {method} Y{y} already exists\")\n",
    "                    continue\n",
    "\n",
    "                col_canon = f\"{method}_canon_{y}\"\n",
    "                if col_canon not in g.columns: continue\n",
    "                \n",
    "                s_year = g[[\"SIGUNGU_CD\", col_canon]].dropna(subset=[col_canon])\n",
    "                s_year[col_canon] = s_year[col_canon].astype(int)\n",
    "                lab_map = s_year.set_index(\"SIGUNGU_CD\")[col_canon].to_dict()\n",
    "                size_of = s_year.groupby(col_canon).size().to_dict()\n",
    "                \n",
    "                ede = ed_use[ed_use[\"year\"] == y].copy()\n",
    "                if ede.empty: flows = {}\n",
    "                else:\n",
    "                    ede[\"m_s\"] = ede[\"source\"].map(lab_map)\n",
    "                    ede[\"m_t\"] = ede[\"target\"].map(lab_map)\n",
    "                    ede = ede.dropna(subset=[\"m_s\",\"m_t\"])\n",
    "                    ede[\"m_s\"] = ede[\"m_s\"].astype(int)\n",
    "                    ede[\"m_t\"] = ede[\"m_t\"].astype(int)\n",
    "                    ede = ede[ede[\"m_s\"] != ede[\"m_t\"]]\n",
    "                    flows = ede.groupby([\"m_s\",\"m_t\"])[weight].sum().to_dict()\n",
    "\n",
    "                # RSI / DII 계산\n",
    "                if flows:\n",
    "                    v_arr = np.array(list(flows.values()), float)\n",
    "                    total_flow = float(np.nansum(v_arr))\n",
    "                    rsi = {k: (val / total_flow if total_flow > 0 else 0.0) for k, val in flows.items()}\n",
    "                    rsi_vals = np.array(list(rsi.values()), float)\n",
    "                    clip_hi  = np.quantile(rsi_vals, RSI_CLIP_HI_QUANT) if rsi_vals.size else 0.01\n",
    "                    alphas   = {k: float(_alpha_from_rsi([rsi[k]], clip_lo=RSI_CLIP_LO, clip_hi=clip_hi)[0]) for k in rsi}\n",
    "                    vmax = np.nanmax(v_arr) if np.isfinite(np.nanmax(v_arr)) else 1.0\n",
    "                    logv = np.log10((v_arr / vmax) * 9.0 + 1.0) if vmax > 0 else np.zeros_like(v_arr)\n",
    "                    w_norm = _norm01(logv)\n",
    "                    rsi01 = {k: (alphas[k] - MIN_ALPHA) / (MAX_ALPHA - MIN_ALPHA) if (MAX_ALPHA - MIN_ALPHA) > 0 else 0 for k in rsi}\n",
    "                    widths = {k: BASE_LW + LW_RANGE * ((1.0 - RSI_W_BLEND) * w_norm[i] + RSI_W_BLEND * rsi01[k]) for i, k in enumerate(flows.keys())}\n",
    "                else:\n",
    "                    rsi, alphas, widths, rsi_vals = {}, {}, {}, np.array([])\n",
    "\n",
    "                if flows:\n",
    "                    io_sum = {}\n",
    "                    for (ms, mt), val in flows.items():\n",
    "                        io_sum[ms] = io_sum.get(ms, 0.0) + float(val)\n",
    "                        io_sum[mt] = io_sum.get(mt, 0.0) + float(val)\n",
    "                    vals = np.array(list(io_sum.values()), float)\n",
    "                    mean_io = float(vals.mean()) if vals.size else 1.0\n",
    "                    dii = {c: (io_sum.get(c, 0.0) / mean_io if mean_io > 0 else 0.0) for c in set(io_sum.keys())}\n",
    "                else:\n",
    "                    dii = {}\n",
    "                \n",
    "                node_ids  = sorted(list(size_of.keys()))\n",
    "                node_cols = {cid: COLOR_OF[cid] for cid in node_ids}\n",
    "                if dii:\n",
    "                    node_sizes = {cid: BASE_NODE_SIZE * min(DII_CLIP_HI, dii.get(cid, 1.0)) for cid in node_ids}\n",
    "                else:\n",
    "                    node_sizes = {cid: BASE_NODE_SIZE for cid in node_ids}\n",
    "                \n",
    "                pos_this_year = {nid: master_pos[nid] for nid in node_ids if nid in master_pos}\n",
    "                node_pts = {cid: Point(p) for cid, p in pos_this_year.items()}\n",
    "                \n",
    "                fig, ax = plt.subplots(figsize=(6, 6))\n",
    "                ax.axis(\"off\")\n",
    "                ax.set_xlim(master_xlim)\n",
    "                ax.set_ylim(master_ylim)\n",
    "\n",
    "                # Draw edges\n",
    "                for (ms, mt), val in flows.items():\n",
    "                    if ms not in node_pts or mt not in node_pts: continue\n",
    "                    p1, p2 = node_pts[ms], node_pts[mt]\n",
    "                    segs = _curve_segments(p1, p2, curvature=CURVATURE, nseg=N_SEG)\n",
    "                    if segs is None: continue\n",
    "                    c0 = mcolors.to_rgba(node_cols.get(ms, (0.2,0.2,0.2,1.0)), 1.0)\n",
    "                    c1 = mcolors.to_rgba(mcolors.to_rgb(node_cols.get(ms, (0.2,0.2,0.2,1.0))), 0.1)\n",
    "                    cols = np.array([np.linspace(c0[i], c1[i], N_SEG) for i in range(4)]).T\n",
    "                    cols[:, 3] *= alphas.get((ms, mt), 0.6)\n",
    "                    lc = LineCollection(segs, colors=cols,\n",
    "                                        linewidths=widths.get((ms,mt), BASE_LW),\n",
    "                                        capstyle=\"round\", zorder=1)\n",
    "                    ax.add_collection(lc)\n",
    "\n",
    "                # Draw nodes\n",
    "                for cid, pt in node_pts.items():\n",
    "                    if pt is None: continue\n",
    "                    s = node_sizes.get(cid, BASE_NODE_SIZE)\n",
    "                    ax.scatter([pt.x],[pt.y], s=s, c=[node_cols[cid]],\n",
    "                               edgecolors=\"white\", linewidths=0.9,\n",
    "                               alpha=0.95, zorder=3)\n",
    "                \n",
    "                # Node labels\n",
    "                for cid, pt in node_pts.items():\n",
    "                    if pt is None: continue\n",
    "                    label = f\"M{cid}\"\n",
    "                    t = ax.text(pt.x, pt.y, label, fontsize=8.5,\n",
    "                                color=\"black\", ha=\"center\", va=\"center\", zorder=4)\n",
    "                    t.set_path_effects([path_effects.Stroke(linewidth=2.4,\n",
    "                                                           foreground=\"white\", alpha=0.8),\n",
    "                                        path_effects.Normal()])\n",
    "\n",
    "                ax.set_title(f\"[{key}] {weight} · {method} · Y{y}\\nCommunity flows\", fontsize=13, y=1.02)\n",
    "\n",
    "                # ---- 범례 ----\n",
    "                handles_main = []\n",
    "                # DII legend\n",
    "                handles_main.append(Line2D([0],[0], color=\"none\", label=\"Dominance Index (DII)\"))\n",
    "                DII_LEGEND_SHRINK = 0.3\n",
    "                legend_dii_levels = [0.5, 1.0, 2.0, DII_CLIP_HI]\n",
    "                legend_dii_levels = [lv for lv in legend_dii_levels if lv <= DII_CLIP_HI]\n",
    "                legend_dii_labels = sorted(list(set([l for l in [0.5, 1.0, 2.0, DII_CLIP_HI] if l <= DII_CLIP_HI])))\n",
    "                for lab in legend_dii_labels:\n",
    "                    ms = BASE_NODE_SIZE * lab \n",
    "                    handles_main.append(Line2D([], [], marker=\"o\", linestyle=\"None\", markersize=np.sqrt(ms), markerfacecolor=\"#66bb6a\", markeredgecolor=\"white\", markeredgewidth=0.8, label=f\"  ×{lab:g}\"))\n",
    "                    handles_main.append(Line2D([0],[0], color=\"none\", linewidth=0.2, label = None))\n",
    "\n",
    "                # RSI legend\n",
    "                handles_main.append(Line2D([0],[0], color=\"none\", label=\"RSI (percentile)\"))\n",
    "                if flows and len(rsi_vals) > 0:\n",
    "                    pct_targets = RSI_PCT_LABELS\n",
    "                    keys_list = list(flows.keys())\n",
    "                    vals_list = list(rsi.values())\n",
    "                    for p in pct_targets:\n",
    "                        rv = np.quantile(rsi_vals, p)\n",
    "                        idx = np.argmin(np.abs(np.array(vals_list) - rv))\n",
    "                        k_demo = keys_list[idx]\n",
    "                        alpha_val = alphas.get(k_demo, 0.9)\n",
    "                        width_demo = widths.get(k_demo, BASE_LW)\n",
    "                        handles_main.append(\n",
    "                            Line2D([0],[0], color=(0,0,0,alpha_val),\n",
    "                                   lw=width_demo, solid_capstyle=\"round\",\n",
    "                                   label=f\"  {vals_list[idx]*100:.2f}% (P{int(p*100)})\")\n",
    "                        )\n",
    "                else:\n",
    "                    handles_main.append(Line2D([0],[0], color=\"none\", label=\"  (no edges)\"))\n",
    "\n",
    "                # Source/Target\n",
    "                handles_main.append(Line2D([0],[0], color=\"none\", label=\"Source / Target\"))\n",
    "                handles_main.append(Patch(facecolor=\"black\", edgecolor=\"black\", label=\"  Source\"))\n",
    "                handles_main.append(Patch(facecolor=\"white\", edgecolor=\"black\", label=\"  Target\"))\n",
    "\n",
    "                leg1 = fig.legend(handles=handles_main, loc=\"center left\",\n",
    "                                  bbox_to_anchor=(1.0, 0.5), frameon=True,\n",
    "                                  fontsize=10, title=\"Legend\", title_fontsize=10)\n",
    "                leg1.get_frame().set_alpha(0.96)\n",
    "\n",
    "                plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "                fig.patch.set_facecolor(\"whitesmoke\")\n",
    "                plt.savefig(outpath, dpi=120, bbox_extra_artists=(leg1,), bbox_inches='tight')\n",
    "                #plt.show()\n",
    "                plt.close(fig)\n",
    "                gc.collect()\n",
    "                print(f\"  [OK] {outpath}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ef8d76",
   "metadata": {},
   "source": [
    "# 4. Inter hirarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953dfbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Paths / Parameters =========\n",
    "# [Modified] Change result save path to 'yearly_distribution'\n",
    "OUT_DIR = \"outputs/figures/communities/inter_hierarchy\"\n",
    "COMM_BASE_DIR = \"outputs/gpkg/communities\"  # …/{key}/{weight}_SGG_map_zone_community.gpkg\n",
    "COMMUNITY_METHODS = ['Infomap', \"Leiden\", \"Louvain\"]\n",
    "WEIGHTS = [\"거래관계\"]\n",
    "\n",
    "# [Added] Natural Breaks classification parameters\n",
    "N_CLASSES = 3  # Number of classes to divide DII into\n",
    "JITTER_STRENGTH = 0.1 # Node jitter strength along X-axis\n",
    "\n",
    "\n",
    "# ===== Main Loop =====\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Assume nw_dict is already loaded\n",
    "for key, ed_original in nw_dict.items():\n",
    "    for weight in WEIGHTS:\n",
    "        gpkg_path = os.path.join(COMM_BASE_DIR, key, f\"{weight}_SGG_map_zone_community.gpkg\")\n",
    "        if not os.path.exists(gpkg_path):\n",
    "            continue\n",
    "\n",
    "        g_comm = gpd.read_file(gpkg_path)\n",
    "        g_comm[\"SIGUNGU_CD\"] = g_comm[\"SIGUNGU_CD\"].astype(str)\n",
    "\n",
    "        for method in COMMUNITY_METHODS:\n",
    "            print(f\"--- Processing [{key} / {weight} / {method}] ---\")\n",
    "            \n",
    "            # --- 1. Aggregate DII data across all years ---\n",
    "            all_dii_records = [] # [{'year': y, 'cid': c, 'dii': d}, ...]\n",
    "            \n",
    "            years = sorted({int(c.split(\"_\")[-1]) for c in g_comm.columns if c.startswith(f\"{method}_canon_\")})\n",
    "            if not years:\n",
    "                print(\"  [SKIP] No canonical years found.\")\n",
    "                continue\n",
    "\n",
    "            # Standardize original data column names\n",
    "            ed = ed_original.copy()\n",
    "            rename_map = {\n",
    "                '1_기준연도': 'year', '8_시군구코드_seller': 'source', '14_시군구코드_buyer': 'target',\n",
    "                weight: 'weight'\n",
    "            }\n",
    "            ed.rename(columns={k: v for k, v in rename_map.items() if k in ed.columns}, inplace=True)\n",
    "            \n",
    "            required_cols = ['year', 'source', 'target', 'weight']\n",
    "            if not all(c in ed.columns for c in required_cols):\n",
    "                print(f\"  [SKIP] Missing required columns in edge data.\")\n",
    "                continue\n",
    "\n",
    "            ed_use = ed[required_cols].dropna().copy()\n",
    "            ed_use['source'] = ed_use['source'].astype(str)\n",
    "            ed_use['target'] = ed_use['target'].astype(str)\n",
    "            ed_use['weight'] = pd.to_numeric(ed_use['weight'], errors='coerce').fillna(0)\n",
    "\n",
    "            for year in years:\n",
    "                col_canon = f\"{method}_canon_{year}\"\n",
    "                if col_canon not in g_comm.columns:\n",
    "                    continue\n",
    "\n",
    "                s_year = g_comm[[\"SIGUNGU_CD\", col_canon]].dropna()\n",
    "                s_year[col_canon] = s_year[col_canon].astype(int)\n",
    "                lab_map = s_year.set_index(\"SIGUNGU_CD\")[col_canon].to_dict()\n",
    "\n",
    "                ede = ed_use[ed_use[\"year\"] == year].copy()\n",
    "                if ede.empty:\n",
    "                    continue\n",
    "                \n",
    "                ede[\"m_s\"] = ede[\"source\"].map(lab_map)\n",
    "                ede[\"m_t\"] = ede[\"target\"].map(lab_map)\n",
    "                ede = ede.dropna(subset=[\"m_s\", \"m_t\"])\n",
    "                ede[\"m_s\"] = ede[\"m_s\"].astype(int)\n",
    "                ede[\"m_t\"] = ede[\"m_t\"].astype(int)\n",
    "                ede = ede[ede[\"m_s\"] != ede[\"m_t\"]]\n",
    "                \n",
    "                flows = ede.groupby([\"m_s\", \"m_t\"])['weight'].sum()\n",
    "                \n",
    "                if not flows.empty:\n",
    "                    io_sum = {}\n",
    "                    for (ms, mt), val in flows.items():\n",
    "                        io_sum[ms] = io_sum.get(ms, 0) + val\n",
    "                        io_sum[mt] = io_sum.get(mt, 0) + val\n",
    "                    \n",
    "                    if not io_sum: continue\n",
    "                    \n",
    "                    mean_io = np.mean(list(io_sum.values()))\n",
    "                    if mean_io > 0:\n",
    "                        for cid, total_io in io_sum.items():\n",
    "                            dii = total_io / mean_io\n",
    "                            all_dii_records.append({'year': year, 'cid': cid, 'dii': dii})\n",
    "            \n",
    "            if not all_dii_records:\n",
    "                print(\"  [SKIP] No DII data to plot.\")\n",
    "                continue\n",
    "\n",
    "            # --- 2. Classify DII data and prepare for visualization ---\n",
    "            dii_df = pd.DataFrame(all_dii_records)\n",
    "            \n",
    "            # Create Natural Breaks classifier\n",
    "            # If fewer than 2 data points or fewer unique values than classes, adjust k\n",
    "            unique_dii_count = dii_df['dii'].nunique()\n",
    "            k = min(N_CLASSES, unique_dii_count)\n",
    "            \n",
    "            if k > 1:\n",
    "                classifier = mapclassify.NaturalBreaks(dii_df['dii'], k=k)\n",
    "                dii_df['dii_class'] = classifier.yb\n",
    "            else: # When classification is meaningless\n",
    "                dii_df['dii_class'] = 0\n",
    "                classifier = None\n",
    "\n",
    "            dii_df['dii_class'] = 3 - dii_df['dii_class'].astype(int)\n",
    "\n",
    "            # --- 3. Scatter plot visualization ---\n",
    "            outdir = os.path.join(OUT_DIR, key)\n",
    "            os.makedirs(outdir, exist_ok=True)\n",
    "            safe_name = lambda s: re.sub(r\"[^0-9A-Za-z_-]+\", \"_\", s)\n",
    "            outpath = os.path.join(outdir, f\"dii_yearly_dist_{safe_name(method)}.png\")\n",
    "\n",
    "            # Plot scatter separately by tier\n",
    "            fig, ax = plt.subplots(figsize=(14, 8), dpi=100)\n",
    "\n",
    "            marker_map = {\n",
    "                1: '*',  # Tier 1\n",
    "                2: 'o',  # Tier 2\n",
    "                3: '^'   # Tier 3\n",
    "            }\n",
    "\n",
    "            color_map = {cid: COLOR_OF.get(cid, 'black') for cid in dii_df['cid'].unique()}\n",
    "\n",
    "            legend_handles = []\n",
    "\n",
    "            # Plot scatter separately by tier\n",
    "            for tier, marker in marker_map.items():\n",
    "                subset = dii_df[dii_df['dii_class'] == tier]\n",
    "                if subset.empty:\n",
    "                    continue\n",
    "\n",
    "                # Build a list of colors for each row\n",
    "                colors = subset['cid'].map(color_map)\n",
    "\n",
    "                # Main scatter\n",
    "                ax.scatter(\n",
    "                    x=subset['year'],\n",
    "                    y=subset['dii'],\n",
    "                    c=colors,\n",
    "                    s=150,\n",
    "                    alpha=0.8,\n",
    "                    edgecolors=colors,\n",
    "                    linewidth=3,\n",
    "                    zorder=10,\n",
    "                    marker=marker\n",
    "                )\n",
    "\n",
    "                # --- Legend entries for each community (instead of text labels) ---\n",
    "                legend_handles.append(Patch(color='none', label=''))\n",
    "                for cid in subset['cid'].unique():\n",
    "                    legend_handles.append(Line2D(\n",
    "                        [], [], \n",
    "                        marker=marker, \n",
    "                        color=color_map[cid],\n",
    "                        markersize=9, \n",
    "                        linestyle='None',\n",
    "                        markeredgecolor=color_map[cid],\n",
    "                        markeredgewidth=2,\n",
    "                        label=f\"{cid}\"\n",
    "                    ))\n",
    "\n",
    "            # --- Graph style settings ---\n",
    "            ax.set_title(f\"Yearly DII Distribution of Communities (Natural Breaks)\\n[{key} / {weight} / {method}]\", fontsize=16, pad=15)\n",
    "            ax.set_xlabel(\"Year\", fontsize=12)\n",
    "            ax.set_ylabel(\"Dominance Index (DII)\", fontsize=12)\n",
    "            ax.grid(True, which='major', linestyle=':', linewidth=0.5, color='gray', alpha=0.5)\n",
    "            \n",
    "            ax.axhline(1.0, color='red', linestyle='--', linewidth=1.2, label='Average (DII=1.0)')\n",
    "            \n",
    "            # Set X-axis ticks to integer years\n",
    "            ax.set_xticks(years)\n",
    "            plt.xticks(rotation=0)\n",
    "            ax.set_ylim(bottom=0, top=4.5)\n",
    "            \n",
    "            # Y-axis scale (consider log scale if values are very large)\n",
    "            # ax.set_yscale('log')\n",
    "\n",
    "            # --- Create legend ---\n",
    "            handles = [ax.get_legend_handles_labels()[0][0]] # Add handle for DII=1.0 line\n",
    "            labels  = [ax.get_legend_handles_labels()[1][0]]\n",
    "            handles.append(Patch(color='none', label='')) # Spacer\n",
    "            labels.append('')\n",
    "\n",
    "            if classifier:\n",
    "                for i in range(k):\n",
    "                    marker = marker_map.get(i+1, 'o')\n",
    "                    lower_bound = dii_df['dii'].min() if i == 0 else classifier.bins[i-1]\n",
    "                    upper_bound = classifier.bins[i]\n",
    "                    tier_label = f\"Tier {i+1} ({lower_bound:.2f} ~ {upper_bound:.2f})\"\n",
    "\n",
    "                    # Tier 범위 라벨 (섹션 헤더 역할)\n",
    "                    handles.append(Patch(color='none', label=tier_label))\n",
    "                    labels.append(tier_label)\n",
    "\n",
    "                    # 해당 Tier에 속한 community들 추가\n",
    "                    subset = dii_df[dii_df['dii_class'] == (i+1)]\n",
    "                    for cid in subset['cid'].unique():\n",
    "                        handles.append(Line2D([], [], \n",
    "                                            marker=marker,\n",
    "                                            color=color_map[cid],\n",
    "                                            markersize=9,\n",
    "                                            linestyle='None',\n",
    "                                            markeredgecolor=color_map[cid],\n",
    "                                            markeredgewidth=2,\n",
    "                                            label=f\"M{cid}\"))\n",
    "                        labels.append(f\"M{cid}\")\n",
    "\n",
    "            ax.legend(handles=handles, labels=labels,\n",
    "                    loc='center left', bbox_to_anchor=(1.02, 0.5),\n",
    "                    title=\"Legend\", fontsize=10)\n",
    "\n",
    "\n",
    "            plt.tight_layout(rect=[0, 0, 0.88, 1]) # Adjust layout so legend isn’t cut off\n",
    "            ax.patch.set_facecolor(\"whitesmoke\")\n",
    "\n",
    "            plt.savefig(outpath, dpi=100, bbox_inches='tight')\n",
    "            #plt.show()\n",
    "            plt.close(fig)\n",
    "            gc.collect()\n",
    "            print(f\"  [OK] Saved DII distribution plot to {outpath}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firm-transaction-network",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
