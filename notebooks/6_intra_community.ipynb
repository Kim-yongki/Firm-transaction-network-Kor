{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96f889aa",
   "metadata": {},
   "source": [
    "## 0. Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f16eb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Standard libraries\n",
    "import gc\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import hashlib\n",
    "\n",
    "GLOBAL_SEED = 20250922\n",
    "\n",
    "\n",
    "# 2. Third-party libraries\n",
    "import geopandas as gpd\n",
    "import mapclassify\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "from shapely.ops import nearest_points\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 3. Visualization libraries (Matplotlib)\n",
    "import matplotlib.image as mimg\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors as mcolors\n",
    "from matplotlib import font_manager as fm\n",
    "from matplotlib import patheffects as path_effects\n",
    "from matplotlib.collections import LineCollection\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.offsetbox import AnnotationBbox, OffsetImage\n",
    "from matplotlib.patches import Patch\n",
    "from shapely.affinity import translate as shp_translate\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "if os.getcwd().endswith('notebooks'):\n",
    "    os.chdir('..')\n",
    "\n",
    "# 1) Specify font using absolute path (modify filename/path accordingly)\n",
    "font_regular = os.path.abspath(\"assets/malgun.ttf\")     # Regular\n",
    "font_bold    = os.path.abspath(\"assets/malgunbd.ttf\")   # Bold\n",
    "\n",
    "# 3) Register fonts in Matplotlib font manager\n",
    "fm.fontManager.addfont(font_regular)\n",
    "fm.fontManager.addfont(font_bold)\n",
    "\n",
    "# 4) Safely extract the family name from the font file and apply globally\n",
    "fam = fm.FontProperties(fname=font_regular).get_name()  # Should be 'Malgun Gothic'\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": fam,\n",
    "    \"axes.unicode_minus\": False,  # Prevent minus sign from breaking\n",
    "})\n",
    "\n",
    "ARROW_FILE    = \"assets/north_arrow.png\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02a8839",
   "metadata": {},
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7c5d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SGG_map = gpd.read_file('data/processed/map/SGG_map.gpkg')\n",
    "SGG_map['centroid'] = SGG_map.geometry.centroid\n",
    "\n",
    "# Get all matching CSV files\n",
    "all_files = glob.glob(\"data/processed/deal_network/deal_by/network_by_*.csv\")\n",
    "\n",
    "# Create a dictionary to store DataFrames\n",
    "nw_dict = {}\n",
    "\n",
    "for file in all_files:\n",
    "    # Extract key from file name, e.g., 'network_by_age (man).csv' → 'age (man)'\n",
    "    key = os.path.basename(file).replace(\"network_by_\", \"\").replace(\".csv\", \"\")\n",
    "    \n",
    "    # Read the CSV file\n",
    "    nw = pd.read_csv(file, dtype={'14_시군구코드_buyer':str, '8_시군구코드_seller':str, '1_기준연도':int})\n",
    "    nw = nw[~nw['14_시군구코드_buyer'].str.startswith('9999') & ~nw['8_시군구코드_seller'].str.startswith('9999')].copy()\n",
    "    \n",
    "    nw.columns = ['year', 'source', 'target', '거래관계']\n",
    "    nw = pd.merge(nw, SGG_map[['SIG_CD', 'centroid']], left_on='source', right_on='SIG_CD', how='left')\n",
    "    nw = pd.merge(nw, SGG_map[['SIG_CD', 'centroid']], left_on='target', right_on='SIG_CD', how='left', suffixes=('_seller', '_buyer'))\n",
    "    nw = nw.drop(columns=['SIG_CD_seller', 'SIG_CD_buyer'])\n",
    "    \n",
    "    # Store DataFrame in dictionary\n",
    "    nw_dict[key] = nw\n",
    "    \n",
    "desired_order = [\n",
    "    'all',\n",
    "    'man',\n",
    "    'innovation',\n",
    "    'urban_size_소상공인',\n",
    "    'urban_size_중소기업',\n",
    "    'urban_size_중견기업',\n",
    "    'urban_size_대기업',\n",
    "    'urban_age_1년 미만',\n",
    "    'urban_age_1~5년 미만',\n",
    "    'urban_age_5~10년 미만',\n",
    "    'urban_age_10년 이상'\n",
    "]\n",
    "\n",
    "# 2. 새로운 딕셔너리를 만들어 순서를 적용합니다.\n",
    "#    (혹시 모를 에러를 방지하기 위해 nw_dict에 실제 있는 키만 가져옵니다.)\n",
    "ordered_nw_dict = {key: nw_dict[key] for key in desired_order if key in nw_dict}\n",
    "\n",
    "# 3. 원래 nw_dict를 정렬된 딕셔너리로 교체합니다.\n",
    "nw_dict = ordered_nw_dict\n",
    "\n",
    "# ===== 유틸리티 함수 =====\n",
    "def build_color_table(max_id=600):\n",
    "    \"\"\"고유 ID에 대한 색상 테이블 생성 (노드 라벨 색상에 사용)\"\"\"\n",
    "    pal = list(plt.get_cmap(\"Set1\").colors) \\\n",
    "        + list(plt.get_cmap(\"Dark2\").colors) \\\n",
    "        + list(plt.get_cmap(\"Accent\").colors)\n",
    "    return {cid: mcolors.to_rgba(pal[cid % len(pal)], 1.0) for cid in range(max_id)}\n",
    "\n",
    "COLOR_OF = build_color_table(600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e260f4",
   "metadata": {},
   "source": [
    "## 2. Data table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d813838",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIDO_NM_map = {\n",
    "    '11': '서울특별시', '26': '부산광역시', '27': '대구광역시', '28': '인천광역시',\n",
    "    '29': '광주광역시', '30': '대전광역시', '31': '울산광역시', '36': '세종특별자치시',\n",
    "    '41': '경기도', '51': '강원특별자치도', '43': '충청북도', '44': '충청남도',\n",
    "    '52': '전북특별자치도', '46': '전라남도', '47': '경상북도', '48': '경상남도',\n",
    "    '50': '제주특별자치도'\n",
    "}\n",
    "\n",
    "# ========= Paths / Parameters =========\n",
    "BASE_DEAL_DIR = \"data/processed/deal_network/deal_by\"\n",
    "COMM_BASE_DIR = \"outputs/gpkg/communities\"\n",
    "TABLE_DIR     = \"outputs/tables/intra_communities_DII_RSI\"\n",
    "os.makedirs(TABLE_DIR, exist_ok=True)\n",
    "\n",
    "COMMUNITY_METHODS = ['Infomap', \"Leiden\", \"Louvain\"]\n",
    "WEIGHTS           = [\"거래관계\"]\n",
    "\n",
    "# ===== Utility functions =====\n",
    "def compute_dii_rsi_intra_per_community(ed: pd.DataFrame, lab_map: dict, weight_col: str, year: int, method: str):\n",
    "    \"\"\"\n",
    "    커뮤니티 내부 edge만 고려하여 각 커뮤니티 안에서 시군구 단위 DII, RSI 계산\n",
    "    \"\"\"\n",
    "    ed_use = ed.dropna(subset=[\"source\",\"target\",weight_col]).copy()\n",
    "    ed_use[weight_col] = pd.to_numeric(ed_use[weight_col], errors=\"coerce\").fillna(0)\n",
    "\n",
    "    # 소속 커뮤니티 매핑\n",
    "    ed_use[\"c_s\"] = ed_use[\"source\"].map(lab_map)\n",
    "    ed_use[\"c_t\"] = ed_use[\"target\"].map(lab_map)\n",
    "    ed_use = ed_use.dropna(subset=[\"c_s\",\"c_t\"])\n",
    "\n",
    "    results_rsi, results_dii = [], []\n",
    "\n",
    "    for comm in sorted(set(lab_map.values())):\n",
    "        sub = ed_use[(ed_use[\"c_s\"] == comm) & (ed_use[\"c_t\"] == comm)].copy()\n",
    "        if sub.empty:\n",
    "            continue\n",
    "\n",
    "        # RSI (edge-level within community)\n",
    "        total_flow = sub[weight_col].sum()\n",
    "        if total_flow > 0:\n",
    "            sub[\"RSI\"] = sub[weight_col] / total_flow * 100\n",
    "        else:\n",
    "            sub[\"RSI\"] = 0.0\n",
    "        rsi_df = sub[[\"source\",\"target\",weight_col,\"RSI\"]].copy()\n",
    "        rsi_df[\"year\"] = year\n",
    "        rsi_df[\"community\"] = comm\n",
    "        rsi_df[\"method\"] = method\n",
    "        results_rsi.append(rsi_df)\n",
    "\n",
    "        # DII (node-level within community)\n",
    "        inflow = sub.groupby(\"target\")[weight_col].sum()\n",
    "        outflow = sub.groupby(\"source\")[weight_col].sum()\n",
    "        io_sum = inflow.add(outflow, fill_value=0)\n",
    "        mean_io = io_sum.mean() if not io_sum.empty else 1.0\n",
    "        dii = io_sum / mean_io if mean_io > 0 else io_sum\n",
    "        dii_df = dii.rename(\"DII\").reset_index().rename(columns={dii.index.name:\"SIG_CD\"})\n",
    "        dii_df[\"year\"] = year\n",
    "        dii_df[\"community\"] = comm\n",
    "        dii_df[\"method\"] = method\n",
    "        results_dii.append(dii_df)\n",
    "\n",
    "    return (\n",
    "        pd.concat(results_rsi, ignore_index=True) if results_rsi else pd.DataFrame(),\n",
    "        pd.concat(results_dii, ignore_index=True) if results_dii else pd.DataFrame()\n",
    "    )\n",
    "\n",
    "# ===== Main loop =====\n",
    "for key, ed in nw_dict.items():\n",
    "    all_rsi, all_dii = [], []\n",
    "\n",
    "    for weight in WEIGHTS:\n",
    "        gpkg = os.path.join(COMM_BASE_DIR, f\"{weight}_{key}_SGG_map_zone_community.gpkg\")\n",
    "        if not os.path.exists(gpkg): \n",
    "            continue\n",
    "\n",
    "        g = gpd.read_file(gpkg)\n",
    "        if \"SIGUNGU_CD\" not in g.columns:\n",
    "            print(f\"[SKIP] {key}/{weight}: SIGUNGU_CD not found\")\n",
    "            continue\n",
    "        g[\"SIGUNGU_CD\"] = g[\"SIGUNGU_CD\"].astype(str)\n",
    "\n",
    "        years = sorted({int(c.split(\"_\")[-1]) for c in g.columns \n",
    "                        if c.startswith(tuple(f\"{m}_canon_\" for m in COMMUNITY_METHODS))})\n",
    "        if not years: \n",
    "            print(f\"[SKIP] {key}/{weight}: no canonical years\")\n",
    "            continue\n",
    "\n",
    "        if not all(c in ed.columns for c in ['year','source','target',weight]):\n",
    "            print(f\"[SKIP] {key}: edges missing required columns\")\n",
    "            continue\n",
    "\n",
    "        ed_use = ed.dropna(subset=[\"source\",\"target\",\"year\"]).copy()\n",
    "        ed_use[\"source\"] = ed_use[\"source\"].astype(str)\n",
    "        ed_use[\"target\"] = ed_use[\"target\"].astype(str)\n",
    "        ed_use[weight] = pd.to_numeric(ed_use[weight], errors=\"coerce\").fillna(0)\n",
    "\n",
    "        for method in COMMUNITY_METHODS:\n",
    "            for y in years:\n",
    "                col_canon = f\"{method}_canon_{y}\"\n",
    "                if col_canon not in g.columns: \n",
    "                    continue\n",
    "\n",
    "                s_year = g[[\"SIGUNGU_CD\", col_canon]].dropna(subset=[col_canon])\n",
    "                s_year[col_canon] = s_year[col_canon].astype(int)\n",
    "                lab_map = s_year.set_index(\"SIGUNGU_CD\")[col_canon].to_dict()\n",
    "\n",
    "                ede = ed_use[ed_use[\"year\"] == y].copy()\n",
    "                if ede.empty: \n",
    "                    continue\n",
    "\n",
    "                # intra-community RSI/DII\n",
    "                rsi_df, dii_df = compute_dii_rsi_intra_per_community(ede, lab_map, weight, year=y, method=method)\n",
    "                if not rsi_df.empty:\n",
    "                    rsi_df[\"key\"] = key\n",
    "                    all_rsi.append(rsi_df)\n",
    "                if not dii_df.empty:\n",
    "                    dii_df[\"key\"] = key\n",
    "                    all_dii.append(dii_df)\n",
    "\n",
    "    # ===== Post-process records =====\n",
    "    if all_dii:\n",
    "        dii_records = pd.concat(all_dii, ignore_index=True)\n",
    "        if 'index' in dii_records.columns:\n",
    "            dii_records['SIG_CD'] = dii_records['index'].astype(str)\n",
    "            dii_records.drop(columns=['index'], inplace=True)\n",
    "\n",
    "        # Merge with SGG names\n",
    "        SGG_map_temp = SGG_map[['SIG_CD','SIG_KOR_NM']].copy()\n",
    "        SGG_map_temp['SIGUNGU_NM'] = SGG_map_temp['SIG_KOR_NM']\n",
    "        dii_records = pd.merge(dii_records, SGG_map_temp[['SIG_CD','SIGUNGU_NM']], on='SIG_CD', how='left')\n",
    "        \n",
    "\n",
    "        dii_classifier = mapclassify.NaturalBreaks(dii_records[\"DII\"], k=5)\n",
    "        dii_records[\"DII_class\"] = 5 - dii_classifier.yb  # Classes start from 1\n",
    "\n",
    "\n",
    "        # Merge with SIDO names\n",
    "        dii_records['SIDO_CD'] = dii_records['SIG_CD'].str[:2].str.zfill(2)\n",
    "        dii_records['SIDO_NM'] = dii_records['SIDO_CD'].map(SIDO_NM_map)\n",
    "        dii_records.drop(columns=['SIDO_CD'], inplace=True)\n",
    "\n",
    "        DII_OUT = os.path.join(TABLE_DIR, f\"dii_{key}.csv\")\n",
    "        dii_records.to_csv(DII_OUT, index=False, encoding=\"cp949\")\n",
    "        print(f\"[OK] DII saved to {DII_OUT}\")\n",
    "    else:\n",
    "        print(f\"[WARN] No DII results for {key}\")\n",
    "\n",
    "    if all_rsi:\n",
    "        rsi_records = pd.concat(all_rsi, ignore_index=True)\n",
    "\n",
    "        # Merge with SGG names\n",
    "        SGG_map_temp = SGG_map[['SIG_CD','SIG_KOR_NM']].copy()\n",
    "        SGG_map_temp['source'] = SGG_map_temp['SIG_CD'].astype(str)\n",
    "        SGG_map_temp['target'] = SGG_map_temp['SIG_CD'].astype(str)\n",
    "        SGG_map_temp['SIGUNGU_NM_source'] = SGG_map_temp['SIG_KOR_NM']\n",
    "        SGG_map_temp['SIGUNGU_NM_target'] = SGG_map_temp['SIG_KOR_NM']\n",
    "\n",
    "        rsi_records = pd.merge(rsi_records, SGG_map_temp[['source','SIGUNGU_NM_source']], on='source', how='left')\n",
    "        rsi_records = pd.merge(rsi_records, SGG_map_temp[['target','SIGUNGU_NM_target']], on='target', how='left')\n",
    "\n",
    "        # Merge with SIDO names\n",
    "        rsi_records['SIDO_source'] = rsi_records['source'].str[:2].str.zfill(2)\n",
    "        rsi_records['SIDO_target'] = rsi_records['target'].str[:2].str.zfill(2)\n",
    "        rsi_records['SIDO_NM_source'] = rsi_records['SIDO_source'].map(SIDO_NM_map)\n",
    "        rsi_records['SIDO_NM_target'] = rsi_records['SIDO_target'].map(SIDO_NM_map)\n",
    "        rsi_records.drop(columns=['SIDO_source','SIDO_target'], inplace=True)\n",
    "\n",
    "        RSI_OUT = os.path.join(TABLE_DIR, f\"rsi_{key}.csv\")\n",
    "        rsi_records.to_csv(RSI_OUT, index=False, encoding=\"cp949\")\n",
    "        print(f\"[OK] RSI saved to {RSI_OUT}\")\n",
    "    else:\n",
    "        print(f\"[WARN] No RSI results for {key}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa7dec3",
   "metadata": {},
   "source": [
    "# 3. Intra commuity geovisualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ab52a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Internal community-flow plotter with island insets (Jeju / Ulleung)\n",
    "- If Jeju (50110, 50130) or Ulleung (47940) are > 50 km from other nodes,\n",
    "  move them near the mainland edge (no polygon overlap), draw a dashed box,\n",
    "  and set axes limits based on node (centroid) extent, not polygon extent.\n",
    "\n",
    "Assumptions\n",
    "- CRS: EPSG:5179 (meters)\n",
    "- `nw_dict` is a dict-like: {key: DataFrame of edges}, already loaded by user\n",
    "- Community GPKGs exist at: outputs/gpkg/communities/{key}/{weight}_SGG_map_zone_community.gpkg\n",
    "- Edge columns (renamed below if needed): 'source', 'target', 'year', and weight (e.g., '거래관계')\n",
    "\"\"\"\n",
    "\n",
    "# ========= 경로/파라미터 =========\n",
    "BASE_DEAL_DIR = \"data/processed/deal_network/deal_by\"\n",
    "COMM_BASE_DIR = \"outputs/gpkg/communities\"\n",
    "SGG_MAP_PATH  = \"data/processed/map/SGG_map.gpkg\"\n",
    "SIDO_PATH     = \"data/raw/bnd_sido_00_2024_2Q/bnd_sido_00_2024_2Q.shp\"\n",
    "OUT_DIR_ORIGINAL = \"outputs/figures/communities/intra_flows/intra_flows_original\"\n",
    "OUT_DIR_BACKBONE = \"outputs/figures/communities/intra_flows/intra_flows_backbone\"\n",
    "ARROW_FILE    = \"north_arrow.png\"\n",
    "\n",
    "COMMUNITY_METHODS = ['Infomap', \"Leiden\", \"Louvain\"]\n",
    "WEIGHTS           = [\"거래관계\"]\n",
    "ALPHA_LEVEL       = 0.1\n",
    "\n",
    "# --- [MODIFIED] Original Link/Node 스타일 ---\n",
    "BASE_LW, LW_RANGE, RSI_W_BLEND = 0.5, 4.5, 0.30\n",
    "MIN_ALPHA, MAX_ALPHA, N_SEG = 0.1, 0.99, 30\n",
    "DII_BASE, DII_RANGE, DII_CLIP_HI = 10, 80, 4.0\n",
    "FONT_SIZE_MIN, FONT_SIZE_MAX = 4.0, 6.0\n",
    "\n",
    "# --- [MODIFIED] Backbone Link/Node 스타일 ---\n",
    "BASE_LW_bb, LW_RANGE_bb, RSI_W_BLEND_bb = 1, 6, 0.30\n",
    "MIN_ALPHA_bb, MAX_ALPHA_bb = 0.1, 0.99\n",
    "\n",
    "# --- RSI 범례 ---\n",
    "RSI_CLIP_HI_QUANT = 0.999\n",
    "RSI_PCT_LABELS    = [0.90, 0.95, 0.99, 0.999]\n",
    "CURVATURE = 0.22\n",
    "\n",
    "# ========= 유틸리티 함수 =========\n",
    "JEJU_CODES     = {\"50110\",\"50130\"}  # 제주시, 서귀포시\n",
    "ULLEUNG_CODES  = {\"47940\"}           # 울릉군\n",
    "\n",
    "\n",
    "def extract_disparity_backbone(df: pd.DataFrame, source_col:str, target_col:str, weight_col: str, alpha: float = 0.05) -> pd.DataFrame:\n",
    "    \"\"\"Serrano et al. (2009) disparity filter backbone (outgoing). Returns subset of df.\"\"\"\n",
    "    edges = df[[source_col, target_col, weight_col]].copy()\n",
    "    edges.rename(columns={source_col: 'source', target_col: 'target', weight_col: 'weight'}, inplace=True)\n",
    "    node_stats = edges.groupby('source')['weight'].agg(['sum', 'count']).rename(columns={'sum': 's_out', 'count': 'k_out'})\n",
    "    edges = edges.merge(node_stats, left_on='source', right_index=True)\n",
    "    edges['p_ij'] = (edges['weight'] / edges['s_out']).fillna(0)\n",
    "    k_minus_1 = edges['k_out'] - 1\n",
    "    alpha_ij = np.power(1.0 - edges['p_ij'], k_minus_1.clip(lower=0))\n",
    "    alpha_ij[edges['k_out'] <= 1] = 0.0\n",
    "    backbone_edges = edges[alpha_ij < alpha]\n",
    "    return df.merge(\n",
    "        backbone_edges[['source', 'target']].rename(columns={'source': source_col, 'target': target_col}),\n",
    "        on=[source_col, target_col], how='inner'\n",
    "    )\n",
    "\n",
    "\n",
    "def _norm01(a):\n",
    "    a = np.asarray(a, float)\n",
    "    if a.size == 0:\n",
    "        return a\n",
    "    vmin, vmax = np.nanmin(a), np.nanmax(a)\n",
    "    if not (np.isfinite(vmin) and np.isfinite(vmax) and vmax > vmin):\n",
    "        return np.full_like(a, 0.5)\n",
    "    return (a - vmin) / (vmax - vmin)\n",
    "\n",
    "\n",
    "def _curve_segments(p1, p2, curvature=CURVATURE, nseg=N_SEG):\n",
    "    mx, my = (p1.x + p2.x)/2, (p1.y + p2.y)/2\n",
    "    dx, dy = p2.x - p1.x, p2.y - p1.y\n",
    "    dist = np.hypot(dx, dy)\n",
    "    if dist == 0:\n",
    "        return None, None\n",
    "    px, py = -dy/dist, dx/dist\n",
    "    cx, cy = mx + px * dist * curvature, my + py * dist * curvature\n",
    "    t = np.linspace(0, 1, nseg + 1)\n",
    "    bx = (1-t)**2*p1.x + 2*(1-t)*t*cx + t**2*p2.x\n",
    "    by = (1-t)**2*p1.y + 2*(1-t)*t*cy + t**2*p2.y\n",
    "    segs = np.stack([np.column_stack([bx[:-1], by[:-1]]), np.column_stack([bx[1:], by[1:]])], axis=1)\n",
    "    cols = np.linspace((1., 0., 0., 1.), (1., 1., 0., 1.), nseg)  # red -> yellow RGBA\n",
    "    return segs, cols\n",
    "\n",
    "\n",
    "def add_north_arrow(ax, x, y, arrow_file, zoom=0.18):\n",
    "    try:\n",
    "        import matplotlib.image as mimg\n",
    "        from matplotlib.offsetbox import AnnotationBbox, OffsetImage\n",
    "        if os.path.exists(arrow_file):\n",
    "            im = mimg.imread(arrow_file)\n",
    "            ab = AnnotationBbox(OffsetImage(im, zoom=zoom), (x, y), xycoords='axes fraction', frameon=False)\n",
    "            ax.add_artist(ab)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not add north arrow: {e}\")\n",
    "\n",
    "\n",
    "def add_scale_bar(ax, location=(0.06,0.03), linewidth=3, color='black'):\n",
    "    xlim = ax.get_xlim(); ylim = ax.get_ylim()\n",
    "    map_width = xlim[1] - xlim[0]\n",
    "    candidates = [1000, 2000, 5000, 10000, 20000, 50000, 100000, 200000]\n",
    "    length = max([c for c in candidates if c <= map_width / 4], default=min(candidates))\n",
    "    sb_x = xlim[0] + (xlim[1] - xlim[0]) * location[0]\n",
    "    sb_y = ylim[0] + (ylim[1] - ylim[0]) * location[1]\n",
    "    ax.plot([sb_x, sb_x + length], [sb_y, sb_y], color=color, linewidth=linewidth)\n",
    "    ax.text(sb_x + length / 2, sb_y, f'{round(length/1000):,} km', va='bottom', ha='center', fontsize=9)\n",
    "\n",
    "\n",
    "def apply_island_insets(SGG: gpd.GeoDataFrame,\n",
    "                        SGG_pts: pd.Series,\n",
    "                        members: list,\n",
    "                        threshold_km: float = 50.0,\n",
    "                        side_for_group: dict = None):\n",
    "    \"\"\"\n",
    "    If Jeju / Ulleung centroids are farther than threshold_km from mainland nodes,\n",
    "    move each island group to an inset location near the mainland bounding box edge.\n",
    "\n",
    "    IMPORTANT: When moving, only move the polygon *that contains the node (centroid)*\n",
    "    for each SIG_CD in the island group. Small detached islets (e.g., Dokdo) are\n",
    "    not moved (and effectively dropped from the inset view), so the inset shows\n",
    "    just the main landmass per node.\n",
    "\n",
    "    Returns: (moved_polygons_gdf, moved_points_series, list_of_rectangle_patches)\n",
    "    \"\"\"\n",
    "    from shapely.ops import unary_union\n",
    "\n",
    "    threshold_m = threshold_km * 1000.0\n",
    "    side_for_group = side_for_group or {\n",
    "        \"JEJU\": \"bottom\",     # left/right/top/bottom\n",
    "        \"ULLEUNG\": \"right\",\n",
    "    }\n",
    "\n",
    "    mem_set = set(map(str, members))\n",
    "    g_comm  = SGG[SGG[\"SIG_CD\"].isin(mem_set)].copy()\n",
    "    pts_all = SGG_pts.copy()\n",
    "\n",
    "    groups = [(\"JEJU\", JEJU_CODES & mem_set), (\"ULLEUNG\", ULLEUNG_CODES & mem_set)]\n",
    "    groups = [(name, codes) for name, codes in groups if len(codes) > 0]\n",
    "\n",
    "    island_all_codes = set().union(*[codes for _, codes in groups]) if groups else set()\n",
    "    mainland_codes   = list(mem_set - island_all_codes)\n",
    "    if len(mainland_codes) == 0:\n",
    "        return g_comm, pts_all, []  # nothing to do if no mainland\n",
    "\n",
    "    # Mainland bbox for where to dock insets\n",
    "    g_main = g_comm[g_comm[\"SIG_CD\"].isin(mainland_codes)]\n",
    "    minx, miny, maxx, maxy = g_main.total_bounds\n",
    "    main_w, main_h = (maxx - minx), (maxy - miny)\n",
    "    max_range = max(main_w, main_h)\n",
    "    gap = max(50000.0, 0.01 * max_range)  # >= 50 km or 1%\n",
    "\n",
    "    main_pts = pts_all[pts_all.index.isin(mainland_codes)]\n",
    "    inset_boxes = []\n",
    "\n",
    "    # Helper: pick the polygon part that contains (or is closest to) the centroid\n",
    "    def _pick_main_part(geom, pt):\n",
    "        try:\n",
    "            parts = list(geom.geoms) if hasattr(geom, \"geoms\") else [geom]\n",
    "        except Exception:\n",
    "            parts = [geom]\n",
    "        # Prefer any part that covers the centroid\n",
    "        covers = [p for p in parts if p.buffer(0).covers(pt)]\n",
    "        if covers:\n",
    "            # If multiple cover (rare), choose the largest area\n",
    "            return max(covers, key=lambda p: p.area)\n",
    "        # Fallback: choose the nearest polygon to the centroid\n",
    "        return min(parts, key=lambda p: p.distance(pt))\n",
    "\n",
    "    for name, codes in groups:\n",
    "        isl_pts = pts_all[pts_all.index.isin(codes)]\n",
    "        if isl_pts.empty or main_pts.empty:\n",
    "            continue\n",
    "\n",
    "        # Measure distance from island group center to any mainland node\n",
    "        cx, cy = isl_pts.x.mean(), isl_pts.y.mean()\n",
    "        dmin = float(\"inf\")\n",
    "        for _, p in main_pts.items():\n",
    "            d = ((p.x - cx)**2 + (p.y - cy)**2)**0.5\n",
    "            if d < dmin:\n",
    "                dmin = d\n",
    "        if dmin <= threshold_m:\n",
    "            continue  # close enough → no inset move\n",
    "\n",
    "        # Build a geometry set made ONLY of the polygon that contains each node (centroid)\n",
    "        selected_polys = []\n",
    "        for c in codes:\n",
    "            row = g_comm[g_comm[\"SIG_CD\"] == c]\n",
    "            if row.empty:\n",
    "                continue\n",
    "            geom = row.iloc[0].geometry\n",
    "            pt   = pts_all.loc[c]\n",
    "            main_part = _pick_main_part(geom, pt)\n",
    "            # Replace geometry for this code with only the main part (drop tiny islets like Dokdo)\n",
    "            g_comm.loc[g_comm[\"SIG_CD\"] == c, \"geometry\"] = main_part\n",
    "            selected_polys.append(main_part)\n",
    "\n",
    "        if not selected_polys:\n",
    "            continue\n",
    "\n",
    "        grp_union = unary_union(selected_polys)\n",
    "        gx0, gy0, gx1, gy1 = grp_union.bounds\n",
    "        gw, gh = (gx1 - gx0), (gy1 - gy0)\n",
    "\n",
    "        side = side_for_group.get(name, \"left\")\n",
    "        if side == \"left\":\n",
    "            target_minx = minx - gap - gw\n",
    "            target_miny = miny + gap\n",
    "        elif side == \"right\":\n",
    "            target_minx = maxx + gap\n",
    "            target_miny = miny + gap*3\n",
    "        elif side == \"top\":\n",
    "            target_minx = minx + gap\n",
    "            target_miny = maxy + gap\n",
    "        else:  # bottom\n",
    "            target_minx = minx + gap*4\n",
    "            target_miny = miny - gap - gh\n",
    "\n",
    "        dx, dy = target_minx - gx0, target_miny - gy0\n",
    "\n",
    "        # Translate ONLY the selected island codes (their geometries are already reduced to main parts)\n",
    "        mask_grp = g_comm[\"SIG_CD\"].isin(codes)\n",
    "        g_comm.loc[mask_grp, \"geometry\"] = g_comm.loc[mask_grp, \"geometry\"].apply(lambda geom: shp_translate(geom, xoff=dx, yoff=dy))\n",
    "        for c in codes:\n",
    "            pts_all.loc[c] = shp_translate(pts_all.loc[c], xoff=dx, yoff=dy)\n",
    "\n",
    "        # Build an inset rectangle around the moved main parts\n",
    "        moved_union = shp_translate(grp_union, xoff=dx, yoff=dy)\n",
    "        x0, y0, x1, y1 = moved_union.bounds\n",
    "        pad = max(5000.0, 0.01 * max_range)\n",
    "        rect = mpatches.Rectangle((x0 - pad, y0 - pad), (x1 - x0) + 2*pad, (y1 - y0) + 2*pad,\n",
    "                                  fill=False, linewidth=1.2, linestyle=\"--\", edgecolor=\"black\", zorder=5)\n",
    "        inset_boxes.append(rect)\n",
    "\n",
    "    return g_comm, pts_all, inset_boxes\n",
    "\n",
    "\n",
    "# ========= 데이터 준비 =========\n",
    "SGG = gpd.read_file(SGG_MAP_PATH).to_crs(epsg=5179)\n",
    "if \"SIG_KOR_NM\" not in SGG.columns and \"SIG_ENG_NM\" in SGG.columns:\n",
    "    SGG[\"SIG_KOR_NM\"] = SGG[\"SIG_ENG_NM\"]\n",
    "SGG[\"centroid\"] = SGG.geometry.centroid\n",
    "SGG_pts = SGG.set_index(\"SIG_CD\")[\"centroid\"]\n",
    "SGG_names = SGG.set_index(\"SIG_CD\")[\"SIG_KOR_NM\"] if \"SIG_KOR_NM\" in SGG.columns else pd.Series(dtype=str)\n",
    "\n",
    "os.makedirs(OUT_DIR_ORIGINAL, exist_ok=True)\n",
    "os.makedirs(OUT_DIR_BACKBONE, exist_ok=True)\n",
    "\n",
    "# ========= 메인 루프 =========\n",
    "# Expect: nw_dict is already defined by the user environment\n",
    "try:\n",
    "    nw_dict\n",
    "except NameError:\n",
    "    raise RuntimeError(\"`nw_dict` is not defined. Load your per-key edge DataFrames into `nw_dict` before running.\")\n",
    "\n",
    "for key, ed in nw_dict.items():\n",
    "    # normalize column names\n",
    "    ed = ed.copy()\n",
    "    ed.rename(columns={'1_기준연도': 'year', '8_시군구코드_seller': 'source', '14_시군구코드_buyer': 'target'}, inplace=True, errors='ignore')\n",
    "\n",
    "    for weight in WEIGHTS:\n",
    "        gpkg = os.path.join(COMM_BASE_DIR, f\"{weight}_{key}_SGG_map_zone_community.gpkg\")\n",
    "        if not os.path.exists(gpkg):\n",
    "            print(f\"[WARN] Missing community file: {gpkg}\")\n",
    "            continue\n",
    "\n",
    "        g = gpd.read_file(gpkg).to_crs(epsg=5179)\n",
    "        g[\"SIGUNGU_CD\"] = g[\"SIGUNGU_CD\"].astype(str)\n",
    "\n",
    "        # find available years from community columns\n",
    "        cand_prefixes = tuple(f\"{m}_canon_\" for m in COMMUNITY_METHODS)\n",
    "        years = sorted({int(c.split(\"_\")[-1]) for c in g.columns if c.startswith(cand_prefixes)})\n",
    "\n",
    "        if not all(c in ed.columns for c in ['source', 'target', 'year', weight]) or not years:\n",
    "            print(f\"[WARN] Columns or years missing for key={key}\")\n",
    "            continue\n",
    "\n",
    "        ed_use = ed.dropna(subset=[\"source\", \"target\", \"year\"]).copy()\n",
    "        ed_use[\"source\"] = ed_use[\"source\"].astype(str)\n",
    "        ed_use[\"target\"] = ed_use[\"target\"].astype(str)\n",
    "        ed_use[weight] = pd.to_numeric(ed_use[weight], errors=\"coerce\").fillna(0)\n",
    "\n",
    "        for method in COMMUNITY_METHODS:\n",
    "            for y in years:\n",
    "                col_canon = f\"{method}_canon_{y}\"\n",
    "                if col_canon not in g.columns:\n",
    "                    continue\n",
    "\n",
    "                s_year = g[[\"SIGUNGU_CD\", col_canon]].copy()\n",
    "                s_year[col_canon] = s_year[col_canon].astype(\"Int64\")\n",
    "                lab_map = s_year.dropna(subset=[col_canon]).set_index(\"SIGUNGU_CD\")[col_canon].astype(int).to_dict()\n",
    "\n",
    "                E = ed_use[ed_use[\"year\"] == y].copy()\n",
    "                if E.empty:\n",
    "                    continue\n",
    "\n",
    "                E[\"m_s\"] = E[\"source\"].map(lab_map)\n",
    "                E[\"m_t\"] = E[\"target\"].map(lab_map)\n",
    "                E_in = E[(E[\"m_s\"] == E[\"m_t\"]) & E[\"m_s\"].notna()].copy()\n",
    "                if E_in.empty:\n",
    "                    continue\n",
    "\n",
    "                E_in[\"m_s\"] = E_in[\"m_s\"].astype(int)\n",
    "                module_ids = sorted(E_in[\"m_s\"].unique().tolist())\n",
    "\n",
    "                for cid in module_ids:\n",
    "                    F_all = E_in[E_in[\"m_s\"] == cid].copy()\n",
    "                    if F_all.empty:\n",
    "                        continue\n",
    "\n",
    "                    members = s_year[s_year[col_canon] == cid][\"SIGUNGU_CD\"].astype(str).tolist()\n",
    "                    if len(members) < 2:\n",
    "                        continue\n",
    "\n",
    "                    # --- 1) Compute DII and RSI on internal flows (excluding self-loops) ---\n",
    "                    io_sum = {}\n",
    "                    for _, r in F_all[F_all['source'] != F_all['target']].iterrows():\n",
    "                        s, t, w_val = r['source'], r['target'], r[weight]\n",
    "                        io_sum[s] = io_sum.get(s, 0.0) + w_val\n",
    "                        io_sum[t] = io_sum.get(t, 0.0) + w_val\n",
    "                    mean_io = np.mean(list(io_sum.values())) if io_sum else 0\n",
    "                    dii_local = {sgg: (io_sum.get(sgg, 0) / mean_io) if mean_io > 0 else 1.0 for sgg in members}\n",
    "                    node_sizes = {s: DII_BASE + DII_RANGE * dii_local.get(s, 1.0) for s in members}\n",
    "\n",
    "                    total_flow = F_all[F_all['source'] != F_all['target']][weight].sum()\n",
    "                    F_all['__RSI__'] = (F_all[weight] / total_flow) if total_flow > 0 else 0\n",
    "\n",
    "                    rsi_vals_all = F_all['__RSI__'].values\n",
    "                    clip_hi = np.quantile(rsi_vals_all, RSI_CLIP_HI_QUANT) if rsi_vals_all.size > 0 else 0.01\n",
    "                    if clip_hi <= 0:\n",
    "                        clip_hi = 0.01\n",
    "\n",
    "                    rsi_scaled_01 = np.clip(F_all['__RSI__'] / clip_hi, 0, 1).values\n",
    "                    F_all['rsi_scaled_01'] = rsi_scaled_01\n",
    "\n",
    "                    vmax = F_all[weight].max()\n",
    "                    logv = np.log10((F_all[weight] / vmax) * 9.0 + 1.0) if vmax > 0 else 0\n",
    "                    F_all['w_norm'] = _norm01(logv if np.isscalar(logv) else logv.values)\n",
    "\n",
    "                    # --- 2) Backbone ---\n",
    "                    F_backbone = extract_disparity_backbone(\n",
    "                        F_all[F_all['source'] != F_all['target']], 'source', 'target', weight, alpha=ALPHA_LEVEL\n",
    "                    )\n",
    "\n",
    "                    # --- 3) Plot tasks ---\n",
    "                    plot_tasks = [\n",
    "                        {'name': 'Original', 'df': F_all, 'dir': OUT_DIR_ORIGINAL, 'prefix': ''},\n",
    "                        {'name': 'Backbone', 'df': F_backbone, 'dir': OUT_DIR_BACKBONE, 'prefix': f'[Backbone ({len(F_backbone)/len(F_all)*100:.1f}%)] '}\n",
    "                    ]\n",
    "\n",
    "                    for task in plot_tasks:\n",
    "                        task_name, df_plot, out_dir_base, title_prefix = task['name'], task['df'], task['dir'], task['prefix']\n",
    "                        outdir = os.path.join(out_dir_base, key)\n",
    "                        os.makedirs(outdir, exist_ok=True)\n",
    "                        out_png = os.path.join(outdir, f\"{method}_{y}_M{cid}_internal_{task_name.lower()}.png\")\n",
    "\n",
    "                        if os.path.exists(out_png):\n",
    "                            print(f\"[SKIP] {out_png}\")\n",
    "                            continue\n",
    "\n",
    "                        df_plot = df_plot.copy()\n",
    "\n",
    "                        # --- Style per task ---\n",
    "                        if task_name == 'Backbone':\n",
    "                            df_plot['alpha'] = MIN_ALPHA_bb + (MAX_ALPHA_bb - MIN_ALPHA_bb) * df_plot['rsi_scaled_01']\n",
    "                            df_plot['width'] = BASE_LW_bb + LW_RANGE_bb * ((1.0 - RSI_W_BLEND_bb) * df_plot['w_norm'] + RSI_W_BLEND_bb * df_plot['rsi_scaled_01'])\n",
    "                        else:  # Original\n",
    "                            df_plot['alpha'] = MIN_ALPHA + (MAX_ALPHA - MIN_ALPHA) * df_plot['rsi_scaled_01']\n",
    "                            df_plot['width'] = BASE_LW + LW_RANGE * ((1.0 - RSI_W_BLEND) * df_plot['w_norm'] + RSI_W_BLEND * df_plot['rsi_scaled_01'])\n",
    "\n",
    "                        fig, ax = plt.subplots(figsize=(10, 11), dpi=100)\n",
    "                        ax.set_position([0.15, 0.12, 0.7, 0.7])\n",
    "                        ax.set_aspect(\"equal\")\n",
    "                        ax.set_axis_off()\n",
    "\n",
    "                        # --- (NEW) 섬 인셋 처리 ---\n",
    "                        COMMUNITY_SGG_plot, SGG_pts_plot, inset_boxes = apply_island_insets(\n",
    "                            SGG=SGG,\n",
    "                            SGG_pts=SGG_pts,\n",
    "                            members=members,\n",
    "                            threshold_km=50.0,\n",
    "                            side_for_group={\"JEJU\":\"bottom\", \"ULLEUNG\":\"right\"}\n",
    "                        )\n",
    "\n",
    "                        COMMUNITY_SGG_plot = COMMUNITY_SGG_plot[COMMUNITY_SGG_plot[\"SIG_CD\"].isin(members)]\n",
    "                        COMMUNITY_SGG_plot.plot(ax=ax, color=\"#f0f0f0\", edgecolor=\"gray\", linewidth=0.5, zorder=1)\n",
    "\n",
    "                        # --- Axis by NODE extent (not polygon) ---\n",
    "                        member_pts_plot = SGG_pts_plot[SGG_pts_plot.index.isin(members)]\n",
    "                        minx_nodes = float(member_pts_plot.x.min()); maxx_nodes = float(member_pts_plot.x.max())\n",
    "                        miny_nodes = float(member_pts_plot.y.min()); maxy_nodes = float(member_pts_plot.y.max())\n",
    "                        dx = maxx_nodes - minx_nodes; dy = maxy_nodes - miny_nodes\n",
    "                        max_range = max(dx, dy)\n",
    "                        pad = max(0.1 * max_range, 20000.0)\n",
    "                        center_x = (minx_nodes + maxx_nodes) / 2.0\n",
    "                        center_y = (miny_nodes + maxy_nodes) / 2.0\n",
    "                        ax.set_xlim(center_x - max_range/2 - pad, center_x + max_range/2 + pad)\n",
    "                        ax.set_ylim(center_y - max_range/2 - pad, center_y + max_range/2 + pad*1.5)\n",
    "\n",
    "                        for rect in inset_boxes:\n",
    "                            ax.add_patch(rect)\n",
    "\n",
    "                        # --- Links (use moved centroids) ---\n",
    "                        all_segments, all_colors, all_widths = [], [], []\n",
    "                        for _, row in df_plot.iterrows():\n",
    "                            s, t = row['source'], row['target']\n",
    "                            if (s not in SGG_pts_plot.index) or (t not in SGG_pts_plot.index):\n",
    "                                continue\n",
    "                            p1, p2 = SGG_pts_plot.loc[s], SGG_pts_plot.loc[t]\n",
    "                            if isinstance(p1, pd.Series):\n",
    "                                p1 = p1.iloc[0]\n",
    "                            if isinstance(p2, pd.Series):\n",
    "                                p2 = p2.iloc[0]\n",
    "                            segs, cols = _curve_segments(p1, p2, curvature=(CURVATURE if s < t else -CURVATURE))\n",
    "                            if segs is None:\n",
    "                                continue\n",
    "                            cols[:, 3] = row['alpha']\n",
    "                            all_segments.extend(segs)\n",
    "                            all_colors.extend(cols)\n",
    "                            all_widths.extend([row['width']] * len(segs))\n",
    "\n",
    "                        if all_segments:\n",
    "                            lc = LineCollection(all_segments, colors=all_colors, linewidths=all_widths, capstyle=\"round\", zorder=3)\n",
    "                            ax.add_collection(lc)\n",
    "\n",
    "                        # --- Nodes (moved coordinates) ---\n",
    "                        node_color = COLOR_OF[cid]\n",
    "                        ax.scatter(member_pts_plot.x, member_pts_plot.y,\n",
    "                                   s=[node_sizes.get(idx, DII_BASE) for idx in member_pts_plot.index],\n",
    "                                   c=[node_color], alpha=0.5, edgecolors=\"black\", linewidth=0.9, zorder=4)\n",
    "\n",
    "                        # # Optional labels (disabled by default)\n",
    "                        # label_offset = (max_range) * 0.01\n",
    "                        # for sgg_code, pt in member_pts_plot.items():\n",
    "                        #     sgg_name = SGG_names.get(sgg_code, '') if not SGG_names.empty else ''\n",
    "                        #     clipped_dii = np.clip(dii_local.get(sgg_code, 1.0), 0, DII_CLIP_HI)\n",
    "                        #     normalized_dii = clipped_dii / DII_CLIP_HI if DII_CLIP_HI > 0 else 0.5\n",
    "                        #     fs = FONT_SIZE_MIN + normalized_dii * (FONT_SIZE_MAX - FONT_SIZE_MIN)\n",
    "                        #     txt = ax.text(pt.x + label_offset, pt.y + label_offset, sgg_name,\n",
    "                        #                   fontsize=fs, color='black', ha='left', va='bottom', zorder=5)\n",
    "                        #     txt.set_path_effects([path_effects.Stroke(linewidth=1.2, foreground='white', alpha=0.8),\n",
    "                        #                           path_effects.Normal()])\n",
    "\n",
    "                        add_north_arrow(ax, 0.9, 0.95, ARROW_FILE)\n",
    "                        add_scale_bar(ax)\n",
    "                        fig.suptitle(f\"{title_prefix}Internal Flows: Module M{cid}\\n[{key} / {method} / {y}]\", fontsize=14, y=0.85)\n",
    "\n",
    "                        # --- Legend ---\n",
    "                        handles = []\n",
    "                        handles.append(Line2D([0],[0], color=\"none\", label=\"Local DII\"))\n",
    "                        for lab in [0.5, 1.0, 2.0, DII_CLIP_HI]:\n",
    "                            size_demo = DII_BASE + DII_RANGE * np.clip(lab, 0, DII_CLIP_HI)\n",
    "                            handles.append(Line2D([], [], marker=\"o\", ls=\"None\", markersize=np.sqrt(size_demo),\n",
    "                                                  markerfacecolor=node_color, alpha=0.8,\n",
    "                                                  markeredgecolor=\"white\", mew=0.9, label=f\"  ×{lab:g}\"))\n",
    "\n",
    "                        handles.extend([Line2D([0],[0], c=\"none\", label=\" \"), Line2D([0],[0], c=\"none\", label=\"RSI (percentile)\")])\n",
    "\n",
    "                        if rsi_vals_all.size > 0:\n",
    "                            rsi_percentile_values = np.quantile(rsi_vals_all, RSI_PCT_LABELS)\n",
    "                        else:\n",
    "                            rsi_percentile_values = np.zeros(len(RSI_PCT_LABELS))\n",
    "                        rsi_demo_clipped = np.clip(rsi_percentile_values, 0, clip_hi)\n",
    "                        rsi_demo_scaled_01 = (rsi_demo_clipped / clip_hi) if clip_hi > 0 else np.zeros_like(rsi_demo_clipped)\n",
    "\n",
    "                        if task_name == 'Backbone':\n",
    "                            demo_alphas = MIN_ALPHA_bb + (MAX_ALPHA_bb - MIN_ALPHA_bb) * rsi_demo_scaled_01\n",
    "                        else:\n",
    "                            demo_alphas = MIN_ALPHA + (MAX_ALPHA - MIN_ALPHA) * rsi_demo_scaled_01\n",
    "\n",
    "                        for i, pctl in enumerate(RSI_PCT_LABELS):\n",
    "                            rv = rsi_percentile_values[i]\n",
    "                            alpha_val = demo_alphas[i]\n",
    "                            if task_name == 'Backbone':\n",
    "                                width_demo = BASE_LW_bb + LW_RANGE_bb * ((1.0 - RSI_W_BLEND_bb) * 0.5 + RSI_W_BLEND_bb * rsi_demo_scaled_01[i])\n",
    "                            else:\n",
    "                                width_demo = BASE_LW + LW_RANGE * ((1.0 - RSI_W_BLEND) * 0.5 + RSI_W_BLEND * rsi_demo_scaled_01[i])\n",
    "                            handles.append(Line2D([0],[0], color=(*mcolors.to_rgb(\"red\"), alpha_val), lw=width_demo, solid_capstyle=\"round\",\n",
    "                                                  label=f\"  {rv*100:.2f}% (P{int(pctl*100)})\"))\n",
    "\n",
    "                        handles.extend([Line2D([0],[0], c=\"none\", label=\" \"), Line2D([0],[0], c=\"none\", label=\"Direction\"),\n",
    "                                        Line2D([0],[0], c=\"red\", lw=3, label=\"  Source\"), Line2D([0],[0], c=\"yellow\", lw=3, label=\"  Target\")])\n",
    "\n",
    "                        leg = fig.legend(handles=handles, loc=\"center left\", bbox_to_anchor=(0.95, 0.5),\n",
    "                                         frameon=True, fontsize=8, title=\"Legend\", title_fontsize=12)\n",
    "                        leg.get_frame().set_alpha(0.96)\n",
    "\n",
    "                        plt.savefig(out_png, dpi=100, bbox_inches=\"tight\")\n",
    "                        #plt.show()\n",
    "                        plt.close(fig)\n",
    "                        gc.collect()\n",
    "                        print(f\"[OK] Saved {task_name} plot to {out_png}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcc396f",
   "metadata": {},
   "source": [
    "# 4. Inter hirarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5755a9ed",
   "metadata": {},
   "source": [
    "### 4.1. naturalbreak(dii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42e5abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Paths / Parameters =========\n",
    "OUT_DIR = \"outputs/figures/communities/intra_hierarchy/natural_break\"   # Output directory\n",
    "COMM_BASE_DIR = \"outputs/gpkg/communities\"                # …/{key}/{weight}_SGG_map_zone_community.gpkg\n",
    "SGG_MAP_PATH  = \"data/processed/map/SGG_map.gpkg\"                # Modify to match project path if needed\n",
    "COMMUNITY_METHODS = ['Infomap', \"Leiden\", \"Louvain\"]\n",
    "WEIGHTS = [\"거래관계\"]\n",
    "\n",
    "# ---- Classification / Display ----\n",
    "N_TIERS = 5                 # Max 3 tiers (auto-reduced if data is small)\n",
    "JITTER_STRENGTH = 0.25      # X-axis jitter strength\n",
    "FONT_SIZE_MIN = 6.0\n",
    "FONT_SIZE_MAX = 10.0\n",
    "\n",
    "# Point size by tier (scatter s is area in pt^2)\n",
    "POINT_SIZE_BASE = 80\n",
    "TIER_SIZE_MAP = {  # Tier 1 should be the largest\n",
    "    1: 500,        # ★\n",
    "    2: 120,        # ●\n",
    "    3: 80,         # ▲\n",
    "    4: 60,         # ▼\n",
    "    5: 40          # ■\n",
    "}\n",
    "POINT_FACE = \"black\"     # Same color for all points (tier is by marker shape only)\n",
    "POINT_EDGE = \"white\"\n",
    "POINT_EW   = 0.7\n",
    "\n",
    "# Distinguish tiers by marker shape\n",
    "MARKER_MAP = {\n",
    "    1: '*',  # Tier 1\n",
    "    2: 'o',  # Tier 2\n",
    "    3: '^',  # Tier 3\n",
    "    4: 'v',  # Tier 4\n",
    "    5: 's'   # Tier 5\n",
    "}\n",
    "\n",
    "# Axis cap (kept consistent with set_ylim top)\n",
    "YMAX = 4.5\n",
    "\n",
    "# Stable jitter seed (kept explicit; deterministic across runs)\n",
    "GLOBAL_SEED = 20250922\n",
    "\n",
    "\n",
    "def safe_name(s: str) -> str:\n",
    "    \"\"\"Sanitize strings to be safe in filenames.\"\"\"\n",
    "    return re.sub(r\"[^0-9A-Za-z가-힣_.\\-]+\", \"_\", str(s))\n",
    "\n",
    "\n",
    "# ===== Fallback SGG names (from map) =====\n",
    "SGG_names_fallback = {}\n",
    "if os.path.exists(SGG_MAP_PATH):\n",
    "    try:\n",
    "        SGG = gpd.read_file(SGG_MAP_PATH)\n",
    "        if \"SIG_KOR_NM\" not in SGG.columns and \"SIG_ENG_NM\" in SGG.columns:\n",
    "            SGG[\"SIG_KOR_NM\"] = SGG[\"SIG_ENG_NM\"]\n",
    "        SGG[\"SIG_CD\"] = SGG[\"SIG_CD\"].astype(str)\n",
    "        SGG_names_fallback = SGG.set_index(\"SIG_CD\")[\"SIG_KOR_NM\"].to_dict()\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] SGG map load failed: {e}\")\n",
    "\n",
    "\n",
    "# === Stable jitter (unchanged behavior, only English comments) ===\n",
    "def stable_jitter(key, method, cid, sgg_code, strength=JITTER_STRENGTH, seed=GLOBAL_SEED):\n",
    "    \"\"\"\n",
    "    Always returns the same jitter for the same (key, method, cid, sgg_code),\n",
    "    independent of year. Changing seed will change the whole pattern.\n",
    "    \"\"\"\n",
    "    s = f\"{seed}|{key}|{method}|{int(cid)}|{sgg_code}\"\n",
    "    h = int.from_bytes(hashlib.blake2b(s.encode(\"utf-8\"), digest_size=8).digest(), \"big\")\n",
    "    u = (h % (1 << 53)) / float(1 << 53)  # uniform [0,1)\n",
    "    return 1.0 + (u * 2.0 - 1.0) * strength  # 1.0 ± strength\n",
    "\n",
    "\n",
    "# ===== NEW: overflow annotation helper (minimal addition) =====\n",
    "def annotate_overflow(ax, df_over, get_name, marker_map, size_map,\n",
    "                      point_face, point_edge, point_ew,\n",
    "                      ymax=YMAX, start_offset=0.12):\n",
    "    \"\"\"\n",
    "    For points with y > ymax, draw a top-cap marker at y=ymax and an upward arrow\n",
    "    to a box label showing [name] [true value]. Labels are staggered to reduce overlap.\n",
    "    \"\"\"\n",
    "    df_over = df_over.sort_values('dii', ascending=False).reset_index(drop=True)\n",
    "    for i, row in df_over.iterrows():\n",
    "        x = float(row['x']); dii = float(row['dii']); code = str(row['sgg_code'])\n",
    "        tier = int(row['tier']); mk = marker_map.get(tier, 'o'); ms = size_map.get(tier, POINT_SIZE_BASE)\n",
    "        # Top cap marker at axis limit\n",
    "        ax.scatter([x], [ymax], s=ms, marker=mk, c=point_face, edgecolors=point_edge,\n",
    "                   linewidth=point_ew, alpha=0.95, zorder=12, clip_on=False)\n",
    "        # Upward arrow + label with true value\n",
    "        y_text = ymax - start_offset\n",
    "        label = f\"{get_name(code)} {dii:.2f}\"\n",
    "        txt = ax.annotate(\n",
    "            label, xy=(x, ymax), xytext=(x, y_text),\n",
    "            ha='center', va='bottom', fontsize=max(9, row.get('font_size', 9)), weight='bold',\n",
    "            bbox=dict(boxstyle='round,pad=0.25', fc='white', ec='gray', alpha=0.95),\n",
    "            arrowprops=dict(arrowstyle='-|>', lw=1.0, color='black', shrinkA=2, shrinkB=2, mutation_scale=12),\n",
    "            zorder=13, clip_on=False\n",
    "        )\n",
    "        try:\n",
    "            txt.set_path_effects([\n",
    "                path_effects.Stroke(linewidth=2.0, foreground='white', alpha=0.7),\n",
    "                path_effects.Normal()\n",
    "            ])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ===== Main Loop =====\n",
    "# Assumption: nw_dict is already loaded\n",
    "for key, ed in nw_dict.items():\n",
    "    for weight in WEIGHTS:\n",
    "        gpkg_path = os.path.join(COMM_BASE_DIR, f\"{weight}_{key}_SGG_map_zone_community.gpkg\")\n",
    "        if not os.path.exists(gpkg_path):\n",
    "            print(f\"[SKIP] {gpkg_path} not found\")\n",
    "            continue\n",
    "\n",
    "        g_comm = gpd.read_file(gpkg_path)\n",
    "        if \"SIGUNGU_CD\" not in g_comm.columns:\n",
    "            print(f\"[SKIP] {key}/{weight}: 'SIGUNGU_CD' not found\")\n",
    "            continue\n",
    "        g_comm[\"SIGUNGU_CD\"] = g_comm[\"SIGUNGU_CD\"].astype(str)\n",
    "\n",
    "        # Prefer names from community GPKG; fallback to SGG map\n",
    "        name_col_candidates = [c for c in [\"SIGUNGU_NM\", \"SIG_KOR_NM\", \"SIGUNGU_NAME\", \"SIG_NM\"] if c in g_comm.columns]\n",
    "        name_map = dict(zip(g_comm[\"SIGUNGU_CD\"].astype(str), g_comm[name_col_candidates[0]].astype(str))) if name_col_candidates else SGG_names_fallback\n",
    "\n",
    "        def get_name(code: str) -> str:\n",
    "            code = str(code)\n",
    "            return name_map.get(code, SGG_names_fallback.get(code, code))\n",
    "\n",
    "        for method in COMMUNITY_METHODS:\n",
    "            print(f\"--- Processing [{key} / {weight} / {method}] ---\")\n",
    "\n",
    "            # Collect canonical years from columns: {method}_canon_{year}\n",
    "            years = sorted({int(c.split(\"_\")[-1]) for c in g_comm.columns if c.startswith(f\"{method}_canon_\") and c.split(\"_\")[-1].isdigit()})\n",
    "            if not years:\n",
    "                print(\"  [SKIP] No canonical years found.\")\n",
    "                continue\n",
    "\n",
    "            # Standardize original edge table (minimal changes to your mapping)\n",
    "            rename_map = {\n",
    "                '1_기준연도': 'year',\n",
    "                '8_시군구코드_seller': 'source',\n",
    "                '14_시군구코드_buyer': 'target',\n",
    "                weight: 'weight'\n",
    "            }\n",
    "            ed.rename(columns={k: v for k, v in rename_map.items() if k in ed.columns}, inplace=True)\n",
    "            required_cols = ['year', 'source', 'target', 'weight']\n",
    "            if not all(c in ed.columns for c in required_cols):\n",
    "                print(f\"  [SKIP] Missing required columns in edge data.\")\n",
    "                continue\n",
    "\n",
    "            ed_use = ed[required_cols].dropna().copy()\n",
    "            ed_use[['source', 'target']] = ed_use[['source', 'target']].astype(str)\n",
    "            ed_use['year'] = pd.to_numeric(ed_use['year'], errors='coerce')\n",
    "            ed_use['weight'] = pd.to_numeric(ed_use['weight'], errors='coerce').fillna(0)\n",
    "\n",
    "            # ===== Process year by year =====\n",
    "            for year in years:\n",
    "                col_canon = f\"{method}_canon_{year}\"\n",
    "                if col_canon not in g_comm.columns:\n",
    "                    continue\n",
    "\n",
    "                # Community (module) label map for this year\n",
    "                s_year = g_comm[[\"SIGUNGU_CD\", col_canon]].dropna()\n",
    "                s_year[col_canon] = s_year[col_canon].astype(int)\n",
    "                lab_map = s_year.set_index(\"SIGUNGU_CD\")[col_canon].to_dict()\n",
    "\n",
    "                # Edge data of that year\n",
    "                ede = ed_use[ed_use[\"year\"] == year].copy()\n",
    "                if ede.empty:\n",
    "                    continue\n",
    "\n",
    "                # Map modules\n",
    "                ede[\"m_s\"] = ede[\"source\"].map(lab_map)\n",
    "                ede[\"m_t\"] = ede[\"target\"].map(lab_map)\n",
    "                ede = ede.dropna(subset=[\"m_s\", \"m_t\"])\n",
    "                ede[[\"m_s\", \"m_t\"]] = ede[[\"m_s\", \"m_t\"]].astype(int)\n",
    "\n",
    "                # Keep only intra-community links\n",
    "                ede_in = ede[ede[\"m_s\"] == ede[\"m_t\"]].copy()\n",
    "                if ede_in.empty:\n",
    "                    continue\n",
    "\n",
    "                # Compute SGG-level DII inside each module & visualize\n",
    "                for cid, F_all in ede_in.groupby('m_s'):\n",
    "                    # Members of this module\n",
    "                    members = s_year[s_year[col_canon] == cid][\"SIGUNGU_CD\"].astype(str).tolist()\n",
    "                    if len(members) < 2:\n",
    "                        continue\n",
    "\n",
    "                    # 1) Sum of IO by SGG (exclude self-loops)\n",
    "                    io_sum = {}\n",
    "                    F_use = F_all[F_all['source'] != F_all['target']]\n",
    "                    for _, r in F_use.iterrows():\n",
    "                        s, t, wv = r['source'], r['target'], r['weight']\n",
    "                        io_sum[s] = io_sum.get(s, 0.0) + wv\n",
    "                        io_sum[t] = io_sum.get(t, 0.0) + wv\n",
    "\n",
    "                    if not io_sum:\n",
    "                        continue\n",
    "                    mean_io = np.mean(list(io_sum.values()))\n",
    "                    if mean_io <= 0:\n",
    "                        continue\n",
    "\n",
    "                    dii_records = [{'sgg_code': sgg, 'dii': io_sum.get(sgg, 0.0) / mean_io} for sgg in members]\n",
    "                    sgg_dii_df = pd.DataFrame(dii_records)\n",
    "                    if sgg_dii_df.empty:\n",
    "                        continue\n",
    "\n",
    "                    # 2) Tier classification (Natural Breaks -> up to 3 tiers)\n",
    "                    uniq = sgg_dii_df['dii'].nunique()\n",
    "                    k = min(N_TIERS, uniq)\n",
    "                    if k > 1:\n",
    "                        classifier = mapclassify.NaturalBreaks(sgg_dii_df['dii'], k=k)\n",
    "                        # yb: 0(low) ~ k-1(high) -> Tier 1(high) ~ Tier k(low), reversed\n",
    "                        sgg_dii_df['tier'] = (k - classifier.yb).astype(int)\n",
    "                        bins = classifier.bins.copy()\n",
    "                    else:\n",
    "                        classifier = None\n",
    "                        sgg_dii_df['tier'] = 1\n",
    "                        bins = None\n",
    "\n",
    "                    # 3) Font size scale (based on DII)\n",
    "                    dii_min, dii_max = sgg_dii_df['dii'].min(), sgg_dii_df['dii'].max()\n",
    "                    span = dii_max - dii_min\n",
    "                    sgg_dii_df['norm'] = (sgg_dii_df['dii'] - dii_min) / span if span > 0 else 0.5\n",
    "                    sgg_dii_df['font_size'] = FONT_SIZE_MIN + sgg_dii_df['norm'] * (FONT_SIZE_MAX - FONT_SIZE_MIN)\n",
    "\n",
    "                    # 4) X-axis jitter (year-invariant, fixed by community/method/SGG)\n",
    "                    sgg_dii_df['x'] = sgg_dii_df['sgg_code'].apply(\n",
    "                        lambda code: stable_jitter(key, method, int(cid), str(code), strength=JITTER_STRENGTH)\n",
    "                    )\n",
    "\n",
    "                    # 5) Plot\n",
    "                    outdir = os.path.join(OUT_DIR, key)\n",
    "                    os.makedirs(outdir, exist_ok=True)\n",
    "                    outpath = os.path.join(outdir, f\"{method}_dii_sgg_dist_{year}__M{safe_name(cid)}.png\")\n",
    "\n",
    "                    fig, ax = plt.subplots(figsize=(14, 8), dpi=100)\n",
    "\n",
    "                    # === Scatter: split into in-range and overflow (minimal change) ===\n",
    "                    present_tiers = sorted(sgg_dii_df['tier'].unique(), reverse=True)\n",
    "                    for tier in present_tiers:\n",
    "                        mk = MARKER_MAP.get(tier, 'o')\n",
    "                        sub = sgg_dii_df[sgg_dii_df['tier'] == tier]\n",
    "                        in_mask = sub['dii'] <= YMAX\n",
    "                        sub_in  = sub[in_mask]\n",
    "                        ax.scatter(\n",
    "                            x=sub_in['x'], y=sub_in['dii'],\n",
    "                            s=TIER_SIZE_MAP.get(tier, POINT_SIZE_BASE),\n",
    "                            marker=mk,\n",
    "                            c=POINT_FACE, edgecolors=POINT_EDGE, linewidth=POINT_EW,\n",
    "                            alpha=0.95, zorder=10\n",
    "                        )\n",
    "\n",
    "                    # Labels: only for in-range points (overflow gets arrow labels)\n",
    "                    for _, row in sgg_dii_df[sgg_dii_df['dii'] <= YMAX].iterrows():\n",
    "                        name = get_name(row['sgg_code'])\n",
    "                        txt = ax.text(\n",
    "                            row['x']+0.006, row['dii']+0.002, f\" {name}\",\n",
    "                            fontsize=row['font_size'], color='black',\n",
    "                            ha='left', va='center', weight='bold', zorder=11\n",
    "                        )\n",
    "                        txt.set_path_effects([\n",
    "                            path_effects.Stroke(linewidth=2.0, foreground='white', alpha=0.85),\n",
    "                            path_effects.Normal()\n",
    "                        ])\n",
    "\n",
    "                    # Overflow: top-cap + arrow + name + true value\n",
    "                    overflow_all = sgg_dii_df[sgg_dii_df['dii'] > YMAX]\n",
    "                    if not overflow_all.empty:\n",
    "                        annotate_overflow(\n",
    "                            ax, overflow_all, get_name, MARKER_MAP, TIER_SIZE_MAP,\n",
    "                            POINT_FACE, POINT_EDGE, POINT_EW, ymax= 4.5,\n",
    "                            start_offset=0.3\n",
    "                        )\n",
    "\n",
    "                    # Reference line & axis formatting\n",
    "                    ax.axhline(1.0, color='red', linestyle='--', linewidth=1.2, label='Module Avg (DII=1.0)')\n",
    "                    ax.set_title(\n",
    "                        f\"Intra-Community DII Distribution (Module M{cid}, n={len(members)})\\n[{key} / {method} / {year}]\",\n",
    "                        fontsize=14, pad=14\n",
    "                    )\n",
    "                    ax.set_ylabel(\"Local Dominance Index (DII)\", fontsize=11)\n",
    "                    ax.set_xlabel(f\"SGGs in Module M{cid}\", fontsize=11)\n",
    "                    ax.set_xticks([])\n",
    "                    ax.grid(axis='y', linestyle=':', linewidth=0.6, color='gray', alpha=0.7)\n",
    "\n",
    "                    # Legend: tiers + reference line (no cap legend as requested)\n",
    "                    leg_handles, leg_labels = [], []\n",
    "                    h_line = Line2D([0], [0], color='red', linestyle='--', linewidth=1.2, label='Module Avg (DII=1.0)')\n",
    "                    leg_handles.append(h_line); leg_labels.append(h_line.get_label())\n",
    "\n",
    "                    if classifier and k > 1:\n",
    "                        # bins: upper bounds (ascending). Reverse so Tier 1 is top bin\n",
    "                        bounds = []\n",
    "                        lo = sgg_dii_df['dii'].min()\n",
    "                        for i in range(k):\n",
    "                            hi = bins[i]\n",
    "                            bounds.append((lo, hi))\n",
    "                            lo = hi\n",
    "                        bounds = list(reversed(bounds))\n",
    "                        for t_idx, t in enumerate(range(1, k+1), start=0):\n",
    "                            mk = MARKER_MAP.get(t, 'o')\n",
    "                            lo_b, hi_b = bounds[t_idx]\n",
    "                            tier_text = f\"Tier {t}  ({lo_b:.2f} ~ {hi_b:.2f})\"\n",
    "                            ms = max(8, (TIER_SIZE_MAP.get(t, POINT_SIZE_BASE)) ** 0.5)\n",
    "                            leg_handles.append(Line2D([], [], marker=mk, linestyle='None',\n",
    "                                                      markersize=ms,\n",
    "                                                      markerfacecolor=POINT_FACE,\n",
    "                                                      markeredgecolor=POINT_EDGE, markeredgewidth=POINT_EW,\n",
    "                                                      label=tier_text))\n",
    "                            leg_labels.append(tier_text)\n",
    "                    else:\n",
    "                        mk = MARKER_MAP.get(1, 'o')\n",
    "                        tier_text = \"Tier 1 (single-bin)\"\n",
    "                        ms = max(8, (TIER_SIZE_MAP.get(1, POINT_SIZE_BASE)) ** 0.5)\n",
    "                        leg_handles.append(Line2D([], [], marker=mk, linestyle='None',\n",
    "                                                  markersize=ms,\n",
    "                                                  markerfacecolor=POINT_FACE,\n",
    "                                                  markeredgecolor=POINT_EDGE, markeredgewidth=POINT_EW,\n",
    "                                                  label=tier_text))\n",
    "                        leg_labels.append(tier_text)\n",
    "\n",
    "                    ax.legend(leg_handles, leg_labels, loc='center left', bbox_to_anchor=(1.02, 0.5),\n",
    "                              title=\"Legend (tiers by Natural Breaks)\", fontsize=10, title_fontsize=11, frameon=True)\n",
    "                    ax.patch.set_facecolor(\"whitesmoke\")\n",
    "                    ax.set_ylim(bottom=0, top=YMAX)\n",
    "                    plt.tight_layout(rect=[0, 0, 0.86, 1])\n",
    "\n",
    "                    # Save\n",
    "                    plt.savefig(outpath, dpi=110, bbox_inches=\"tight\")\n",
    "                    #plt.show()\n",
    "                    plt.close(fig)\n",
    "                    gc.collect()\n",
    "                    print(f\"  \\u2714 Saved: {outpath}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66378884",
   "metadata": {},
   "source": [
    "### 4.2. K-means(dii,pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee05396b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Paths / Parameters =========\n",
    "OUT_DIR = \"outputs/figures/communities/intra_hierarchy/k_means\"   # Output directory\n",
    "COMM_BASE_DIR = \"outputs/gpkg/communities\"                # …/{key}/{weight}_SGG_map_zone_community.gpkg\n",
    "SGG_MAP_PATH  = \"data/processed/map/SGG_map.gpkg\"                # Modify to match project path if needed\n",
    "COMMUNITY_METHODS = ['Infomap', \"Leiden\", \"Louvain\"]\n",
    "WEIGHTS = [\"거래관계\"]\n",
    "\n",
    "# ---- Classification / Display ----\n",
    "N_TIERS = 5                 # Max 3 tiers (auto-reduced if data is small)\n",
    "JITTER_STRENGTH = 0.25      # X-axis jitter strength\n",
    "FONT_SIZE_MIN = 6.0\n",
    "FONT_SIZE_MAX = 10.0\n",
    "\n",
    "# Point size by tier (scatter s is area in pt^2)\n",
    "POINT_SIZE_BASE = 80\n",
    "TIER_SIZE_MAP = {  # Tier 1 should be the largest\n",
    "    1: 500,        # ★\n",
    "    2: 120,        # ●\n",
    "    3: 80,         # ▲\n",
    "    4: 60,         # ▼\n",
    "    5: 40          # ■\n",
    "}\n",
    "POINT_FACE = \"black\"     # Same color for all points (tier is by marker shape only)\n",
    "POINT_EDGE = \"white\"\n",
    "POINT_EW   = 0.7\n",
    "\n",
    "# Distinguish tiers by marker shape\n",
    "MARKER_MAP = {\n",
    "    1: '*',  # Tier 1\n",
    "    2: 'o',  # Tier 2\n",
    "    3: '^',  # Tier 3\n",
    "    4: 'v',  # Tier 4\n",
    "    5: 's'   # Tier 5\n",
    "}\n",
    "\n",
    "# Axis cap (kept consistent with set_ylim top)\n",
    "YMAX = 4.5\n",
    "\n",
    "# Stable jitter seed (kept explicit; deterministic across runs)\n",
    "GLOBAL_SEED = 20250922\n",
    "\n",
    "\n",
    "def safe_name(s: str) -> str:\n",
    "    \"\"\"Sanitize strings to be safe in filenames.\"\"\"\n",
    "    return re.sub(r\"[^0-9A-Za-z가-힣_.\\-]+\", \"_\", str(s))\n",
    "\n",
    "\n",
    "# ===== Fallback SGG names (from map) =====\n",
    "SGG_names_fallback = {}\n",
    "if os.path.exists(SGG_MAP_PATH):\n",
    "    try:\n",
    "        SGG = gpd.read_file(SGG_MAP_PATH)\n",
    "        if \"SIG_KOR_NM\" not in SGG.columns and \"SIG_ENG_NM\" in SGG.columns:\n",
    "            SGG[\"SIG_KOR_NM\"] = SGG[\"SIG_ENG_NM\"]\n",
    "        SGG[\"SIG_CD\"] = SGG[\"SIG_CD\"].astype(str)\n",
    "        SGG_names_fallback = SGG.set_index(\"SIG_CD\")[\"SIG_KOR_NM\"].to_dict()\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] SGG map load failed: {e}\")\n",
    "\n",
    "\n",
    "# === Stable jitter (unchanged behavior, only English comments) ===\n",
    "def stable_jitter(key, method, cid, sgg_code, strength=JITTER_STRENGTH, seed=GLOBAL_SEED):\n",
    "    \"\"\"\n",
    "    Always returns the same jitter for the same (key, method, cid, sgg_code),\n",
    "    independent of year. Changing seed will change the whole pattern.\n",
    "    \"\"\"\n",
    "    s = f\"{seed}|{key}|{method}|{int(cid)}|{sgg_code}\"\n",
    "    h = int.from_bytes(hashlib.blake2b(s.encode(\"utf-8\"), digest_size=8).digest(), \"big\")\n",
    "    u = (h % (1 << 53)) / float(1 << 53)  # uniform [0,1)\n",
    "    return 1.0 + (u * 2.0 - 1.0) * strength  # 1.0 ± strength\n",
    "\n",
    "\n",
    "# ===== NEW: overflow annotation helper (minimal addition) =====\n",
    "def annotate_overflow(ax, df_over, get_name, marker_map, size_map,\n",
    "                      point_face, point_edge, point_ew,\n",
    "                      ymax=YMAX, start_offset=0.12):\n",
    "    \"\"\"\n",
    "    For points with y > ymax, draw a top-cap marker at y=ymax and an upward arrow\n",
    "    to a box label showing [name] [true value]. Labels are staggered to reduce overlap.\n",
    "    \"\"\"\n",
    "    df_over = df_over.sort_values('dii', ascending=False).reset_index(drop=True)\n",
    "    for i, row in df_over.iterrows():\n",
    "        x = float(row['x']); dii = float(row['dii']); code = str(row['sgg_code'])\n",
    "        tier = int(row['tier']); mk = marker_map.get(tier, 'o'); ms = size_map.get(tier, POINT_SIZE_BASE)\n",
    "        # Top cap marker at axis limit\n",
    "        ax.scatter([x], [ymax], s=ms, marker=mk, c=point_face, edgecolors=point_edge,\n",
    "                   linewidth=point_ew, alpha=0.95, zorder=12, clip_on=False)\n",
    "        # Upward arrow + label with true value\n",
    "        y_text = ymax - start_offset\n",
    "        label = f\"{get_name(code)} {dii:.2f}\"\n",
    "        txt = ax.annotate(\n",
    "            label, xy=(x, ymax), xytext=(x, y_text),\n",
    "            ha='center', va='bottom', fontsize=max(9, row.get('font_size', 9)), weight='bold',\n",
    "            bbox=dict(boxstyle='round,pad=0.25', fc='white', ec='gray', alpha=0.95),\n",
    "            arrowprops=dict(arrowstyle='-|>', lw=1.0, color='black', shrinkA=2, shrinkB=2, mutation_scale=12),\n",
    "            zorder=13, clip_on=False\n",
    "        )\n",
    "        try:\n",
    "            txt.set_path_effects([\n",
    "                path_effects.Stroke(linewidth=2.0, foreground='white', alpha=0.7),\n",
    "                path_effects.Normal()\n",
    "            ])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ===== Main Loop =====\n",
    "# Assumption: nw_dict is already loaded\n",
    "for key, ed in nw_dict.items():\n",
    "    for weight in WEIGHTS:\n",
    "        gpkg_path = os.path.join(COMM_BASE_DIR, f\"{weight}_{key}_SGG_map_zone_community.gpkg\")\n",
    "        if not os.path.exists(gpkg_path):\n",
    "            print(f\"[SKIP] {gpkg_path} not found\")\n",
    "            continue\n",
    "\n",
    "        g_comm = gpd.read_file(gpkg_path)\n",
    "        if \"SIGUNGU_CD\" not in g_comm.columns:\n",
    "            print(f\"[SKIP] {key}/{weight}: 'SIGUNGU_CD' not found\")\n",
    "            continue\n",
    "        g_comm[\"SIGUNGU_CD\"] = g_comm[\"SIGUNGU_CD\"].astype(str)\n",
    "\n",
    "        # Prefer names from community GPKG; fallback to SGG map\n",
    "        name_col_candidates = [c for c in [\"SIGUNGU_NM\", \"SIG_KOR_NM\", \"SIGUNGU_NAME\", \"SIG_NM\"] if c in g_comm.columns]\n",
    "        name_map = dict(zip(g_comm[\"SIGUNGU_CD\"].astype(str), g_comm[name_col_candidates[0]].astype(str))) if name_col_candidates else SGG_names_fallback\n",
    "\n",
    "        def get_name(code: str) -> str:\n",
    "            code = str(code)\n",
    "            return name_map.get(code, SGG_names_fallback.get(code, code))\n",
    "\n",
    "        for method in COMMUNITY_METHODS:\n",
    "            print(f\"--- Processing [{key} / {weight} / {method}] ---\")\n",
    "\n",
    "            # Collect canonical years from columns: {method}_canon_{year}\n",
    "            years = sorted({int(c.split(\"_\")[-1]) for c in g_comm.columns if c.startswith(f\"{method}_canon_\") and c.split(\"_\")[-1].isdigit()})\n",
    "            if not years:\n",
    "                print(\"  [SKIP] No canonical years found.\")\n",
    "                continue\n",
    "\n",
    "            # Standardize original edge table (minimal changes to your mapping)\n",
    "            rename_map = {\n",
    "                '1_기준연도': 'year',\n",
    "                '8_시군구코드_seller': 'source',\n",
    "                '14_시군구코드_buyer': 'target',\n",
    "                weight: 'weight'\n",
    "            }\n",
    "            ed.rename(columns={k: v for k, v in rename_map.items() if k in ed.columns}, inplace=True)\n",
    "            required_cols = ['year', 'source', 'target', 'weight']\n",
    "            if not all(c in ed.columns for c in required_cols):\n",
    "                print(f\"  [SKIP] Missing required columns in edge data.\")\n",
    "                continue\n",
    "\n",
    "            ed_use = ed[required_cols].dropna().copy()\n",
    "            ed_use[['source', 'target']] = ed_use[['source', 'target']].astype(str)\n",
    "            ed_use['year'] = pd.to_numeric(ed_use['year'], errors='coerce')\n",
    "            ed_use['weight'] = pd.to_numeric(ed_use['weight'], errors='coerce').fillna(0)\n",
    "\n",
    "            # ===== Process year by year =====\n",
    "            for year in years:\n",
    "                col_canon = f\"{method}_canon_{year}\"\n",
    "                if col_canon not in g_comm.columns:\n",
    "                    continue\n",
    "\n",
    "                # Community (module) label map for this year\n",
    "                s_year = g_comm[[\"SIGUNGU_CD\", col_canon]].dropna()\n",
    "                s_year[col_canon] = s_year[col_canon].astype(int)\n",
    "                lab_map = s_year.set_index(\"SIGUNGU_CD\")[col_canon].to_dict()\n",
    "\n",
    "                # Edge data of that year\n",
    "                ede = ed_use[ed_use[\"year\"] == year].copy()\n",
    "                if ede.empty:\n",
    "                    continue\n",
    "\n",
    "                # Map modules\n",
    "                ede[\"m_s\"] = ede[\"source\"].map(lab_map)\n",
    "                ede[\"m_t\"] = ede[\"target\"].map(lab_map)\n",
    "                ede = ede.dropna(subset=[\"m_s\", \"m_t\"])\n",
    "                ede[[\"m_s\", \"m_t\"]] = ede[[\"m_s\", \"m_t\"]].astype(int)\n",
    "\n",
    "                # Keep only intra-community links\n",
    "                ede_in = ede[ede[\"m_s\"] == ede[\"m_t\"]].copy()\n",
    "                if ede_in.empty:\n",
    "                    continue\n",
    "\n",
    "                # Compute SGG-level DII inside each module & visualize\n",
    "                for cid, F_all in ede_in.groupby('m_s'):\n",
    "                    # Members of this module\n",
    "                    members = s_year[s_year[col_canon] == cid][\"SIGUNGU_CD\"].astype(str).tolist()\n",
    "                    if len(members) < 2:\n",
    "                        continue\n",
    "\n",
    "                    # 1) Sum of IO by SGG (exclude self-loops)\n",
    "                    io_sum = {}\n",
    "                    F_use = F_all[F_all['source'] != F_all['target']]\n",
    "                    for _, r in F_use.iterrows():\n",
    "                        s, t, wv = r['source'], r['target'], r['weight']\n",
    "                        io_sum[s] = io_sum.get(s, 0.0) + wv\n",
    "                        io_sum[t] = io_sum.get(t, 0.0) + wv\n",
    "\n",
    "                    if not io_sum:\n",
    "                        continue\n",
    "                    mean_io = np.mean(list(io_sum.values()))\n",
    "                    if mean_io <= 0:\n",
    "                        continue\n",
    "\n",
    "                    dii_records = [{'sgg_code': sgg, 'dii': io_sum.get(sgg, 0.0) / mean_io} for sgg in members]\n",
    "                    sgg_dii_df = pd.DataFrame(dii_records)\n",
    "                    if sgg_dii_df.empty:\n",
    "                        continue\n",
    "\n",
    "                    # 2) Font size scale (based on DII)\n",
    "                    dii_min, dii_max = sgg_dii_df['dii'].min(), sgg_dii_df['dii'].max()\n",
    "                    span = dii_max - dii_min\n",
    "                    sgg_dii_df['norm'] = (sgg_dii_df['dii'] - dii_min) / span if span > 0 else 0.5\n",
    "                    sgg_dii_df['font_size'] = FONT_SIZE_MIN + sgg_dii_df['norm'] * (FONT_SIZE_MAX - FONT_SIZE_MIN)\n",
    "                    \n",
    "                    SGG_temp = SGG[['SIG_CD'] + [col for col in SGG.columns if col.startswith(str(year))]]\n",
    "                    sgg_dii_df = sgg_dii_df.merge(SGG_temp, left_on='sgg_code', right_on='SIG_CD', how='left')\n",
    "                    \n",
    "                    # 3) Tier classification (Natural Breaks -> up to 3 tiers)\n",
    "                    col_pop = f\"{year}_pop\"\n",
    "                    uniq = min(sgg_dii_df['dii'].nunique(), sgg_dii_df[col_pop].nunique())\n",
    "                    k = int(max(1, min(N_TIERS, uniq, len(sgg_dii_df))))\n",
    "                    \n",
    "                    \n",
    "    \n",
    "                    if k > 1:\n",
    "                        # (1) 스케일링: dii, pop 동시 표준화 (스케일 차이로 pop이 지배하는 것 방지)\n",
    "                        X_raw = sgg_dii_df[['dii', col_pop]].values.astype(float)\n",
    "                        X = StandardScaler().fit_transform(X_raw)\n",
    "\n",
    "                        # (2) KMeans\n",
    "                        kmeans = KMeans(n_clusters=k, random_state=0, n_init=10)\n",
    "                        sgg_dii_df['_label'] = kmeans.fit_predict(X)\n",
    "\n",
    "                        # (3) 클러스터 중심의 'dii' 기준으로 티어 우선순위(내림차순) 정하기\n",
    "                        #     - 중심은 원스케일로 보기 위해 역변환 없이, label별 dii 평균으로 순위화\n",
    "                        mean_dii_by_c = sgg_dii_df.groupby('_label')['dii'].mean()\n",
    "                        order = {c: rank+1 for rank, c in enumerate(mean_dii_by_c.sort_values(ascending=False).index)}\n",
    "\n",
    "                        # (4) tier 할당 (1이 최고)\n",
    "                        sgg_dii_df['tier'] = sgg_dii_df['_label'].map(order).astype(int)\n",
    "                        sgg_dii_df.loc[sgg_dii_df.index, 'tier'] = sgg_dii_df['tier'].values\n",
    "\n",
    "                        # (5) NaturalBreaks의 bins 대체:\n",
    "                        #     - 티어별 dii 최대값을 오름차순으로 정렬하여 bins로 제공\n",
    "                        #       (기존 코드가 bins를 이용해 [min→bin1], [bin1→bin2] …로 경계를 만들고,\n",
    "                        #        마지막에 reverse 하여 Tier 1이 최상위 구간이 되도록 했던 로직과 합치)\n",
    "                        max_dii_by_tier = (\n",
    "                            sgg_dii_df.groupby('tier')['dii'].max()  # 각 티어의 상한\n",
    "                            .sort_values(ascending=True)      # 오름차순(하위→상위)\n",
    "                            .to_numpy()\n",
    "                            .copy()\n",
    "                        )\n",
    "                        bins = max_dii_by_tier\n",
    "                        classifier = kmeans\n",
    "                    else:\n",
    "                        classifier = None\n",
    "                        sgg_dii_df['tier'] = 1\n",
    "                        bins = None\n",
    "\n",
    "                    # 4) X-axis jitter (year-invariant, fixed by community/method/SGG)\n",
    "                    sgg_dii_df['x'] = sgg_dii_df[col_pop]\n",
    "\n",
    "                    # 5) Plot\n",
    "                    outdir = os.path.join(OUT_DIR, key)\n",
    "                    os.makedirs(outdir, exist_ok=True)\n",
    "                    outpath = os.path.join(outdir, f\"{method}_dii_sgg_dist_{year}__M{safe_name(cid)}.png\")\n",
    "\n",
    "                    fig, ax = plt.subplots(figsize=(14, 8), dpi=100)\n",
    "\n",
    "                    # === Scatter: split into in-range and overflow (minimal change) ===\n",
    "                    present_tiers = sorted(sgg_dii_df['tier'].unique(), reverse=True)\n",
    "                    for tier in present_tiers:\n",
    "                        mk = MARKER_MAP.get(tier, 'o')\n",
    "                        sub = sgg_dii_df[sgg_dii_df['tier'] == tier]\n",
    "                        in_mask = sub['dii'] <= YMAX\n",
    "                        sub_in  = sub[in_mask]\n",
    "                        ax.scatter(\n",
    "                            x=sub_in['x'], y=sub_in['dii'],\n",
    "                            s=TIER_SIZE_MAP.get(tier, POINT_SIZE_BASE),\n",
    "                            marker=mk,\n",
    "                            c=POINT_FACE, edgecolors=POINT_EDGE, linewidth=POINT_EW,\n",
    "                            alpha=0.95, zorder=10\n",
    "                        )\n",
    "\n",
    "                    # Labels: only for in-range points (overflow gets arrow labels)\n",
    "                    for _, row in sgg_dii_df[sgg_dii_df['dii'] <= YMAX].iterrows():\n",
    "                        name = get_name(row['sgg_code'])\n",
    "                        txt = ax.text(\n",
    "                            row['x']+0.006, row['dii']+0.002, f\" {name}\",\n",
    "                            fontsize=row['font_size'], color='black',\n",
    "                            ha='left', va='center', weight='bold', zorder=11\n",
    "                        )\n",
    "                        txt.set_path_effects([\n",
    "                            path_effects.Stroke(linewidth=2.0, foreground='white', alpha=0.85),\n",
    "                            path_effects.Normal()\n",
    "                        ])\n",
    "\n",
    "                    # Overflow: top-cap + arrow + name + true value\n",
    "                    overflow_all = sgg_dii_df[sgg_dii_df['dii'] > YMAX]\n",
    "                    if not overflow_all.empty:\n",
    "                        annotate_overflow(\n",
    "                            ax, overflow_all, get_name, MARKER_MAP, TIER_SIZE_MAP,\n",
    "                            POINT_FACE, POINT_EDGE, POINT_EW, ymax= 4.5,\n",
    "                            start_offset=0.3\n",
    "                        )\n",
    "\n",
    "                    # Reference line & axis formatting\n",
    "                    ax.axhline(1.0, color='red', linestyle='--', linewidth=1.2, label='Module Avg (DII=1.0)')\n",
    "                    ax.set_title(\n",
    "                        f\"Intra-Community DII Distribution (Module M{cid}, n={len(members)})\\n[{key} / {method} / {year}]\",\n",
    "                        fontsize=14, pad=14\n",
    "                    )\n",
    "                    ax.set_ylabel(\"Local Dominance Index (DII)\", fontsize=11)\n",
    "                    ax.set_xlabel(f\"SGGs in Module M{cid}\", fontsize=11)\n",
    "                    \n",
    "                    xticks = ax.get_xticks()\n",
    "                    # Set xticks and labels explicitly to avoid UserWarning\n",
    "                    ax.set_xticks(xticks)\n",
    "                    ax.set_xticklabels([f\"{int(x/10000)}만\" if x > 0 else \"0\" for x in xticks])\n",
    "                    ax.grid(axis='y', linestyle=':', linewidth=0.6, color='gray', alpha=0.7)\n",
    "\n",
    "                    # Legend: tiers + reference line (no cap legend as requested)\n",
    "                    leg_handles, leg_labels = [], []\n",
    "                    h_line = Line2D([0], [0], color='red', linestyle='--', linewidth=1.2, label='Module Avg (DII=1.0)')\n",
    "                    leg_handles.append(h_line); leg_labels.append(h_line.get_label())\n",
    "\n",
    "                    if classifier and k > 1:\n",
    "                        # bins: upper bounds (ascending). Reverse so Tier 1 is top bin\n",
    "                        bounds = []\n",
    "                        lo = sgg_dii_df['dii'].min()\n",
    "                        for i in range(k):\n",
    "                            hi = bins[i]\n",
    "                            bounds.append((lo, hi))\n",
    "                            lo = hi\n",
    "                        bounds = list(reversed(bounds))\n",
    "                        for t_idx, t in enumerate(range(1, k+1), start=0):\n",
    "                            mk = MARKER_MAP.get(t, 'o')\n",
    "                            lo_b, hi_b = bounds[t_idx]\n",
    "                            tier_text = f\"Tier {t}  ({lo_b:.2f} ~ {hi_b:.2f})\"\n",
    "                            ms = max(8, (TIER_SIZE_MAP.get(t, POINT_SIZE_BASE)) ** 0.5)\n",
    "                            leg_handles.append(Line2D([], [], marker=mk, linestyle='None',\n",
    "                                                      markersize=ms,\n",
    "                                                      markerfacecolor=POINT_FACE,\n",
    "                                                      markeredgecolor=POINT_EDGE, markeredgewidth=POINT_EW,\n",
    "                                                      label=tier_text))\n",
    "                            leg_labels.append(tier_text)\n",
    "                    else:\n",
    "                        mk = MARKER_MAP.get(1, 'o')\n",
    "                        tier_text = \"Tier 1 (single-bin)\"\n",
    "                        ms = max(8, (TIER_SIZE_MAP.get(1, POINT_SIZE_BASE)) ** 0.5)\n",
    "                        leg_handles.append(Line2D([], [], marker=mk, linestyle='None',\n",
    "                                                  markersize=ms,\n",
    "                                                  markerfacecolor=POINT_FACE,\n",
    "                                                  markeredgecolor=POINT_EDGE, markeredgewidth=POINT_EW,\n",
    "                                                  label=tier_text))\n",
    "                        leg_labels.append(tier_text)\n",
    "\n",
    "                    ax.legend(leg_handles, leg_labels, loc='center left', bbox_to_anchor=(1.02, 0.5),\n",
    "                              title=\"Legend (tiers by K-means)\", fontsize=10, title_fontsize=11, frameon=True)\n",
    "                    ax.patch.set_facecolor(\"whitesmoke\")\n",
    "                    ax.set_ylim(bottom=0, top=YMAX)\n",
    "                    plt.tight_layout(rect=[0, 0, 0.86, 1])\n",
    "\n",
    "                    # Save\n",
    "                    plt.savefig(outpath, dpi=110, bbox_inches=\"tight\")\n",
    "                    #plt.show()\n",
    "                    plt.close(fig)\n",
    "                    gc.collect()\n",
    "                    print(f\"  \\u2714 Saved: {outpath}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firm-transaction-network",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
